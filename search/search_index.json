{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Data Engineering","text":"<p>The amount of data in the world, the form these data take, and the ways to interact with data have all increased exponentially in recent years. The extraction of useful knowledge from data has long been one of the grand challenges of computer science, and the dawn of \"big data\" has transformed the landscape of data storage, manipulation, and analysis. In this module, we will look at the tools used to store and interact with data.</p> <p>The objective of this class is that students gain:</p> <ul> <li>First hand experience with and detailed knowledge of computing models, notably cloud computing</li> <li>An understanding of distributed programming models and data distribution</li> <li>Broad knowledge of many databases and their respective strengths</li> </ul> <p>As a part of the Data and Decision Sciences Master's program, this module aims specifically at providing the tool set students will use for data analysis and knowledge extraction using skills acquired in the Algorithms of Machine Learning and Digital Economy and Data Uses classes.</p>"},{"location":"index.html#class-structure","title":"Class structure","text":"<p>The class is structured in four parts:</p>"},{"location":"index.html#data-engineering-fundamentals","title":"Data engineering fundamentals","text":"<p>In this primer class, students will cover the basics of Linux command line usage, git, ssh, and data manipulation in python. The format of this class is an interactive capture-the-flag event.</p>"},{"location":"index.html#data-storage","title":"Data storage","text":"<p>This module covers Database Management Systems with a focus on SQL systems. For evaluation, students will install and manipulate data in PostgreSQL and MongoDB and compare the two systems.</p>"},{"location":"index.html#data-computation","title":"Data computation","text":"<p>A technical overview of the computing platforms used in the data ecosystem. We will briefly cover cluster computing and then go in depth on cloud computing, using Google Cloud Platform as an example. Finally, a class on GPU computing will be given in coordination with the deep learning section of the AML class.</p>"},{"location":"index.html#data-distribution","title":"Data distribution","text":"<p>In the final module, we cover the distribution of data, with a focus on distributed programming models. We will introduce functional programming and MapReduce, then use these concepts in a practical session on Spark. Finally, students will do a graded exercise with Dask.</p>"},{"location":"0_1_databases.html","title":"Data Storage","text":"<p>In this module on databases, database management systems will be covered. A basic understanding of SQL is considered as a prerequisite, and students can refer to the slides and additional resources if needed. For evaluation, students will install and explore the advantages of different DBMSs as a graded project.</p> <p>In this first class, we introduce the basics of database management systems and cover high level DBMS functionality.</p> <p>Slides</p> <p>For the next class, students should install PostgreSQL and MongoDB on their local machines.</p>"},{"location":"0_1_databases.html#additional-resources","title":"Additional Resources","text":"<ul> <li>PostgreSQL documentation</li> <li>MongoDB documentation</li> <li>SQLBolt - SQL exercises</li> <li>Databases introduction (fr)</li> <li>A comprehensive overview of database systems (en)</li> </ul>"},{"location":"0_2_ETL.html","title":"Extract, Transform, Load (ETL)","text":"<p>In this module on ETL, we will cover the fundamental concepts and practices of data integration and processing. A basic understanding of databases and SQL is considered a prerequisite. Students can refer to the slides and additional resources if they need to refresh their knowledge.</p> <p>In this first class, we introduce the basics of ETL processes and cover high-level ETL functionality and tools.</p> <p>Slides</p> <p>In the second class, we will work through an example case on aircraft component data.</p> <p>For evaluation, students will design and implement an ETL pipeline as a graded project.</p>"},{"location":"0_3_dbms.html","title":"Evolution of Data Management Systems","text":""},{"location":"0_3_dbms.html#fundamental-concepts-methods-and-applications","title":"Fundamental Concepts, Methods and Applications","text":"<p>In this three part class, students will cover the history of data management systems, from file systems to databases to distributed cloud storage. This class is given over the length of the Data Engineering course. Questions from the first two parts are integrated into the exam on cloud computing, and questions from the Cloud DMS section are integrated into the Dask notebook evaluation.</p>"},{"location":"0_3_dbms.html#objectives","title":"Objectives","text":"<p>The objectives of this course are: - Introduce the fundamental concepts - Describe, in a synthetic way, the main characteristics of the evolution of DMS (Data Management Systems) - Highlight targeted application classes.</p>"},{"location":"0_3_dbms.html#key-words","title":"Key Words","text":"<p>Data Management Systems, Uni-processor DBMS, Parallel DBMS, Data Integration Systems,Big Data, Cloud  Data Management Systems, High Performance, Scalability, Elasticity, Multi-store/Poly-store Systems</p>"},{"location":"0_3_dbms.html#targeted-skills","title":"Targeted Skills","text":"<ul> <li>Effectively exploit the DMS according to the environment (uniprocessor, parallel, distributed, cloud) in a perspective of decision support within an organization.</li> <li>Ability to choose, in a relevant way, a DMS in multiple environments for an optimal functioning of the applications of an organization</li> </ul>"},{"location":"0_3_dbms.html#indicative-program","title":"Indicative Program","text":"<ol> <li> <p>Introduction to Main Problems of Data Management</p> <ul> <li>From File Management Systems FMS to Database MS DBMS</li> <li>Motivations, Objectives, Organizations &amp; Drawbacks</li> <li>Databases &amp; Rel. DBMS: Motivations &amp; Objectives</li> <li>Resources:<ul> <li>Introduction</li> <li>SGF - File Systems</li> <li>Views - Relational Systems</li> <li>File Organization</li> </ul> </li> </ul> </li> <li> <p>Parallel Database Systems</p> <ul> <li>Objectives and Parallel Architecture Models</li> <li>Data Partitioning Strategies</li> <li>Parallel Query Processing</li> <li>Resources:<ul> <li>Parallel DBMS</li> <li>Parallel Queries</li> <li>Systems DB Parallel</li> </ul> </li> </ul> </li> <li> <p>From Distributed DB to Data Integration Systems DIS</p> <ul> <li>An Ex. of DDB, Motivations &amp; Objectives</li> <li>Designing of DDB</li> <li>Distributed Query Processing</li> <li>An Ex. of DIS</li> <li>Motivations &amp; Objectives</li> <li>Mediator-Adapters Architecture</li> <li>Design of a Global Schema (GAV, LAV)</li> <li>Query Processing Methodologies</li> <li>Resources:<ul> <li>Distributed DBMS - Chapter 1</li> <li>Distributed DBMS - Chapter 2</li> <li>Distributed DBMS - Chapter 3</li> <li>Systems for integrating heterogeneous and distributed data</li> <li>Integration Systems complement</li> <li>Distributed DBMS Dec 2023</li> </ul> </li> </ul> </li> <li> <p>Cloud Data Management Systems CDMS</p> <ul> <li>Motivations and Objectives</li> <li>Main Characteristics of Big Data and CDMS</li> <li>Classification of Cloud Data Management Systems CDMS</li> <li>Advantages and Weakness of Parallel RDBMS and CDMS</li> <li>Comparison between Parallel RDBMS and CDMS</li> <li>Introduction to Multi-store/Ploystore Systems</li> <li>Resources:<ul> <li>Cloud Systems</li> <li>MapReduce examples</li> </ul> </li> </ul> </li> <li> <p>Conclusion</p> <ul> <li>Maturity of Cloud DMS</li> <li>Key Criteria for Choosing a Data Management System</li> </ul> </li> </ol>"},{"location":"0_3_dbms.html#additional-reading","title":"Additional Reading","text":"<ol> <li> <p>Principles of Distributed Database Systems,  M. Tamer Ozsu  and Patrick Valduriez; Springer-Verlag ;  Fourth Edition,  December 2019.</p> </li> <li> <p>Data Management in the Cloud: Challenges and Opportunities Divyakant Agrawal, Sudipto Das, and Amr El Abbadi; Synthesis Lectures on Data Management, December 2012, Vol. 4, No. 6 , Pages 1-138.</p> </li> <li> <p>Query Processing in Parallel Relational Database Systems; H. Lu, B.-C Ooi and K.-L. Tan; IEEE Computer Society Press, CA, USA, 1994.</p> </li> <li> <p>Traitement parall\u00e8le dans les bases de donn\u00e9es relationnelles : concepts, m\u00e9thodes et applications Abdelkader Hameurlain, Pierre Bazex, Franck Morvan; C\u00e9padu\u00e8s Editions,  Octobre 1996.  </p> </li> </ol>"},{"location":"0_3_postgres.html","title":"PostgeSQL","text":"<p>In this practical session, we cover many examples of database queries with the popular DBMS PostgreSQL.</p> <p>Based on the TP by Christophe Garion, CC BY-NC-SA 2015.</p>"},{"location":"0_3_postgres.html#setup","title":"Setup","text":"<p>Before class, please install PostgreSQL and pgAdmin.</p>"},{"location":"0_3_postgres.html#postgresql-installation","title":"PostgreSQL installation","text":"<p>For this session, students should install PostgreSQL (v9 or higher) and pgAdmin (v4). Follow the installation instructions and make sure you have an initial database setup and the <code>postgresql</code> service running.</p> <ul> <li>Installation on Ubuntu</li> <li>Installation on Mac OS</li> <li>Installation on Arch Linux</li> <li>Installation on Windows Subsystem for Linux</li> <li>Installation on Windows (and add the PostgreSQL binaries to your path)</li> </ul> <p>Additionally, add your login user as a postgresql superuser to enable database creation with your user:</p> <pre><code># bash shell in Linux or OSX\n$ sudo su -l postgres\n[postgres]$ createuser --interactive\n</code></pre>"},{"location":"0_3_postgres.html#pgadmin","title":"pgAdmin","text":"<p>You can do all exercises directly through the <code>psql</code> shell for this class. However, it is useful to have a graphical confirmation of the database configuration. pgAdmin is one of many front-ends for Postgres. Install it by following the instructions on the pgAdmin site.</p>"},{"location":"0_3_postgres.html#setup-database-creation","title":"Setup - database creation","text":"<p>Once you've installated and configured PostgreSQL, create the first exercise database:</p> <pre><code># bash shell in Linux or OSX or windows powershell\n$ createdb db-mexico86\n</code></pre> <p>you can also do this through an SQL shell:</p> <pre><code># SQL shell\npostgres=# CREATE DATABASE \"db-mexico86\";\n</code></pre> <p>Confirm with pgAdmin that your database <code>db-mexico86</code> was created. If you don't have any servers, create one by right-clicking. The host address is <code>127.0.0.1</code> and the maintenance database and username should be <code>postgres</code>.</p> <p>In pgAdmin, if you are asked for a password and don't know what your password is, you can reset the password of the postgres user:</p> change password <pre><code>postgres=# ALTER USER postgres WITH PASSWORD \"newpassword\";\n</code></pre>"},{"location":"0_3_postgres.html#mexico86-database-simple-queries","title":"Mexico86 database - simple queries","text":"<p>This database contains data from the 1986 football World Cup. </p> <p>You can download the database creation script individually: <pre><code>$ wget https://raw.githubusercontent.com/SupaeroDataScience/DE/master/scripts/mexico86/create-tables-std.sql\n</code></pre></p> <p>Or git clone the class repository and navigate to the creation and insertion scripts.</p> <p>Once you have the scripts, run the database creation script in the <code>mexico</code> folder.</p> <pre><code># bash shell in Linux or OSX, or windows powershell\n$ psql -d db-mexico86 -f mexico86/create-tables-std.sql\n</code></pre> <p>If that doesn't work, you can copy the script into the Query Tool in pgAdmin.</p> <p>Exercise 1.1: Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin.</p> Response  Pays: (nom, groupe)  Typematch: (type)  Match: (paysl, paysv, butsl, butsv, type, date)   <p>You should be able to make queries now. You can either use PostgreSQL in interactive mode by running </p> <pre><code>$ psql -d db-mexico86\n</code></pre> <p>or write your solutions in an SQL file and run the file:</p> <pre><code>$ echo \"SELECT groupe FROM pays;\" &gt; a.sql\n$ psql -d db-mexico86 -f a.sql\n</code></pre> <p>You can also use the Query Editor in pgAdmin for a graphical interface.</p> <p>Exercise 1.2: Write a query which lists the countries participating in the World Cup.</p> Response <pre><code>        nom \n---------------------\nArgentine\nItalie\nBulgarie\nCor\u00e9e\nMexique\nParaguay\nBelgique\nIrak\nURSS\nHongrie\nFrance\nCanada\nBr\u00e9sil\nEspagne\nIrlande du Nord\nAlg\u00e9rie\nDanemark\nRFA\nUruguay\n\u00c9cosse\nMaroc\nAngleterre\nPologne\nPortugal\n(24 rows)\n</code></pre> <p>Exercise 1.3: Write a query which lists all matches as a pair of countries per match.</p> Response <pre><code>        paysl        |        paysv \n---------------------|---------------------\nBulgarie            | Italie\nArgentine           | Cor\u00e9e\nItalie              | Argentine\nCor\u00e9e               | Bulgarie\nCor\u00e9e               | Italie\nArgentine           | Bulgarie\nBelgique            | Mexique\nParaguay            | Irak\nMexique             | Paraguay\nIrak                | Belgique\nIrak                | Mexique\nParaguay            | Belgique\nCanada              | France\nURSS                | Hongrie\nFrance              | URSS\nHongrie             | Canada\nURSS                | Canada\nHongrie             | France\nEspagne             | Br\u00e9sil\nAlg\u00e9rie             | Irlande du Nord\nBr\u00e9sil              | Alg\u00e9rie\nIrlande du Nord     | Espagne\nIrlande du Nord     | Br\u00e9sil\nAlg\u00e9rie             | Espagne\nUruguay             | RFA\n\u00c9cosse              | Danemark\nDanemark            | Uruguay\nRFA                 | \u00c9cosse\n\u00c9cosse              | Uruguay\nDanemark            | RFA\nMaroc               | Pologne\nPortugal            | Angleterre\nAngleterre          | Maroc\nPologne             | Portugal\nAngleterre          | Pologne\nMaroc               | Portugal\nBr\u00e9sil              | Pologne\nFrance              | Italie\nMaroc               | RFA\nMexique             | Bulgarie\nArgentine           | Uruguay\nAngleterre          | Paraguay\nURSS                | Belgique\nEspagne             | Danemark\nBr\u00e9sil              | France\nRFA                 | Mexique\nArgentine           | Angleterre\nBelgique            | Espagne\nFrance              | RFA\nArgentine           | Belgique\nRFA                 | Argentine\n(51 rows)\n</code></pre> <p>Exercise 1.4: Write a query which lists the matches which took place on June 5, 1986.</p> Response <pre><code>        paysl        |   paysv\n---------------------|-----------\nItalie              | Argentine\nCor\u00e9e               | Bulgarie\nFrance              | URSS\n(3 rows)\n</code></pre> <p>Exercise 1.5: Write a query which lists the countries which France played against (hint, France could have played either side).</p> Response <pre><code>pays\n---------\nBr\u00e9sil\nCanada\nHongrie\nItalie\nRFA\nURSS\n(6 rows)\n</code></pre> <p>Exercise 1.6: Write a query which returns the winner of the World Cup</p> Response <pre><code>pays\n-----------\nArgentine\n(1 row)\n</code></pre>"},{"location":"0_3_postgres.html#beer-database","title":"Beer database","text":"<p>We'll now use a database which tracks the beers that a group of friends enjoy. Create the database and populate it using the provided scripts.</p> <pre><code>$ createdb db-beer\n$ psql -d db-beer -f beer/create-tables-std.sql\n$ psql -d db-beer -f beer/insert.sql\n</code></pre> <p>Exercise 2.1: Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin.</p> Response  Frequente: (buveur, bar)  Sert: (bar, biere)  Aime: (buveur, biere)   <p>Write queries which respond to the following questions. Hint, understanding natural joins may help.</p> <p>Exercise 2.2 What is the list of bars which serve the beer that Martin likes?</p> Response <pre><code>        bar \n-------------------\n Ancienne Belgique\n La Tireuse\n Le Filochard\n(3 rows)\n</code></pre> <p>Exercise 2.3 What is the list of drinkers who go to at least one bar which servers a beer they like?</p> Response <pre><code> buveur \n--------\n Bob\n David\n Emilie\n Martin\n(4 rows)\n</code></pre> <p>Exercise 2.3 What is the list of drinkers who don't go to any bars which serve the beer they like?</p> Response <pre><code> buveur \n--------\n Cecile\n Alice\n(2 rows)\n</code></pre>"},{"location":"0_3_postgres.html#complex-queries-mexico-database","title":"Complex queries - Mexico database","text":"<p>Exercise 3.1: Create a table with an entry for each match which lists the total number of goals (scored by either side), the match type, and the date. As we'll use this table later on, create a VIEW called \"matchbutsglobal\" with this information.</p> Response <pre><code>        paysl        |        paysv        | buts |  type  |    date \n---------------------+---------------------+------+--------+------------\n URSS                | Belgique            |    7 | 1/8    | 1986-06-15\n France              | Italie              |    2 | 1/8    | 1986-06-17\n Maroc               | Pologne             |    0 | Poule  | 1986-06-02\n RFA                 | Argentine           |    5 | Finale | 1986-06-29\n Br\u00e9sil              | France              |    2 | 1/4    | 1986-06-21\n Italie              | Argentine           |    2 | Poule  | 1986-06-05\n Maroc               | Portugal            |    4 | Poule  | 1986-06-11\n Br\u00e9sil              | Alg\u00e9rie             |    1 | Poule  | 1986-06-06\n Paraguay            | Belgique            |    4 | Poule  | 1986-06-11\n Hongrie             | France              |    3 | Poule  | 1986-06-09\n Irak                | Belgique            |    3 | Poule  | 1986-06-08\n Danemark            | RFA                 |    2 | Poule  | 1986-06-13\n Irlande du Nord     | Espagne             |    3 | Poule  | 1986-06-07\n Alg\u00e9rie             | Irlande du Nord     |    2 | Poule  | 1986-06-03\n RFA                 | Mexique             |    0 | 1/4    | 1986-06-21\n URSS                | Hongrie             |    6 | Poule  | 1986-06-02\n Mexique             | Paraguay            |    2 | Poule  | 1986-06-07\n Belgique            | Espagne             |    2 | 1/4    | 1986-06-22\n Irak                | Mexique             |    1 | Poule  | 1986-06-11\n Espagne             | Br\u00e9sil              |    1 | Poule  | 1986-06-01\n Angleterre          | Maroc               |    0 | Poule  | 1986-06-06\n Irlande du Nord     | Br\u00e9sil              |    2 | Poule  | 1986-06-12\n Maroc               | RFA                 |    1 | 1/8    | 1986-06-17\n Belgique            | Mexique             |    3 | Poule  | 1986-06-03\n Bulgarie            | Italie              |    2 | Poule  | 1986-05-31\n \u00c9cosse              | Uruguay             |    0 | Poule  | 1986-06-13\n Alg\u00e9rie             | Espagne             |    3 | Poule  | 1986-06-12\n Argentine           | Belgique            |    2 | 1/2    | 1986-06-25\n Br\u00e9sil              | Pologne             |    4 | 1/8    | 1986-06-16\n Danemark            | Uruguay             |    7 | Poule  | 1986-06-08\n Cor\u00e9e               | Italie              |    5 | Poule  | 1986-06-10\n Canada              | France              |    1 | Poule  | 1986-06-01\n Argentine           | Uruguay             |    1 | 1/8    | 1986-06-16\n France              | RFA                 |    2 | 1/2    | 1986-06-25\n France              | URSS                |    2 | Poule  | 1986-06-05\n Uruguay             | RFA                 |    2 | Poule  | 1986-06-04\n Angleterre          | Pologne             |    3 | Poule  | 1986-06-11\n Portugal            | Angleterre          |    1 | Poule  | 1986-06-03\n \u00c9cosse              | Danemark            |    1 | Poule  | 1986-06-04\n Angleterre          | Paraguay            |    3 | 1/8    | 1986-06-18\n Hongrie             | Canada              |    2 | Poule  | 1986-06-06\n Argentine           | Cor\u00e9e               |    4 | Poule  | 1986-06-02\n Pologne             | Portugal            |    1 | Poule  | 1986-06-07\n RFA                 | \u00c9cosse              |    3 | Poule  | 1986-06-08\n Mexique             | Bulgarie            |    2 | 1/8    | 1986-06-15\n URSS                | Canada              |    2 | Poule  | 1986-06-09\n Espagne             | Danemark            |    6 | 1/8    | 1986-06-18\n Paraguay            | Irak                |    1 | Poule  | 1986-06-04\n Argentine           | Bulgarie            |    2 | Poule  | 1986-06-10\n Argentine           | Angleterre          |    3 | 1/4    | 1986-06-22\n Cor\u00e9e               | Bulgarie            |    2 | Poule  | 1986-06-05\n(51 rows)\n</code></pre> <p>Exercise 3.2: Write a query which caluculates the number of goals scored on average in all the matches of the French team.</p> Response <pre><code>    Moyenne buts\n--------------------\n 2.0000000000000000\n(1 row)\n</code></pre> <p>Exercise 3.3: Write a query which calculates the total number of goals scored only by the French team.</p> Response <pre><code> buts \n------\n    8\n(1 row)\n</code></pre> <p>Exercise 3.4: Write a query which caluclates the total number of goals scored in each Poule match. Order the results by group.</p> Response <pre><code> groupe | sum \n--------+-----\n A      |  17\n B      |  14\n C      |  16\n D      |  12\n E      |  15\n F      |   9\n(6 rows)\n</code></pre> <p>Exercise 3.5: Write a function <code>vainquer</code> which takes in the two countries of a match and the match type and which returns the winner. Apply your function to the following pairs:</p> <pre><code>SELECT * FROM vainqueur('Espagne', 'Danemark', '1/8');\nSELECT * FROM vainqueur('Br\u00e9sil', 'France', '1/4');\n</code></pre> Response <pre><code> vainqueur \n-----------\n Espagne\n(1 row)\n\n vainqueur \n-----------\n Match nul\n(1 row)\n</code></pre> <p>Exercise 3.6: Write a function <code>butsparequipe</code> which returns the total and the average number of points scored by a team. Apply your function to the French team. Bonus points for making the result display the name of the team.</p> <pre><code>SELECT * FROM butsparequipe('France');\n</code></pre> Response <pre><code>  pays  | total |      moyenne \n--------+-------+--------------------\n France |     8 | 1.3333333333333333\n(1 row)\n</code></pre> <p>Exercise 3.7: Using the <code>butsparequipe</code> function, write a query which lists all countries and the points they scored. </p> Response <pre><code>        pays         | total \n---------------------+-------\n Argentine           |    14\n Italie              |     5\n Bulgarie            |     2\n Cor\u00e9e               |     4\n Mexique             |     6\n Paraguay            |     4\n Belgique            |    10\n Irak                |     1\n URSS                |    12\n Hongrie             |     2\n France              |     8\n Canada              |     0\n Br\u00e9sil              |     9\n Espagne             |    11\n Irlande du Nord     |     2\n Alg\u00e9rie             |     1\n Danemark            |    10\n RFA                 |     8\n Uruguay             |     2\n \u00c9cosse              |     1\n Maroc               |     3\n Angleterre          |     7\n Pologne             |     1\n Portugal            |     2\n(24 rows)\n</code></pre> <p>Exercise 3.8: Using the <code>butsparequipe</code> function, write a query which shows the country which scored the most points and the number of points they scored.</p> Response <pre><code>   pays    | total \n-----------+-------\n Argentine |    14\n(1 row)\n</code></pre>"},{"location":"0_3_postgres.html#pull-the-trigger","title":"Pull the trigger","text":"<p>In this exercise, we're going to create a TRIGGER, a mechanism which allows for automatically executing actions when an event occurs.</p> <p>Create the <code>db-trigger</code> database.</p> <pre><code>$ createdb db-trigger\n</code></pre> <p>Exercise 4.1: Create a table <code>rel(nom, value)</code> where <code>nom</code> is a string of characters and <code>value</code> is an integer. <code>nom</code> will be the primary key</p> Solution <pre><code>CREATE TABLE IF NOT EXISTS rel (\n    nom VARCHAR(20),\n    valeur INTEGER,\n    PRIMARY KEY (nom)\n);\n</code></pre> <p>Exercise 4.2: Add 5 tuples into the table</p> Solution <pre><code>INSERT INTO rel VALUES\n       ('Alice', 10),\n       ('Bob', 5),\n       ('Carl', 20),\n       ('Denise', 11),\n       ('Esther', 6);\n</code></pre> <p>Exercise 4.3: Write a trigger such that, when adding new tuples, the average value of <code>val</code> cannot decrease. If a new tuple is added which would decrease the average, an exception should be raised.</p> <p>The following insertion should work:</p> <pre><code>INSERT INTO rel VALUES ('Fab', 15);\n\nSELECT * FROM rel;\n</code></pre> <p>As we can see, the <code>(Fab, 15)</code> tuple was added:</p> <pre><code>  nom   | valeur \n--------+--------\n Alice  |     10\n Bob    |      5\n Carl   |     20\n Denise |     11\n Esther |      6\n Fab    |     15\n(6 rows)\n</code></pre> <p>However, the following insertion should give an exception:</p> <pre><code>INSERT INTO rel VALUES ('Guy', 2);\n</code></pre> Solution <pre><code>CREATE OR REPLACE FUNCTION verifier_moyenne()\n                  RETURNS trigger AS $verifier_moyenne$\n    DECLARE\n      moyenne FLOAT;\n      nb      INTEGER;\n    BEGIN\n        moyenne := AVG(valeur) FROM rel;\n        nb := COUNT(*) FROM rel;\n\n        IF ((nb * moyenne + NEW.valeur) / (nb + 1)) &lt; moyenne THEN\n            RAISE EXCEPTION 'problem with insertion: valeur average is decreasing!';\n        END IF;\n\n        RETURN NEW;\n    END;\n$verifier_moyenne$ LANGUAGE plpgsql;\n\nCREATE TRIGGER VerificationMoyenne\nBEFORE INSERT ON rel\nFOR EACH ROW\nEXECUTE PROCEDURE verifier_moyenne();\n</code></pre>"},{"location":"0_4_project.html","title":"Databases Project","text":"<p>This project is detailed in the ETL class.</p> <p>You are part of a 4-person data engineering team at a startup, tasked with designing and implementing an ETL/ELT pipeline. Your assignment is to submit a 2-4 page report detailing the choices made for the ETL/ELT pipeline and to provide a demo of an example database. Your startup ideas will be defined in the AI Business Models class; the goal of this project is to make a proof-of-concept of the data pipeline for part of your startup.</p> <p>In your report, you need to clearly explain and justify your decisions for each phase of the pipeline:</p> <ol> <li> <p>Extract (E): Identify and explain where the data is coming from. Discuss the sources and why they were chosen.</p> </li> <li> <p>Transform (T): Explain how the data is being transformed. Describe the processes, tools, and techniques used to clean, aggregate, or modify the data to make it useful for its intended purpose.</p> </li> <li> <p>Load (L): Detail how the data is loaded into the system, how it is stored, and how it will be used or queried. Discuss the database or storage options chosen, and explain how the data will be utilized by the organization or application.</p> </li> </ol> <p>Along with the report, you are expected to provide a demo of an example database. You can use PostgreSQL, MongoDB, or another database system of your choice. The demo should include:</p> <ul> <li>Documented scripts to load and manipulate example data that demonstrates the choices made for the ETL pipeline.</li> <li>The data used in the demo does not need to be exhaustive, but it should be sufficient to illustrate the key decisions in the ETL process.</li> </ul>"},{"location":"0_4_project.html#grading-criteria","title":"Grading Criteria:","text":"<ul> <li>Report Rigor (6 points): Depth and thoroughness in explaining your ETL/ELT choices.</li> <li>Report Clarity (6 points): How clearly and effectively your report communicates the ETL/ELT pipeline.</li> <li>Demo Data (4 points): Appropriateness and accuracy of the example data used in the demo.</li> <li>Demo Manipulation (4 points): Functionality and quality of the data manipulation demonstrated in the example.</li> </ul>"},{"location":"0_4_project.html#deadline","title":"Deadline:","text":"<ul> <li>There will be a 3 hour work session on this project on Sep 30 2025</li> <li>The report and demo must be submitted by October 23, 2025, end of day to the LMS.</li> </ul>"},{"location":"0_5_airlife.html","title":"Extract, Transform, Load (ETL) - AirLife Workshop","text":""},{"location":"0_5_airlife.html#overview","title":"Overview","text":"<p>In this 3-hour hands-on workshop, you will build a proof-of-concept ETL pipeline for AirLife, a startup that tracks aircraft lifecycle data including flight history, carbon footprint, and real-time location. You'll work through the complete ETL process from API data extraction to database loading, gaining practical experience with real-world data engineering challenges.</p> <p>Learning Objectives:</p> <ul> <li>Understand ETL pipeline design and implementation</li> <li>Work with REST APIs for data extraction  </li> <li>Apply data transformation techniques using Python and SQL</li> <li>Implement data loading into PostgreSQL</li> <li>Use Git for collaborative development</li> <li>Document and present your ETL solution</li> </ul> <p>Prerequisites:</p> <ul> <li>Basic understanding of Python programming</li> <li>Familiarity with SQL queries</li> <li>Git fundamentals</li> <li>Access to a development environment with Python 3.7+ and PostgreSQL</li> </ul>"},{"location":"0_5_airlife.html#part-1-understanding-airlife-and-setup-30-minutes","title":"Part 1: Understanding AirLife and Setup (30 minutes)","text":""},{"location":"0_5_airlife.html#11-airlife-company-overview-10-minutes","title":"1.1 AirLife Company Overview (10 minutes)","text":"<p>AirLife is a startup that tracks basic information about aircraft and flights. For this workshop, we'll focus on a simple version of their data pipeline.</p> <p>Our Goal Today:</p> <ul> <li>Extract airport data from a CSV file</li> <li>Extract some live flight data from a simple API</li> <li>Clean and combine this data </li> <li>Load it into a PostgreSQL database</li> </ul> <p>Data Sources We'll Use:</p> <ul> <li>OpenFlights CSV: Airport information (name, city, country, coordinates)</li> <li>OpenSky Network API: Current flights over Europe</li> </ul>"},{"location":"0_5_airlife.html#12-fork-and-explore-the-repository-10-minutes","title":"1.2 Fork and Explore the Repository (10 minutes)","text":"<ol> <li> <p>Access the Starter Repository: https://github.com/SupaeroDataScience/ETL-AirLife</p> </li> <li> <p>Fork the Repository: </p> <ul> <li>Click \"Fork\" on the repository page</li> <li>Clone your forked version locally: <pre><code>git clone https://github.com/YOUR_USERNAME/ETL-AirLife.git\ncd ETL-AirLife\n</code></pre></li> </ul> </li> <li> <p>Explore Repository Structure: <pre><code>ETL-AirLife/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 airports.csv (sample data provided)\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 extract_data.py\n\u2502   \u251c\u2500\u2500 transform_data.py\n\u2502   \u2514\u2500\u2500 load_data.py\n\u251c\u2500\u2500 database_setup.sql\n\u2514\u2500\u2500 main.py\n</code></pre></p> </li> <li> <p>Install Dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"0_5_airlife.html#13-quick-api-test-10-minutes","title":"1.3 Quick API Test (10 minutes)","text":"<p>Let's test the OpenSky Network API to see what data looks like:</p> <ol> <li> <p>Test the API: Open a web browser and visit:    <pre><code>https://opensky-network.org/api/states/all?lamin=45&amp;lomin=5&amp;lamax=50&amp;lomax=15\n</code></pre></p> </li> <li> <p>Understand the Response: You'll see JSON data with current flights. Each flight has:</p> <ul> <li>Aircraft identifier</li> <li>Country of origin  </li> <li>Longitude and latitude</li> <li>Altitude and speed</li> </ul> </li> <li> <p>Note: This API has rate limits, so we'll be careful not to call it too often!</p> </li> </ol>"},{"location":"0_5_airlife.html#part-2-building-the-etl-pipeline-120-minutes","title":"Part 2: Building the ETL Pipeline (120 minutes)","text":""},{"location":"0_5_airlife.html#21-database-setup-15-minutes","title":"2.1 Database Setup (15 minutes)","text":"<p>First, let's create our PostgreSQL database and tables.</p> <ol> <li> <p>Create Database: <pre><code># Connect to PostgreSQL\npsql -U your_username -d postgres\n</code></pre></p> </li> <li> <p>Run Database Setup: <pre><code>-- Create database\nCREATE DATABASE airlife_db;\n\n-- Connect to the new database\n\\c airlife_db;\n\n-- Run the setup script\n\\i database_setup.sql\n</code></pre></p> </li> <li> <p>Verify Tables: Check that your tables were created:    <pre><code>SELECT * FROM airports LIMIT 5;\n</code></pre></p> </li> </ol>"},{"location":"0_5_airlife.html#22-data-extraction-35-minutes","title":"2.2 Data Extraction (35 minutes)","text":"<p>Now let's work on <code>src/extract_data.py</code>. This file will handle getting our data.</p>"},{"location":"0_5_airlife.html#extract-airport-data-from-csv","title":"Extract Airport Data from CSV","text":"<p>Complete the function to read airport data:</p> <pre><code>import pandas as pd\nimport requests\nimport time\n\ndef extract_airports():\n    \"\"\"Extract airport data from CSV file\"\"\"\n    # TODO: Read the airports.csv file using pandas\n    # TODO: Print how many airports were loaded\n    # TODO: Return the DataFrame\n    # The file is in data/airports.csv\n    pass\n\ndef extract_flights():\n    \"\"\"Extract current flight data from OpenSky Network API\"\"\"\n    url = \"https://opensky-network.org/api/states/all\"\n\n    # Parameters to limit to Europe area\n    params = {\n        'lamin': 45,  # South boundary\n        'lomin': 5,   # West boundary  \n        'lamax': 50,  # North boundary\n        'lomax': 15   # East boundary\n    }\n\n    try:\n        # TODO: Make the API request using requests.get()\n        # TODO: Check if the response is successful (status_code == 200)\n        # TODO: Convert JSON response to pandas DataFrame\n        # TODO: Return the DataFrame\n\n        # HINT: response.json() gives you the data\n        # HINT: The actual flight data is in response.json()['states']\n        pass\n\n    except Exception as e:\n        print(f\"Error fetching flight data: {e}\")\n        return pd.DataFrame()  # Return empty DataFrame on error\n</code></pre>"},{"location":"0_5_airlife.html#your-tasks","title":"Your Tasks:","text":"<ol> <li>Complete <code>extract_airports()</code>: Read the CSV file and return a pandas DataFrame</li> <li>Complete <code>extract_flights()</code>: Make the API call and convert the response to a DataFrame</li> <li>Test Your Functions: Run the extraction script to see if it works</li> </ol>"},{"location":"0_5_airlife.html#23-data-transformation-35-minutes","title":"2.3 Data Transformation (35 minutes)","text":"<p>Open <code>src/transform_data.py</code> and work on cleaning our data:</p> <pre><code>import pandas as pd\n\ndef clean_airports(airports_df):\n    \"\"\"Clean airport data\"\"\"\n    print(f\"Starting with {len(airports_df)} airports\")\n\n    # TODO: Remove rows with missing latitude or longitude\n    # TODO: Remove airports with invalid coordinates\n    # TODO: Print how many airports remain after cleaning\n\n    return airports_df\n\ndef clean_flights(flights_df):\n    \"\"\"Clean flight data\"\"\"\n    if flights_df.empty:\n        print(\"No flight data to clean\")\n        return flights_df\n\n    print(f\"Starting with {len(flights_df)} flights\")\n\n    # The OpenSky API returns data as a list of lists\n    # We need to give it proper column names\n    columns = ['icao24', 'callsign', 'origin_country', 'time_position', \n               'last_contact', 'longitude', 'latitude', 'altitude', \n               'on_ground', 'velocity', 'true_track', 'vertical_rate']\n\n    # TODO: Assign column names to the DataFrame\n    # TODO: Remove flights with missing coordinates\n    # TODO: Convert altitude from meters to feet (multiply by 3.28084)\n    # TODO: Print how many flights remain after cleaning\n\n    return flights_df\n\ndef combine_data(airports_df, flights_df):\n    \"\"\"Combine airport and flight data for analysis\"\"\"\n    # For this simple exercise, we'll just return both DataFrames\n    # In a real system, you might join them based on proximity\n\n    return airports_df, flights_df\n</code></pre>"},{"location":"0_5_airlife.html#your-tasks_1","title":"Your Tasks:","text":"<ol> <li>Complete the cleaning functions: Remove invalid data and fix data types</li> <li>Test Your Transformations: Make sure your cleaned data looks correct</li> <li>Handle Edge Cases: What happens if there's no flight data? Make sure your code doesn't crash</li> </ol>"},{"location":"0_5_airlife.html#24-data-loading-35-minutes","title":"2.4 Data Loading (35 minutes)","text":"<p>Finally, let's load our data into PostgreSQL. Open <code>src/load_data.py</code>:</p> <pre><code>import pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\n\ndef load_to_database(airports_df, flights_df):\n    \"\"\"Load cleaned data into PostgreSQL database\"\"\"\n\n    # TODO: Create connection string\n    # Format: postgresql://username:password@localhost:5432/airlife_db\n    connection_string = \"postgresql://your_username:your_password@localhost:5432/airlife_db\"\n\n    try:\n        # TODO: Create SQLAlchemy engine\n        engine = create_engine(connection_string)\n\n        # TODO: Load airports data\n        # TODO: Load flights data (only if not empty)\n\n        print(\"Data loaded successfully!\")\n\n        # TODO: Print some basic statistics\n        # How many airports were loaded?\n        # How many flights were loaded?\n\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n\ndef verify_data():\n    \"\"\"Check that data was loaded correctly\"\"\"\n    connection_string = \"postgresql://your_username:your_password@localhost:5432/airlife_db\"\n\n    try:\n        engine = create_engine(connection_string)\n\n        # TODO: Query the database to verify data\n\n        airports_count = pd.read_sql(\"SELECT COUNT(*) FROM airports\", engine)\n        print(f\"Airports in database: {airports_count.iloc[0,0]}\")\n\n        flights_count = pd.read_sql(\"SELECT COUNT(*) FROM flights\", engine) \n        print(f\"Flights in database: {flights_count.iloc[0,0]}\")\n\n        # TODO: Show sample data\n        sample_airports = pd.read_sql(\"SELECT * FROM airports LIMIT 3\", engine)\n        print(\"Sample airports:\")\n        print(sample_airports)\n\n    except Exception as e:\n        print(f\"Error verifying data: {e}\")\n</code></pre>"},{"location":"0_5_airlife.html#your-tasks_2","title":"Your Tasks:","text":"<ol> <li>Update Connection String: Use the correct database credentials</li> <li>Complete the Loading Functions: Use pandas to_sql() to load data</li> <li>Test the Pipeline: Run the verification function to make sure it worked</li> </ol>"},{"location":"0_5_airlife.html#part-3-putting-it-all-together-30-minutes","title":"Part 3: Putting It All Together (30 minutes)","text":""},{"location":"0_5_airlife.html#31-complete-the-main-pipeline-15-minutes","title":"3.1 Complete the Main Pipeline (15 minutes)","text":"<p>Now let's connect all our pieces in <code>main.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nAirLife ETL Pipeline - Simple Version\n\"\"\"\n\nfrom src.extract_data import extract_airports, extract_flights\nfrom src.transform_data import clean_airports, clean_flights, combine_data\nfrom src.load_data import load_to_database, verify_data\n\ndef main():\n    \"\"\"Run the complete ETL pipeline\"\"\"\n    print(\"Starting AirLife ETL Pipeline...\")\n\n    # Step 1: Extract data\n    print(\"\\n=== EXTRACTION ===\")\n    airports = extract_airports()\n    flights = extract_flights()\n\n    # Step 2: Transform data\n    print(\"\\n=== TRANSFORMATION ===\")\n    clean_airports_data = clean_airports(airports)\n    clean_flights_data = clean_flights(flights)\n    final_airports, final_flights = combine_data(clean_airports_data, clean_flights_data)\n\n    # Step 3: Load data\n    print(\"\\n=== LOADING ===\")\n    load_to_database(final_airports, final_flights)\n\n    # Step 4: Verify everything worked\n    print(\"\\n=== VERIFICATION ===\")\n    verify_data()\n\n    print(\"\\n\u2705 ETL Pipeline completed!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"0_5_airlife.html#your-tasks_3","title":"Your Tasks:","text":"<ol> <li>Test the Full Pipeline: Run <code>python main.py</code> and fix any errors</li> <li>Handle Missing Data: What happens if the API call fails? Make sure your pipeline still works</li> <li>Add Basic Logging: Add print statements to track progress</li> </ol>"},{"location":"0_5_airlife.html#32-simple-documentation-and-git-commit-15-minutes","title":"3.2 Simple Documentation and Git Commit (15 minutes)","text":""},{"location":"0_5_airlife.html#update-your-readme","title":"Update Your README","text":"<p>Create a simple README.md explaining:</p> <pre><code># AirLife ETL Pipeline\n\n## What This Does\nThis pipeline extracts airport data from a CSV file and live flight data from an API, \ncleans the data, and loads it into a PostgreSQL database.\n\n## How to Run It\n1. Install dependencies: `pip install -r requirements.txt`\n2. Set up PostgreSQL database\n3. Update database connection in `src/load_data.py`\n4. Run: `python main.py`\n\n## What We Built\n- **Extract:** Gets airport data from CSV and flight data from OpenSky Network API\n- **Transform:** Cleans invalid coordinates and converts units\n- **Load:** Puts clean data into PostgreSQL tables\n\n## Team Members\n- [Add your names here]\n</code></pre>"},{"location":"0_5_airlife.html#commit-your-work","title":"Commit Your Work","text":"<p>You should save your progress to Git often:</p> <pre><code># Add all your changes\ngit add CHANGED_FILENAMES\n\n# Commit with a descriptive message\ngit commit -m \"MESSAGE HERE\"\n\n# Push to your forked repository\ngit push origin main\n</code></pre>"},{"location":"0_5_airlife.html#workshop-deliverables","title":"Workshop Deliverables","text":"<p>By the end of this 3-hour workshop, you should have:</p>"},{"location":"0_5_airlife.html#working-pipeline","title":"\u2705 Working Pipeline","text":"<ul> <li>Extracts airport data from CSV</li> <li>Fetches live flight data from API  </li> <li>Cleans and validates the data</li> <li>Loads everything into PostgreSQL</li> </ul>"},{"location":"0_5_airlife.html#code-repository","title":"\u2705 Code Repository","text":"<ul> <li>All Python files completed with your solutions</li> <li>Basic README documentation</li> <li>Code committed to Git</li> </ul>"},{"location":"0_5_airlife.html#database-with-data","title":"\u2705 Database with Data","text":"<ul> <li>PostgreSQL tables with actual airport and flight data</li> <li>Able to run simple queries on your data</li> </ul>"},{"location":"0_5_airlife.html#quick-demo-if-time-allows","title":"Quick Demo (If Time Allows)","text":"<p>If you finish early, try these simple queries on your data:</p> <pre><code>-- How many airports do we have?\nSELECT COUNT(*) FROM airports;\n\n-- Show airports in France\nSELECT name, city FROM airports WHERE country = 'France' LIMIT 5;\n\n-- How many flights are currently active?\nSELECT COUNT(*) FROM flights;\n\n-- Show the highest flying aircraft\nSELECT callsign, altitude FROM flights \nWHERE altitude IS NOT NULL \nORDER BY altitude DESC LIMIT 3;\n</code></pre>"},{"location":"0_5_airlife.html#what-weve-covered","title":"What We've Covered","text":"<ul> <li>ETL Basics: Extract, Transform, Load workflow</li> <li>API Integration: Making HTTP requests and handling JSON responses  </li> <li>Data Cleaning: Handling missing values and invalid data</li> <li>Database Loading: Using pandas to insert data into PostgreSQL</li> <li>Error Handling: Making code robust when things go wrong</li> </ul>"},{"location":"0_5_airlife.html#next-steps","title":"Next Steps","text":"<p>This workshop prepares you for the larger ETL Project where you'll design your own startup's ETL pipeline with more complex requirements, better error handling, and features that showcase the value points of your startup.</p>"},{"location":"1_1_overview.html","title":"Data Computation Part 1: Cloud Computing, Containers &amp; Deployment","text":""},{"location":"1_1_overview.html#syllabus","title":"Syllabus","text":""},{"location":"1_1_overview.html#introduction","title":"Introduction","text":"<p>Introduction to data computation module</p> <p>Link to slides</p> <p> </p>"},{"location":"1_1_overview.html#lecture-hands-on-cloud-computing-remote-development-3h","title":"Lecture &amp; Hands-on : Cloud Computing &amp; Remote Development (3h)","text":"<p>Intro to cloud computing &amp; remote development environments</p> <p>Discover Google Cloud Platform</p>"},{"location":"1_1_overview.html#lecture-hands-on-containers-3h","title":"Lecture &amp; Hands-on : Containers (3h)","text":"<p>Intro to containers with docker</p>"},{"location":"1_1_overview.html#in-practice-gcp-containers-3h","title":"In Practice : GCP &amp; Containers (3h)","text":"<p>A small workshop that puts everything together: Google cloud &amp; docker</p>"},{"location":"1_1_overview.html#in-practice-deploy-your-ml-model-in-production-3h","title":"In Practice : Deploy your ML model in production (3h)","text":"<p>Deploy your machine learning model in production with everything you've learnt</p> <p>We will then switch to the introduction to orchestration and kubernetes lectures</p>"},{"location":"1_1_overview.html#quiz-and-recap","title":"Quiz and recap","text":"<p>The evaluation of this section will be done with an open-resource quiz covering all cloud computing topics. </p> <p>The conclusion slides should be used to recap the previous courses.</p>"},{"location":"1_2a_cloud.html","title":"Cloud Computing &amp; Remote Development Environment","text":""},{"location":"1_2a_cloud.html#1-introduction-to-cloud-computing","title":"1. Introduction to Cloud Computing","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_2a_cloud.html#2-google-cloud-platform-remote-development","title":"2. Google Cloud Platform &amp; Remote Development","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_2b_handson_setup.html","title":"Remote Development hands-on","text":""},{"location":"1_2b_handson_setup.html#1-abstract","title":"1. Abstract","text":"<p>Abstract</p> <p>This is the setup part of the hands-on. You should do it first, according to the instructions given in class.</p> <p>Warning</p> <p>Some things may only work on eduroam or in 4G... Some things may only works on Google Chrome</p>"},{"location":"1_2b_handson_setup.html#2-setting-up-your-google-cloud-platform-account","title":"2. Setting up your Google Cloud Platform account","text":""},{"location":"1_2b_handson_setup.html#21-option-a-join-an-existing-gcp-project","title":"2.1 Option A : Join an existing GCP project","text":"<p>Note</p> <p>Only do this if you are asked to</p> <p>For this, you need a valid gmail address, then</p> <p>Fill the form that was posted on slack to be added to the common ISAE-SDD project. The project id for this year will also be given on the slack.</p> <p>Check that you have access to the project before starting the hands-on. Copy-paste the project id in the following URL, and check your browser</p> <p>https://console.cloud.google.com/home/dashboard?project=${PROJECT_ID}</p> <p>Success</p> <p>This should open a webpage indicating that you have access to the project.</p>"},{"location":"1_2b_handson_setup.html#22-option-b-create-your-own-gcp-project","title":"2.2 Option B : Create your own GCP project","text":"<p>Note</p> <p>Only do this if you are asked to</p> <p>Here you will each create a Google Cloud Platform account and project using the student credits given this year,</p> <p>Overview link</p> <ul> <li>Create an account within Google Cloud Platform using your ISAE e-mail</li> <li>Use the code given by Dennis to redeem your free credits</li> <li>You should have a free tier available to you as well as a coupon giving your credit</li> <li>From the interface you should create a project with a name of your choice (it is recommended to put for example sdd2526-yourname so that it is clear)</li> </ul>"},{"location":"1_2b_handson_setup.html#3-github-vscode","title":"3. GitHub &amp; VSCode","text":"<p>You should have a GitHub account already. Ensure you are connected with it in GitHub.</p> <p>You should also have vscode configured. In vscode, install the CodeSpace extension. Check the vscode documentation for more information.</p>"},{"location":"1_2c_handson_codespace.html","title":"Remote Development Hands-on","text":""},{"location":"1_2c_handson_codespace.html#0-abstract","title":"0. Abstract","text":"<p>Abstract</p> <p>In this hands-on, you will use a GitHub Codespace remote development environment to get familiar with working on code and data not stored on your computer. We will also discover Streamlit, a Python library for building web frontends, and learn how to preview remote applications from your browser.</p> <p>Warning</p> <p>Some things may only work on eduroam or with 4G... Some things may only work on Google Chrome</p> <p>Warning</p> <p>Don't forget to shut down everything when you're done!</p> <p>Tip</p> <p>When replacing <code>{something}</code> in commands, don't include the brackets. Write <code>yourname</code>, not <code>{yourname}</code>.</p>"},{"location":"1_2c_handson_codespace.html#1-my-first-virtual-machine-github-codespaces","title":"1. My first \"Virtual Machine\": GitHub Codespaces","text":"<p>First, you will need a GitHub account. You should already have one, otherwise create one.</p>"},{"location":"1_2c_handson_codespace.html#intro-to-github-codespaces","title":"Intro to GitHub Codespaces","text":"<ul> <li>GitHub Codespaces is a managed VM that lets you develop without configuring your local environment.</li> <li>Compared to configuring a VM yourself, Codespaces comes pre-loaded with developer tools, making it faster to get started.</li> <li>You have a free tier of 60 CPU hours / months and some disk space</li> <li>You pay for the CPU when the VM is ON and for the disk when the codespace is created</li> </ul> <p>Have a look at the overview: https://docs.github.com/en/codespaces/overview</p> <p>Question</p> <p>Can you describe it in your own words? How would ChatGPT (or any LLM) describe it?</p> <p>Note</p> <p>Google Cloud has a similar service called Google Cloud Shell, but Codespaces is more powerful, so we will use that instead</p> How Codespaces work under the hood <p>What is a Codespace technically?</p> <p>A Codespace is a Linux VM running in Microsoft Azure (GitHub's parent company). When you create a Codespace:</p> <ol> <li>Azure provisions a VM with your chosen resources (CPU cores, RAM)</li> <li>A Docker container is started on that VM with your development environment</li> <li>The container is configured via a <code>.devcontainer/devcontainer.json</code> file in your repo</li> </ol> <p>How does the browser connection work?</p> <p>When you open a Codespace in your browser:</p> <ul> <li>Your browser connects to a VS Code Server running in the container</li> <li>The VS Code UI is rendered locally in your browser (it's a web app)</li> <li>Commands and file operations are sent to the remote server via WebSocket</li> </ul> <p>How does port forwarding work?</p> <p>When you run a server (e.g., Jupyter on port 8888):</p> <ol> <li>The server binds to a port inside the container</li> <li>GitHub's infrastructure creates a reverse proxy with a unique URL</li> <li>Your browser connects to <code>https://{codespace-name}-8888.app.github.dev</code></li> <li>The proxy forwards traffic through secure tunnels to your container</li> </ol> <p>This is why you can access localhost services without any firewall configuration! </p>"},{"location":"1_2c_handson_codespace.html#web-ui-create-your-codespace-and-connect-to-it-using-the-web-interface","title":"Web UI : Create your codespace and connect to it using the web interface","text":"<p>Go to https://github.com/fchouteau/isae-cloud-computing-codespace</p> <p></p> <ul> <li>Click on the green \"Code\" button, then select \"Create codespace on main\"</li> <li>It should open a browser tab with VS Code</li> <li>Launch a terminal using the top menu (Terminal &gt; New Terminal)</li> </ul> <p>You should see a VS Code instance</p> <p></p> <p>Question</p> <p>Where is this running?</p> <p>If you go to https://github.com/codespaces, you should see your codespace running</p> <p></p>"},{"location":"1_2c_handson_codespace.html#alternative-connect-from-local-vscode","title":"Alternative: Connect from local VSCode","text":"<p>You can also connect to your Codespace from your local VSCode installation instead of using the browser.</p> <p>Prerequisites:</p> <ul> <li>VSCode installed on your machine</li> <li>GitHub Codespaces extension installed</li> </ul> <p>Steps: (via Web UI)</p> <p></p> <ul> <li>Go to https://github.com/codespaces and click on \"Open in VS Code Desktop\"</li> </ul> <p>Steps: (via VSCode)</p> <ol> <li>Open VSCode locally</li> <li>Open the Command Palette (<code>Ctrl+Shift+P</code> / <code>Cmd+Shift+P</code>)</li> <li>Type \"Codespaces: Create New Codespace\"</li> <li>Paste the following: <code>fchouteau/isae-cloud-computing-codespace</code></li> </ol> <p>To reconnect to an existing codespace: 1. Open VSCode locally 2. Open the Command Palette (<code>Ctrl+Shift+P</code> / <code>Cmd+Shift+P</code>) 3. Type \"Codespaces: Connect to Codespace\" 4. Select your running Codespace from the list</p> <p>Benefits of local VSCode:</p> <ul> <li>Use your familiar keybindings and settings</li> <li>Better performance (native app vs browser)</li> <li>Access to all your local extensions</li> <li>Easier file transfers via drag-and-drop</li> </ul> <p>Note: The Codespace still runs remotely - only the UI is local.</p>"},{"location":"1_2c_handson_codespace.html#explore-github-codespaces","title":"Explore GitHub Codespaces","text":"<p>GitHub Codespaces Getting Started</p> <p>Identify the following features in the interface:</p> <ul> <li>Code editor (VS Code)</li> <li>Terminal</li> <li>File explorer</li> <li>Debugging tools (breakpoints, console output)</li> </ul> <p>Run these commands to get a feel for the \"computer\" behind the interface:</p> <ul> <li>Check available disk space</li> </ul> Bash command to run <p><code>df -h</code></p> <ul> <li>Check the OS name</li> </ul> Bash command to run <p><code>cat /etc/os-release</code></p> <ul> <li>Check the CPU model</li> </ul> Bash command to run <p><code>cat /proc/cpuinfo</code></p> <ul> <li>This is the hardware model... How many cores do you have available? How much RAM?</li> </ul> Help <p><code>htop</code> will give you your current usage and available cores, or you can do <code>nproc</code></p> <ul> <li>Check the python installation</li> </ul> Bash command to run <p><code>python --version</code> <code>pip list</code></p> <ul> <li> <p>Try and upload a file from your computer to the codespace by right clicking on the file explorer on the left</p> </li> <li> <p>Create a new file and write a simple python \"Hello World\", then execute it from the terminal</p> </li> </ul>"},{"location":"1_2c_handson_codespace.html#a-quick-look-at-port-forwarding","title":"A quick look at port forwarding","text":"<p>When you run a server on a remote machine (like this Codespace), you need a way to access it from your browser. This is called port forwarding - we'll explore this in detail in Section 3 with Streamlit.</p> <p>For now, try this quick demo:</p> <ul> <li>In your codespace, run <code>jupyter lab</code> to launch Jupyter Lab</li> <li>Check the \"Ports\" tab in VS Code: it should show a new entry for port 8888</li> <li>Click \"Open in Browser\" to see how Codespace automatically forwards the port</li> <li>Cancel the Jupyter process with <code>CTRL+C</code></li> </ul> <p>We'll dive deeper into how port forwarding works in the Streamlit section below.</p> <p>What You Learned</p> <ul> <li>Remote development environments: Codespaces provide a cloud-hosted VM with pre-configured tools</li> <li>VS Code in the browser: The same IDE experience, but running remotely</li> <li>System exploration: Commands like <code>df -h</code>, <code>htop</code>, <code>cat /proc/cpuinfo</code> to understand your environment</li> <li>Port forwarding preview: A glimpse of how to access remote services (more in Section 3)</li> </ul>"},{"location":"1_2c_handson_codespace.html#2-running-a-ml-training-script-in-the-codespace","title":"2. Running a ML training script in the Codespace","text":"<p>In this section, you will run a machine learning training script directly in your Codespace. This demonstrates remote computation: the training runs on the Codespace VM, not on your laptop.</p>"},{"location":"1_2c_handson_codespace.html#21-locate-the-training-script","title":"2.1. Locate the training script","text":"<p>In your Codespace file explorer, navigate to the <code>training/</code> folder. You should see a <code>training.py</code> script.</p> <p>Take a look at the code:</p> <pre><code>cat training/training.py\n</code></pre> <p>Question</p> <p>What does this script do? What model is it training? What dataset does it use?</p>"},{"location":"1_2c_handson_codespace.html#22-install-dependencies-and-run-the-training","title":"2.2. Install dependencies and run the training","text":"<p>First, install the required dependencies:</p> <pre><code>pip install torch torchvision\n</code></pre> <p>Then run the training script:</p> <pre><code>python training/training.py --epochs 2 --save-model\n</code></pre> <p>Think about it</p> <p>Where is this training actually running?</p> <ul> <li>On your laptop?</li> <li>On the Codespace VM (in the cloud)?</li> <li>Somewhere else?</li> </ul> <p>How can you verify this? (Hint: check CPU usage with <code>htop</code> in another terminal)</p> <p>Watch the training progress. The script will save a model file (e.g., <code>model.pth</code>) when complete.</p>"},{"location":"1_2c_handson_codespace.html#23-download-the-trained-model-to-your-laptop","title":"2.3. Download the trained model to your laptop","text":"<p>Once training is complete, you need to retrieve the model file to your local machine.</p> <p>Option A: Via the file explorer</p> <ul> <li>Right-click on the <code>mnist_cnn.pt</code> file in the VS Code file explorer</li> <li>Select \"Download\"</li> </ul> <p>Option B: Via the terminal (if you have <code>gh</code> CLI locally)</p> <pre><code># From your local machine terminal\ngh codespace cp remote:/workspaces/isae-cloud-computing-codespace/training/model.pth ./model.pth\n</code></pre> <p>Checkpoint</p> <p>You have successfully:</p> <ul> <li>[x] Run a training script on a remote machine (Codespace)</li> <li>[x] Downloaded the resulting model to your laptop</li> </ul> <p>This is the fundamental workflow of remote computation: run heavy tasks in the cloud, retrieve results locally.</p> <p>Question</p> <p>How comfortable do you feel with this remote machine? Is it easy to get data in or out? Code in or out?</p> <p>What You Learned</p> <ul> <li>Remote computation: Running code on a cloud machine instead of your laptop</li> <li>File transfer: Moving files between local and remote environments</li> <li>The cloud workflow: Edit locally (or remotely), run remotely, retrieve results</li> </ul>"},{"location":"1_2c_handson_codespace.html#3-building-a-streamlit-webapp","title":"3. Building a Streamlit Webapp","text":"<p>We will introduce Streamlit, a Python library to build quick web apps for data science.</p> <p>In this section, you will build your first interactive webapp in Python and learn about port forwarding - how to access applications running on remote machines from your browser. This is a fundamental concept for cloud development.</p> <p>First, watch this video:</p>  Your browser does not support the video tag.  <p>Then, take a look at an introduction to Streamlit and the Streamlit application gallery</p> <p>Question</p> <p>Can you describe what Streamlit is? Can you think of ways it could be useful to you?</p>"},{"location":"1_2c_handson_codespace.html#31-your-first-streamlit-application","title":"3.1. Your first Streamlit application","text":"<p>Take a look at the code below: </p> <pre><code>import streamlit as st\nfrom streamlit_image_comparison import image_comparison\nimport cv2\n\nst.set_page_config(\"Webb Space Telescope vs Hubble Telescope\", \"\ud83d\udd2d\")\n\nst.header(\"\ud83d\udd2d J. Webb Space Telescope vs Hubble Telescope\")\n\nst.write(\"\")\n\"This is a reproduction of the fantastic [WebbCompare](https://www.webbcompare.com/index.html) app by [John Christensen](https://twitter.com/JohnnyC1423). It's built in Streamlit and takes only 10 lines of Python code. If you like this app, please star [John's original repo](https://github.com/JohnEdChristensen/WebbCompare)!\"\nst.write(\"\")\n\nst.markdown(\"### Southern Nebula\")\nimage_comparison(\n    img1=\"https://www.webbcompare.com/img/hubble/southern_nebula_700.jpg\",\n    img2=\"https://www.webbcompare.com/img/webb/southern_nebula_700.jpg\",\n    label1=\"Hubble\",\n    label2=\"Webb\",\n)\n\n\nst.markdown(\"### Galaxy Cluster SMACS 0723\")\nimage_comparison(\n    img1=\"https://www.webbcompare.com/img/hubble/deep_field_700.jpg\",\n    img2=\"https://www.webbcompare.com/img/webb/deep_field_700.jpg\",\n    label1=\"Hubble\",\n    label2=\"Webb\",\n)\n\nst.markdown(\"### Carina Nebula\")\nimage_comparison(\n    img1=\"https://www.webbcompare.com/img/hubble/carina_2800.png\",\n    img2=\"https://www.webbcompare.com/img/webb/carina_2800.jpg\",\n    label1=\"Hubble\",\n    label2=\"Webb\",\n)\n\nst.markdown(\"### Stephan's Quintet\")\nimage_comparison(\n    img1=\"https://www.webbcompare.com/img/hubble/stephans_quintet_2800.jpg\",\n    img2=\"https://www.webbcompare.com/img/webb/stephans_quintet_2800.jpg\",\n    label1=\"Hubble\",\n    label2=\"Webb\",\n)\n</code></pre> <p>Question</p> <p>By reading the code and documentation, can you describe what this code does?</p>"},{"location":"1_2c_handson_codespace.html#32-running-streamlit-in-the-codespace","title":"3.2. Running Streamlit in the Codespace","text":"What is a port? <p>A port is a 16-bit number (0-65535) that identifies a specific process or service on a machine. Think of it like apartment numbers in a building:</p> <ul> <li>The IP address is the building address</li> <li>The port is the apartment number</li> </ul> <p>Common ports:</p> Port Service 22 SSH 80 HTTP (web) 443 HTTPS (secure web) 8888 Jupyter (default) 8501 Streamlit (default) <p>When you run <code>jupyter lab</code>, it starts a web server listening on port 8888. Anyone who can reach that port can access Jupyter.</p> What is port forwarding? <p>Port forwarding (or tunneling) connects a port on one machine to a port on another through a secure channel.</p> <pre><code>Your laptop:8888  &lt;--tunnel--&gt;  Remote VM:8888\n</code></pre> <p>When you access <code>localhost:8888</code> on your laptop, the traffic is forwarded through the tunnel to port 8888 on the remote machine.</p> <p>Why is this useful?</p> <ul> <li>The remote machine might not be directly accessible from the internet</li> <li>You want to access services as if they were running locally</li> <li>Security: the tunnel encrypts traffic</li> </ul> <p>Install the dependencies:</p> <pre><code>pip install streamlit opencv-python-headless streamlit-image-comparison\n</code></pre> <p>Create a file <code>streamlit_jswt.py</code> and copy/paste the code above.</p> <p>Run the Streamlit server:</p> <pre><code>streamlit run streamlit_jswt.py\n</code></pre> <p>This will launch the application on port 8501 of your Codespace.</p> <p>To view it:</p> <ol> <li>Check the \"Ports\" tab in VS Code (bottom panel)</li> <li>You should see port 8501 listed</li> <li>Click \"Open in Browser\" or hover and click the globe icon</li> </ol> <p>Understanding port forwarding</p> <p>The Streamlit server is running on the Codespace VM, not on your laptop.</p> <p>Yet you can see it in your browser. How is this possible?</p> <p>Answer: Codespace automatically creates a tunnel (port forward) from the remote port 8501 to a public URL that your browser can access.</p> <p>Once you're done exploring, quit the server with <code>CTRL+C</code>.</p> <p>What You Learned</p> <ul> <li>Port forwarding: Accessing a remote service through a tunnel</li> <li>Web app basics: A Python process serving HTTP on a port</li> <li>Deployment preview: In Day 2, you'll deploy apps like this to the cloud</li> </ul>"},{"location":"1_2c_handson_codespace.html#whats-next","title":"What's Next","text":"<p>In Session 2, you'll learn how to package applications using Docker so they can run anywhere.</p> <p>In Day 2, you'll combine these skills to deploy ML models to the cloud.</p>"},{"location":"1_2d_handson_gcp.html","title":"Google Cloud Platform Hands-on","text":""},{"location":"1_2d_handson_gcp.html#0-abstract","title":"0. Abstract","text":"<p>Abstract</p> <p>In this hands-on, you will configure your GCP account and the Google Cloud SDK. You will create and manage Compute Engine VMs, learn to transfer files, and interact with Cloud Storage.</p> <p>Warning</p> <p>Some things may only work on eduroam or in 4G...</p> <p>Warning</p> <p>Don't forget to shutdown everything when you're done since it costs you money. At the end, even if you have not finished the TP, go to Section 7 \"Cleaning Up\"</p> <p>Tip</p> <p>When replacing <code>{something}</code> in commands, don't include the brackets. Write <code>yourname</code>, not <code>{yourname}</code>.</p> <p>Tip</p> <p>If you are lost on where you are, normally the terminal has the hostname indicated, otherwise run the command <code>hostname</code></p> <p>Cost Warning</p> <p>GCP resources cost money. Even with free credits:</p> <ul> <li>Always delete VMs when done (Section 7)</li> <li>Delete buckets you create</li> <li>Stop your Codespace when finished</li> </ul> <p>Forgot to clean up? Your credits will drain quickly.</p> <p>If you reach the end of the class without having finished everything, go to the cleaning Section 7 at the end.</p>"},{"location":"1_2d_handson_gcp.html#1-create-your-gcp-account","title":"1. Create your GCP Account","text":"<p>Note</p> <p>You can skip this for this year, it is provided as reference</p> <p>Here you will each create a Google Cloud Platform account and project using the student credits given this year,</p> <p>Overview link</p> <ul> <li>Create an account within Google cloud Platform using your ISAE e-mail</li> <li>Use the code given by Dennis to redeem your free credits</li> <li>You should have a free tier available to you as well as coupons</li> <li>From the interface you should create a project with a name of your choice (it is recommended to put for example sdd2526-yourname so that it is clear)</li> </ul>"},{"location":"1_2d_handson_gcp.html#2-reconnect-to-github-codespaces","title":"2. (re)connect to GitHub Codespaces","text":"<p>Note</p> <p>If you are still on the codespace, skip this</p>"},{"location":"1_2d_handson_gcp.html#if-you-still-have-your-codespace-from-last-time","title":"If you still have your codespace from last time","text":"<p>If you go to the core page of https://github.com/codespaces and you see an existing codespace from the morning, you can restart it using the (...) menu</p> <p></p> <p>If you don't have one, recreate it (see below)</p>"},{"location":"1_2d_handson_gcp.html#create-your-codespace-and-connect-to-it","title":"Create your codespace and connect to it","text":"<p>Go to https://github.com/fchouteau/isae-cloud-computing-codespace</p> <p></p> <ul> <li>Click on the top left corner for a new codespace</li> <li>It should launch a browser with a vscode</li> <li>Launch a terminal using the top right menu</li> </ul> <p>If that does not work, go to https://github.com/github/codespaces-blank and create a codespace from there</p> <p></p> <p>You should arrive to a VScode instance</p> <p></p> <p>If you go to the core page of https://github.com/codespaces you should see your codespace running</p> <p></p>"},{"location":"1_2d_handson_gcp.html#3-configure-the-google-cloud-sdk","title":"3. Configure the Google Cloud SDK","text":"<p>The Google Cloud SDK is required to interact with GCP from the command line.</p> <p>Already installed</p> <p>If you are using the course Codespace, the Google Cloud SDK should be already installed. You can verify this by running <code>gcloud --version</code>.</p> <p>Warning</p> <p>If the gcloud is not installed in the codespace, run <code>bash install.sh</code> to install it.</p> <p>Run <code>gcloud init</code> in your terminal to configure the SDK with your account:</p> <pre><code>gcloud init\n</code></pre> <p>You should see a terminal with a link. Click on the link, login with your Google account, then copy the token back to your Codespace.</p> <p></p> <p>Your GitHub Codespace is now configured with your Google Cloud Platform credentials.</p> Reference: Installing gcloud CLI (not needed for this hands-on) <p>If you need to install the Google Cloud SDK on your own machine later:</p> <ul> <li>Ubuntu / Debian: https://cloud.google.com/sdk/docs/install#deb</li> <li>Other Linux: https://cloud.google.com/sdk/docs/install#linux</li> <li>MacOS: https://cloud.google.com/sdk/docs/install#mac</li> <li>Windows: https://cloud.google.com/sdk/docs/install#windows</li> </ul>"},{"location":"1_2d_handson_gcp.html#4-my-first-google-compute-engine-instance","title":"4. My first Google Compute Engine Instance","text":"<p>Learning objectives</p> <ul> <li>Create a VM using the GCP Console</li> <li>Connect via SSH from your Codespace</li> <li>Understand cloud elasticity (resizing VMs)</li> <li>Transfer files between local machine and cloud VM</li> </ul> <p>First, we will make our first steps by creating a compute engine instance (a VM) using the console, connecting to it via SSH, interacting with it, uploading some files, and we will shut it down and make the magic happen by resizing it.</p> <ul> <li>What is google cloud compute engine ? try to describe it with your own words</li> </ul>"},{"location":"1_2d_handson_gcp.html#4a-creating-my-vm-using-the-console-the-gui","title":"4a. Creating my VM using the console (the GUI)","text":"<ul> <li> <p>Create your VM from the google cloud interface : Go to this link and follow the \"CONSOLE\" instruction</p> </li> <li> <p>Create an instance with the following parameters</p> <ul> <li>type: e2-standard-2</li> <li>zone: europe-west9-a (Paris)</li> <li>os: ubuntu 22.04 x86</li> <li>boot disk size: 10 GB</li> <li>boot disk type: pd-standard</li> </ul> </li> <li>Give it a name of your choice (that you can remember)</li> <li>DO NOT SHUT IT DOWN for now</li> </ul> CLI equivalent <p>If you were using the command line, you would run:</p> <pre><code>gcloud compute instances create {name} \\\n  --project={your-project} \\\n  --zone=europe-west9-a \\\n  --machine-type=e2-standard-2 \\\n  --image-family=ubuntu-2204-lts \\\n  --image-project=ubuntu-os-cloud \\\n  --boot-disk-size=10GB \\\n  --boot-disk-type=pd-standard\n</code></pre>"},{"location":"1_2d_handson_gcp.html#4b-connecting-to-ssh","title":"4b. Connecting to SSH","text":"<ul> <li> <p>Connect to SSH from the GitHub Codespace</p> Solution <p><code>gcloud compute ssh ${MACHINE-NAME}</code></p> </li> </ul> Why use gcloud compute ssh instead of regular ssh? <p><code>gcloud compute ssh</code> is a wrapper around standard SSH that:</p> <ol> <li>Automatically finds your VM - no need to know the IP address</li> <li>Handles SSH key management - creates and distributes keys automatically</li> <li>Works through IAP - can connect even without public IP (more secure)</li> </ol> <p>Under the hood, it's essentially: <code>ssh -i ~/.ssh/google_compute_engine username@&lt;vm-ip&gt;</code></p> <ul> <li> <p>Check available disk space</p> Solution <p><code>df -h</code></p> </li> <li> <p>Check the OS name</p> Solution <p><code>cat /etc/os-release</code></p> </li> <li> <p>Check the CPU model</p> Solution <p><code>cat /proc/cpuinfo</code></p> </li> <li> <p>Check the number of cores available and the RAM</p> Solution <p><code>htop</code></p> </li> </ul>"},{"location":"1_2d_handson_gcp.html#4c-the-magic-of-redimensioning-vms","title":"4c. The magic of redimensioning VMs","text":"<ul> <li>Shutdown the VM (from the web browser), check the previous codelab to see how to do it</li> <li>Select it and click on EDIT</li> <li>Change the machine type to <code>e2-standard-4</code> (link to documentation)</li> <li>Relaunch it, reconnect to it and try to check using <code>htop</code> the number of cores &amp; RAM available</li> <li>Note : If you run <code>cat /proc/cpuinfo</code> again you will see that you are running on the same hardware !</li> </ul> <p>Magic isn't it ? </p> <p>Note: If you had any files and specific configuration, they would still be here !</p>"},{"location":"1_2d_handson_gcp.html#4d-transfering-files-from-the-computer-or-codespaces-to-this-machine","title":"4d. Transfering files from the computer (or codespaces) to this machine","text":"<ul> <li>We will use the terminal to transfer some files from* your computer (or codespaces) to** this machine,</li> <li> <p>If you use cloud shell you can do it as well : create a dummy file in cloud shell</p> </li> <li> <p>Follow this link to learn how to use the gcloud CLI tool to transfer files to your instance</p> </li> <li> <p>For experts, it's possible to do it manually using rsync from ssh or scp</p> </li> <li> <p>Transfer some files to your <code>/home/${USER}</code> directory</p> </li> <li> <p>List them from your instance (<code>ls</code>)</p> </li> </ul> <p>How do we do the opposite ?</p> <p>See section 5.</p>"},{"location":"1_2d_handson_gcp.html#4e-persistent-ssh-sessions-with-tmux","title":"4e. Persistent SSH sessions with TMUX","text":"<ul> <li>Connect to your GCE instance using SSH from the codespace</li> <li>Question: What happens if you start a long computation and disconnect ?</li> <li>Check that tmux is installed on the remote instance (run <code>tmux</code>). if not install it</li> <li>Follow this tutorial: https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/</li> <li>To check you have understood you should be able to:<ul> <li>Connect to your remote instance with ssh</li> <li>Start a tmux session</li> <li>Launch a process (for example <code>htop</code>) inside it</li> <li>Detach from the session (<code>CTRL+B</code> then type <code>:detach</code>)</li> <li>Kill the ssh connection</li> <li>Connect again</li> <li><code>tmux attach</code> to your session</li> <li>Your process should still be here !</li> </ul> </li> </ul> <p>Congratulations :)</p> <p>What You Learned</p> <ul> <li>VM creation: Using both the Console (GUI) and CLI to create compute instances</li> <li>SSH access: Connecting to remote VMs with <code>gcloud compute ssh</code></li> <li>Cloud elasticity: Resizing VMs on-the-fly (more CPU/RAM without losing data)</li> <li>File transfer: Using <code>gcloud compute scp</code> to move files to/from VMs</li> <li>Persistent sessions: Using <code>tmux</code> to keep processes running after disconnection</li> </ul>"},{"location":"1_2d_handson_gcp.html#5-interacting-with-google-cloud-storage","title":"5. Interacting with Google Cloud Storage","text":"<p>Learning objectives</p> <ul> <li>Understand object storage concepts (buckets, objects)</li> <li>Use the GCP Console to manage storage</li> <li>Use the <code>gcloud storage</code> CLI for file operations</li> <li>Configure VM access scopes for Cloud Storage</li> </ul> <p>Here we will discover Google Cloud Storage, upload some files from your computer and download them from your instance in the cloud.</p> <ul> <li> <p>What is Google Cloud Storage? Try to describe it in your own words.</p> </li> <li> <p>Use this tutorial to upload something from your computer to Google Cloud Storage from the web browser (DO NOT DELETE THE FILES YET)</p> </li> </ul> <p>Now we will download it using the <code>gcloud storage</code> CLI. Here's the documentation.</p> <p>Common commands:</p> <pre><code># List buckets\ngcloud storage ls\n\n# List contents of a bucket\ngcloud storage ls gs://your-bucket-name\n\n# Upload a file\ngcloud storage cp local-file.txt gs://your-bucket-name/\n\n# Download a file\ngcloud storage cp gs://your-bucket-name/remote-file.txt ./\n</code></pre> <ul> <li>List the content of the bucket you just created (if you deleted it previously, create a new one)</li> <li>Upload a file to a bucket</li> <li>Download a file from a bucket</li> </ul> <p>Optional: What if we want to do the same from the GCE instance?</p> <ul> <li> <p>Now go back to your machine</p> </li> <li> <p>Try to list buckets, download and upload files</p> </li> <li> <p>Is it possible?</p> </li> <li> <p>If not, it's because you have to allow the instance to access Google Cloud Storage</p> </li> <li> <p>Shutdown the VM and edit it (like we did when we resized the instance)</p> </li> <li> <p>Check \"access scopes\", select \"set access for each API\", and select \"storage / admin\"</p> </li> </ul> What are access scopes? <p>Access scopes are a legacy way to control what GCP APIs a VM can access.</p> <p>By default, VMs cannot access Cloud Storage. You must explicitly grant access:</p> <ul> <li><code>storage-ro</code> - Read-only access to buckets</li> <li><code>storage-rw</code> - Read and write access</li> <li><code>storage-full</code> - Full control (including delete, set ACLs)</li> </ul> <p>Modern best practice: Use service accounts with IAM roles instead.</p> <ul> <li> <p>Now restart your machine, connect back to it. You should be able to upload to Google Cloud Storage now</p> </li> <li> <p>You can delete the VM as well, we will not use it</p> </li> </ul> <p>What You Learned</p> <ul> <li>Object storage vs file storage: Buckets contain objects (files), accessed via URLs</li> <li>Cloud Storage CLI: Using <code>gcloud storage ls</code>, <code>cp</code> for uploads/downloads</li> <li>Access scopes: VMs need explicit permissions to access other GCP services</li> <li>Data pipeline pattern: Upload data to storage, process on VMs, store results back</li> </ul>"},{"location":"1_2d_handson_gcp.html#6-end-to-end-example-preview","title":"6. End-to-End Example (Preview)","text":"<p>What you'll learn on Day 2</p> <p>In the next session, you'll combine all these skills into a complete ML workflow:</p> <ul> <li>Transfer training code to a VM</li> <li>Run ML training on remote hardware</li> <li>Upload model weights to Cloud Storage</li> <li>Download results to your development machine</li> </ul> <p>Here's the workflow you'll implement:</p> <p></p> <p>This pattern\u2014compute on VMs, store on GCS, develop locally\u2014is how real ML teams work. You now have all the building blocks; Day 2 puts them together.</p> <p>What You Learned</p> <ul> <li>VM lifecycle: Create, use, resize, delete cloud resources on-demand</li> <li>SSH access: Connect to remote machines with <code>gcloud compute ssh</code></li> <li>File transfer: Move data with <code>gcloud compute scp</code></li> <li>Cloud Storage: Durable object storage accessible from anywhere</li> <li>tmux: Keep processes running after disconnection</li> </ul>"},{"location":"1_2d_handson_gcp.html#7-important-cleaning-up","title":"7. IMPORTANT: Cleaning up","text":"<p>Warning</p> <ul> <li>DELETE ALL THE BUCKETS YOU CREATED</li> <li>DELETE ALL THE GCP INSTANCES YOU CREATED</li> <li>SHUTDOWN YOUR CODESPACE</li> </ul> <p>How to shutdown Codespaces:</p> <p></p> <ul> <li>Click on \"Stop codespace\" to shut it down (you \"pay\" for the disk with your free credits)</li> <li>Click on \"Delete\" to remove it completely</li> </ul>"},{"location":"1_2d_handson_gcp.html#8-advanced-infrastructure-as-code","title":"8. Advanced - Infrastructure as Code","text":"<p>So far, you have been creating VMs manually (via Console or CLI). In production environments, infrastructure is defined in configuration files that can be versioned and automated.</p> <p>This tutorial will guide you through Google Cloud Deployment Manager, which is a way to deploy Google Compute Engine instances using configuration files.</p> <p>Modern alternatives</p> <p>Google Cloud Deployment Manager is GCP-specific. In practice, many teams use Terraform which works across all cloud providers (AWS, Azure, GCP).</p> <ul> <li>Don't forget to adapt machine configurations and zone to your use case (see above)</li> </ul> <p>If you run this, don't forget to clean everything up afterwards!</p>"},{"location":"1_2d_handson_gcp.html#9-advanced-managed-databases","title":"9. Advanced - Managed Databases","text":"<ul> <li> <p>You have just completed a class on SQL databases</p> </li> <li> <p>Here are the managed SQL services of Google Cloud</p> </li> </ul> <p>Question</p> <p>Can you describe what it is? What do you pay to Google? How much does it cost? What is a \"managed service\" in cloud vocabulary?</p> <ul> <li>If you still have some code to interact with a database, you can try launching one here and redoing your exercises</li> </ul>"},{"location":"1_2d_handson_gcp.html#10-advanced-deep-learning-vms","title":"10. Advanced - Deep Learning VMs","text":"<p>Google Cloud offers Deep Learning VMs\u2014pre-configured VM images with ML frameworks (PyTorch, TensorFlow) and Jupyter pre-installed.</p> <p>Instead of installing dependencies manually, you can create a VM from these images:</p> <pre><code>gcloud compute instances create my-dlvm \\\n    --image-family=pytorch-latest-cpu \\\n    --image-project=deeplearning-platform-release \\\n    --machine-type=n1-standard-2\n</code></pre> <p>You'll use this approach in Day 2's workflow exercise. For more details: Deep Learning VM documentation.</p>"},{"location":"1_3a_containers.html","title":"From Virtualisation to Containerisation","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_3b_handson_docker.html","title":"Docker Hands-on","text":"<p>Note</p> <p>If you are lost, <code>docker system prune</code> will remove dangling images and stopped containers.</p>"},{"location":"1_3b_handson_docker.html#0-abstract","title":"0. Abstract","text":"<p>Abstract</p> <p>In this hands-on, you will discover the basics of Docker and learn to manipulate images and containers.</p> <p>You should be inside the Github Codespace you created and have google cloud SDK installed in it</p> <p>If not, refer to the previous tutorial and do step 2 and 3</p> <p>This codespace has everything you need, including docker</p> <p>If you want to do everything from your linux machine you can install docker but I don't recommend it for now</p>"},{"location":"1_3b_handson_docker.html#1-manipulating-docker-for-the-1st-time","title":"1. Manipulating docker for the 1st time","text":"<p>Source: https://github.com/docker/labs</p> <p>To get started, let's run the following in our terminal:</p> <pre><code>$ docker pull alpine\n</code></pre> <p>The <code>pull</code> command fetches the alpine image from the Docker registry and saves it in our system. You can use the <code>docker images</code> command to see a list of all images on your system. <pre><code>$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nalpine                 latest              c51f86c28340        4 weeks ago         1.109 MB\nhello-world             latest              690ed74de00f        5 months ago        960 B\n</code></pre></p>"},{"location":"1_3b_handson_docker.html#11-docker-run","title":"1.1 Docker Run","text":"<p>Great! Let's now run a Docker container based on this image. To do that you are going to use the <code>docker run</code> command.</p> <pre><code>$ docker run alpine ls -l\ntotal 48\ndrwxr-xr-x    2 root     root          4096 Mar  2 16:20 bin\ndrwxr-xr-x    5 root     root           360 Mar 18 09:47 dev\ndrwxr-xr-x   13 root     root          4096 Mar 18 09:47 etc\ndrwxr-xr-x    2 root     root          4096 Mar  2 16:20 home\ndrwxr-xr-x    5 root     root          4096 Mar  2 16:20 lib\n......\n......\n</code></pre> <p>What happened? Behind the scenes, a lot of stuff happened. When you call <code>run</code>,</p> <ol> <li>The Docker client contacts the Docker daemon</li> <li>The Docker daemon checks local store if the image (alpine in this case) is available locally, and if not, downloads it from Docker Hub. (Since we have issued <code>docker pull alpine</code> before, the download step is not necessary)</li> <li>The Docker daemon creates the container and then runs a command in that container.</li> <li>The Docker daemon streams the output of the command to the Docker client</li> </ol> <p>When you run <code>docker run alpine</code>, you provided a command (<code>ls -l</code>), so Docker started the command specified and you saw the listing.</p> <p>Let's try something more exciting.</p> <p><pre><code>$ docker run alpine echo \"hello from alpine\"\nhello from alpine\n</code></pre> OK, that's some actual output. In this case, the Docker client dutifully ran the <code>echo</code> command in our alpine container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast!</p> <p>Try another command.</p> <pre><code>docker run alpine /bin/sh\n</code></pre> <p>Wait, nothing happened! Is that a bug? Well, no. These interactive shells will exit after running any scripted commands, unless they are run in an interactive terminal - so for this example to not exit, you need to <code>docker run -it alpine /bin/sh</code>.</p> <p>You are now inside the container shell and you can try out a few commands like <code>ls -l</code>, <code>uname -a</code> and others. Exit out of the container by giving the <code>exit</code> command.</p> <p>Ok, now it's time to see the <code>docker ps</code> command. The <code>docker ps</code> command shows you all containers that are currently running.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre> <p>Since no containers are running, you see a blank line. Let's try a more useful variant: <code>docker ps -a</code></p> <pre><code>$ docker ps -a\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\n36171a5da744        alpine              \"/bin/sh\"                5 minutes ago       Exited (0) 2 minutes ago                        fervent_newton\na6a9d46d0b2f        alpine             \"echo 'hello from alp\"    6 minutes ago       Exited (0) 6 minutes ago                        lonely_kilby\nff0a5c3750b9        alpine             \"ls -l\"                   8 minutes ago       Exited (0) 8 minutes ago                        elated_ramanujan\nc317d0a9e3d2        hello-world         \"/hello\"                 34 seconds ago      Exited (0) 12 minutes ago                       stupefied_mcclintock\n</code></pre> <p>What you see above is a list of all containers that you ran. Notice that the <code>STATUS</code> column shows that these containers exited a few minutes ago. You're probably wondering if there is a way to run more than just one command in a container. Let's try that now:</p> <p><pre><code>$ docker run -it alpine /bin/sh\n/ # ls\nbin      dev      etc      home     lib      linuxrc  media    mnt      proc     root     run      sbin     sys      tmp      usr      var\n/ # uname -a\nLinux 97916e8cb5dc 4.4.27-moby #1 SMP Wed Oct 26 14:01:48 UTC 2016 x86_64 Linux\n</code></pre> Running the <code>run</code> command with the <code>-it</code> flags attaches us to an interactive tty in the container. Now you can run as many commands in the container as you want. Take some time to run your favorite commands.</p> <p>That concludes a whirlwind tour of the <code>docker run</code> command which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about <code>run</code>, use <code>docker run --help</code> to see a list of all flags it supports. As you proceed further, we'll see a few more variants of <code>docker run</code>.</p>"},{"location":"1_3b_handson_docker.html#12-terminology","title":"1.2 Terminology","text":"<p>In the last section, you saw a lot of Docker-specific jargon which might be confusing to some. So before you go further, let's clarify some terminology that is used frequently in the Docker ecosystem.</p> <ul> <li>Images - The file system and configuration of our application which are used to create containers. To find out more about a Docker image, run <code>docker inspect alpine</code>. In the demo above, you used the <code>docker pull</code> command to download the alpine image. When you executed the command <code>docker run hello-world</code>, it also did a <code>docker pull</code> behind the scenes to download the hello-world image.</li> <li>Containers - Running instances of Docker images \u2014 containers run the actual applications. A container includes an application and all of its dependencies. It shares the kernel with other containers, and runs as an isolated process in user space on the host OS. You created a container using <code>docker run</code> which you did using the alpine image that you downloaded. A list of running containers can be seen using the <code>docker ps</code> command.</li> <li>Docker daemon - The background service running on the host that manages building, running and distributing Docker containers.</li> <li>Docker client - The command line tool that allows the user to interact with the Docker daemon.</li> <li>Docker Hub - A registry of Docker images, where you can find trusted and enterprise ready containers, plugins, and Docker editions. You'll be using this later in this tutorial.</li> </ul> <p>What You Learned</p> <ul> <li>docker pull: Download images from a registry (Docker Hub)</li> <li>docker run: Create and start a container from an image</li> <li>docker ps: List running containers (<code>-a</code> for all, including stopped)</li> <li>Interactive mode: Use <code>-it</code> flags to get a shell inside a container</li> <li>Key vocabulary: Images (blueprints) vs Containers (running instances)</li> </ul>"},{"location":"1_3b_handson_docker.html#2-webapps-with-docker","title":"2. Webapps with Docker","text":"<p>Source: https://github.com/docker/labs</p> <p>Great! So you have now looked at <code>docker run</code>, played with a Docker container and also got the hang of some terminology. Armed with all this knowledge, you are now ready to get to the real stuff \u2014 deploying web applications with Docker.</p>"},{"location":"1_3b_handson_docker.html#21-run-a-static-website-in-a-container","title":"2.1 Run a static website in a container","text":"<p>Note: Code for this section is in this repo in the website directory</p> <p>Let's start by taking baby-steps. First, we'll use Docker to run a static website in a container. The website is based on an existing image. We'll pull a Docker image from Docker Hub, run the container, and see how easy it is to set up a web server.</p> <p>The image that you are going to use is a single-page website that was already created for this demo and is available on Docker Hub as <code>dockersamples/static-site</code>. You can download and run the image directly in one go using <code>docker run</code> as follows.</p> <pre><code>docker run -d dockersamples/static-site\n</code></pre> <p>Files:</p> <ul> <li>Dockerfile</li> <li>hello_docker.html</li> </ul> <p>Note: The <code>-d</code> flag enables detached mode, which detaches the running container from the terminal/shell and returns your prompt after the container starts.</p> <p>So, what happens when you run this command?</p> <p>Since the image doesn't exist on your Docker host, the Docker daemon first fetches it from the registry and then runs it as a container.</p> <p>Now that the server is running, do you see the website? What port is it running on? And more importantly, how do you access the container directly from our host machine?</p> <p>Actually, you probably won't be able to answer any of these questions yet! \u263a In this case, the client didn't tell the Docker Engine to publish any of the ports, so you need to re-run the <code>docker run</code> command to add this instruction.</p> <p>Let's re-run the command with some new flags to publish ports and pass your name to the container to customize the message displayed. We'll use the -d option again to run the container in detached mode.</p> <p>First, stop the container that you have just launched. In order to do this, we need the container ID.</p> <p>Since we ran the container in detached mode, we don't have to launch another terminal to do this. Run <code>docker ps</code> to view the running containers.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES\na7a0e504ca3e        dockersamples/static-site   \"/bin/sh -c 'cd /usr/\"   28 seconds ago      Up 26 seconds       80/tcp, 443/tcp     stupefied_mahavira\n</code></pre> <p>Check out the <code>CONTAINER ID</code> column. You will need to use this <code>CONTAINER ID</code> value, a long sequence of characters, to identify the container you want to stop, and then to remove it. The example below provides the <code>CONTAINER ID</code> on our system; you should use the value that you see in your terminal.</p> <pre><code>$ docker stop a7a0e504ca3e\n$ docker rm   a7a0e504ca3e\n</code></pre> <p>Note: A cool feature is that you do not need to specify the entire <code>CONTAINER ID</code>. You can just specify a few starting characters and if it is unique among all the containers that you have launched, the Docker client will intelligently pick it up.</p> <p>Now, let's launch a container in detached mode as shown below:</p> <pre><code>$ docker run --name static-site -e AUTHOR=\"Your Name\" -d -p 8080:80 dockersamples/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n</code></pre> <p>In the above command:</p> <ul> <li><code>-d</code> will create a container with the process detached from our terminal</li> <li><code>-p 8080:80</code> maps port 8080 on your host to port 80 in the container (format: <code>host:container</code>)</li> <li><code>-e</code> is how you pass environment variables to the container</li> <li><code>--name</code> allows you to specify a container name</li> <li><code>AUTHOR</code> is the environment variable name and <code>Your Name</code> is the value that you can pass</li> </ul> <p>You can verify the port mapping by running the <code>docker port</code> command:</p> <pre><code>$ docker port static-site\n80/tcp -&gt; 0.0.0.0:8080\n</code></pre> <p>If you are on Codespace, the port 8080 should be automatically detected. You can also manually forward port 8080 to access the website.</p> <p>Open <code>http://localhost:8080</code> in your browser to see the website.</p> <p>You can also run a second webserver at the same time on a different port:</p> <pre><code>$ docker run --name static-site-2 -e AUTHOR=\"Your Name\" -d -p 8888:80 dockersamples/static-site\n</code></pre> <p></p> <p>To deploy this on a real server you would just need to install Docker, and run the above <code>docker</code> command (as in this case you can see the <code>AUTHOR</code> is Docker which we passed as an environment variable).</p> <p>Now that you've seen how to run a webserver inside a Docker container, how do you create your own Docker image? This is the question we'll explore in the next section.</p> <p>But first, let's stop and remove the containers since you won't be using them anymore.</p> <pre><code>$ docker stop static-site\n$ docker rm static-site\n</code></pre> <p>Let's use a shortcut to remove the second site:</p> <pre><code>$ docker rm -f static-site-2\n</code></pre> <p>Run <code>docker ps</code> to make sure the containers are gone.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre>"},{"location":"1_3b_handson_docker.html#22-docker-images","title":"2.2 Docker Images","text":"<p>In this section, let's dive deeper into what Docker images are. You will build your own image, use that image to run an application locally.</p> <p>Docker images are the basis of containers. In the previous example, you pulled the dockersamples/static-site image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the <code>docker images</code> command.</p> <pre><code>$ docker images\nREPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE\ndockersamples/static-site   latest              92a386b6e686        2 hours ago         190.5 MB\nnginx                       latest              af4b3d7d5401        3 hours ago         190.5 MB\npython                      3.11                1c32174fd534        14 hours ago        1.01 GB\npostgres                    16                  88d845ac7a88        14 hours ago        432 MB\nredis                       7-alpine            4f5f397d4b7c        7 days ago          41 MB\nalpine                      3.18                70c557e50ed6        8 days ago          7.8 MB\n</code></pre> <p>Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The <code>TAG</code> refers to a particular snapshot of the image and the <code>ID</code> is the corresponding unique identifier for that image.</p> <p>For simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to <code>latest</code>.</p> <p>For example you could pull a specific version of <code>ubuntu</code> image as follows:</p> <pre><code>$ docker pull ubuntu:22.04\n</code></pre> <p>If you do not specify the version number of the image then, as mentioned, the Docker client will default to a version named <code>latest</code>.</p> <p>So for example, the <code>docker pull</code> command given below will pull an image named <code>ubuntu:latest</code>:</p> <pre><code>$ docker pull ubuntu\n</code></pre> <p>To get a new Docker image you can either get it from a registry (such as Docker Hub) or create your own. There are hundreds of thousands of images available on Docker Hub. You can also search for images directly from the command line using <code>docker search</code>.</p> <p>An important distinction with regard to images is between base images and child images.</p> <ul> <li> <p>Base images are images that have no parent images, usually images with an OS like ubuntu, alpine or debian.</p> </li> <li> <p>Child images are images that build on base images and add additional functionality.</p> </li> </ul> <p>Another key concept is the idea of official images and user images. (Both of which can be base images or child images.)</p> <ul> <li> <p>Official images are Docker sanctioned images. Docker, Inc. sponsors a dedicated team that is responsible for reviewing and publishing all Official Repositories content. This team works in collaboration with upstream software maintainers, security experts, and the broader Docker community. These are not prefixed by an organization or user name. In the list of images above, the <code>python</code>, <code>node</code>, <code>alpine</code> and <code>nginx</code> images are official (base) images. To find out more about them, check out the Official Images Documentation.</p> </li> <li> <p>User images are images created and shared by users like you. They build on base images and add additional functionality. Typically these are formatted as <code>user/image-name</code>. The <code>user</code> value in the image name is your Docker Hub user or organization name.</p> </li> </ul>"},{"location":"1_3b_handson_docker.html#23-create-your-first-image","title":"2.3 Create your first image","text":"<p>Note: The code for this section is in this repository in the flask-app directory.</p> <p>Now that you have a better understanding of images, it's time to create your own. Our goal here is to create an image that sandboxes a small Flask application.</p> <p>The goal of this exercise is to create a Docker image which will run a Flask app.</p> <p>We'll do this by first pulling together the components for a random cat picture generator built with Python Flask, then dockerizing it by writing a Dockerfile. Finally, we'll build the image, and then run it.</p> <ul> <li>Create a Python Flask app that displays random cat pix</li> <li>Write a Dockerfile</li> <li>Build the image</li> <li>Run your image</li> <li>Dockerfile commands summary</li> </ul>"},{"location":"1_3b_handson_docker.html#231-create-a-python-flask-app-that-displays-random-cat-pix","title":"2.3.1 Create a Python Flask app that displays random cat pix","text":"<p>For the purposes of this workshop, we've created a fun little Python Flask app that displays a random cat <code>.gif</code> every time it is loaded - because, you know, who doesn't like cats?</p> <p>Start by creating a directory called <code>flask-app</code> where we'll create the following files:</p> <ul> <li>app.py</li> <li>requirements.txt</li> <li>templates/index.html</li> <li>Dockerfile</li> </ul> <p>Make sure to <code>cd flask-app</code> before you start creating the files, because you don't want to start adding a whole bunch of other random files to your image.</p>"},{"location":"1_3b_handson_docker.html#apppy","title":"app.py","text":"<p>Create the app.py with the following content:</p> <pre><code>from flask import Flask, render_template\nimport random\n\napp = Flask(__name__)\n\n# list of cat images\nimages = [\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif1.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif2.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif3.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif4.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif5.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif6.gif\",\n    ]\n\n@app.route('/')\ndef index():\n    url = random.choice(images)\n    return render_template('index.html', url=url)\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\")\n</code></pre>"},{"location":"1_3b_handson_docker.html#requirementstxt","title":"requirements.txt","text":"<p>In order to install the Python modules required for our app, we need to create a file called requirements.txt and add the following line to that file:</p> <pre><code>flask\ntyper\n</code></pre>"},{"location":"1_3b_handson_docker.html#templatesindexhtml","title":"templates/index.html","text":"<p>Create a directory called <code>templates</code> and create an index.html file in that directory with the following content in it:</p> <pre><code>&lt;html&gt;\n  &lt;head&gt;\n    &lt;style type=\"text/css\"&gt;\n      body {\n        background: black;\n        color: white;\n      }\n      div.container {\n        max-width: 500px;\n        margin: 100px auto;\n        border: 20px solid white;\n        padding: 10px;\n        text-align: center;\n      }\n      h4 {\n        text-transform: uppercase;\n      }\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"container\"&gt;\n      &lt;h4&gt;Cat Gif of the day&lt;/h4&gt;\n      &lt;img src=\"{{url}}\" /&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"1_3b_handson_docker.html#232-write-a-dockerfile","title":"2.3.2 Write a Dockerfile","text":"<p>We want to create a Docker image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, we will build our own Python image based on Alpine. We'll do that using a Dockerfile.</p> <p>A Dockerfile is a text file that contains a list of commands that the Docker daemon calls while creating an image. The Dockerfile contains all the information that Docker needs to know to run the app \u2014 a base Docker image to run from, location of your project code, any dependencies it has, and what commands to run at start-up. It is a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own Dockerfiles.</p> <ol> <li>Create a file called Dockerfile, and add content to it as described below.</li> </ol> <p>We'll start by specifying our base image, using the <code>FROM</code> keyword:</p> <pre><code>FROM alpine:3.18\n</code></pre> <p>Note : If you use the latest version of alpine which is 3.20, follow this tutorial to handle an error you might be getting</p> <ol> <li>The next step usually is to write the commands of copying the files and installing the dependencies. But first we will install the Python pip package to the alpine linux distribution. This will not just install the pip package but any other dependencies too, which includes the python interpreter. Add the following RUN command next.</li> </ol> <pre><code>RUN apk add --update py-pip\n</code></pre> <ol> <li>Let's add the files that make up the Flask Application.</li> </ol> <p>Install all Python requirements for our app to run. This will be accomplished by adding the lines:</p> <pre><code>COPY requirements.txt /usr/src/app/\nRUN pip install --no-cache-dir -r /usr/src/app/requirements.txt\n</code></pre> <p>Copy the files you have created earlier into our image by using COPY  command.</p> <pre><code>COPY app.py /usr/src/app/\nCOPY templates/index.html /usr/src/app/templates/\n</code></pre> <ol> <li>Specify the port number which needs to be exposed. Since our flask app is running on <code>5000</code> that's what we'll expose.</li> </ol> <pre><code>EXPOSE 5000\n</code></pre> <ol> <li>The last step is the command for running the application which is simply - <code>python ./app.py</code>. Use the CMD command to do that:</li> </ol> <pre><code>CMD [\"python\", \"/usr/src/app/app.py\"]\n</code></pre> <p>The primary purpose of <code>CMD</code> is to tell the container which command it should run by default when it is started.</p> <ol> <li>Verify your Dockerfile.</li> </ol> <p>Our <code>Dockerfile</code> is now ready. This is how it looks:</p> <pre><code># our base image\nFROM alpine:3.18\n\n# Install python and pip\nRUN apk add --update py-pip\n\n# install Python modules needed by the Python app\nCOPY requirements.txt /usr/src/app/\nRUN pip install --no-cache-dir -r /usr/src/app/requirements.txt\n\n# copy files required for the app to run\nCOPY app.py /usr/src/app/\nCOPY templates/index.html /usr/src/app/templates/\n\n# tell the port number the container should expose\nEXPOSE 5000\n\n# run the application\nCMD [\"python\", \"/usr/src/app/app.py\"]\n</code></pre>"},{"location":"1_3b_handson_docker.html#233-build-the-image","title":"2.3.3 Build the image","text":"<p>Now that you have your <code>Dockerfile</code>, you can build your image. The <code>docker build</code> command does the heavy-lifting of creating a docker image from a <code>Dockerfile</code>.</p> <p>The <code>docker build</code> command is quite simple - it takes an optional tag name with the <code>-t</code> flag, and the location of the directory containing the <code>Dockerfile</code> - the <code>.</code> indicates the current directory:</p> <p><code>docker build -t myfirstapp:1.0 .</code></p> <pre><code>$ docker build -t myfirstapp:1.0 .\n[+] Building 45.2s (10/10) FINISHED\n =&gt; [internal] load build definition from Dockerfile                       0.0s\n =&gt; [internal] load .dockerignore                                          0.0s\n =&gt; [internal] load metadata for docker.io/library/alpine:3.18             1.2s\n =&gt; [1/5] FROM docker.io/library/alpine:3.18                               2.1s\n =&gt; [2/5] RUN apk add --update py-pip                                     15.3s\n =&gt; [3/5] COPY requirements.txt /usr/src/app/                              0.0s\n =&gt; [4/5] RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt 20.1s\n =&gt; [5/5] COPY app.py /usr/src/app/                                        0.0s\n =&gt; [6/5] COPY templates/index.html /usr/src/app/templates/                0.0s\n =&gt; exporting to image                                                     1.2s\n =&gt; =&gt; writing image sha256:2f7357a0805d...                                0.0s\n =&gt; =&gt; naming to docker.io/library/myfirstapp:1.0                          0.0s\n</code></pre> <p>If you don't have the <code>alpine:3.18</code> image, the client will first pull the image and then create your image. Therefore, your output on running the command will look different from mine. If everything went well, your image should be ready! Run <code>docker images</code> and see if your image (<code>myfirstapp:1.0</code>) shows.</p>"},{"location":"1_3b_handson_docker.html#234-run-your-image","title":"2.3.4 Run your image","text":"<p>The next step in this section is to run the image and see if it actually works.</p> <pre><code>$ docker run -p 8888:5000 --name myfirstapp myfirstapp:1.0\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n</code></pre> <p>Head over to <code>http://localhost:8888</code> and your app should be live. If you're using Codespace, make sure port 8888 is forwarded.</p> <p></p> <p>Hit the Refresh button in the web browser to see a few more cat images.</p>"},{"location":"1_3b_handson_docker.html#235-dockerfile-commands-summary","title":"2.3.5 Dockerfile commands summary","text":"<p>Here's a quick summary of the few basic commands we used in our Dockerfile.</p> <ul> <li> <p><code>FROM</code> starts the Dockerfile. It is a requirement that the Dockerfile must start with the <code>FROM</code> command. Images are created in layers, which means you can use another image as the base image for your own. The <code>FROM</code> command defines your base layer. As arguments, it takes the name of the image. Optionally, you can add the Docker Hub username and image version, in the format <code>username/imagename:version</code>.</p> </li> <li> <p><code>RUN</code> is used to build up the Image you're creating. For each <code>RUN</code> command, Docker will run the command then create a new layer of the image. This way you can roll back your image to previous states easily. The syntax for a <code>RUN</code> instruction is to place the full text of the shell command after the <code>RUN</code> (e.g., <code>RUN mkdir /user/local/foo</code>). This will automatically run in a <code>/bin/sh</code> shell. You can define a different shell like this: <code>RUN /bin/bash -c 'mkdir /user/local/foo'</code></p> </li> <li> <p><code>COPY</code> copies local files into the container.</p> </li> <li> <p><code>CMD</code> defines the commands that will run on the Image at start-up. Unlike a <code>RUN</code>, this does not create a new layer for the Image, but simply runs the command. There can only be one <code>CMD</code> per a Dockerfile/Image. If you need to run multiple commands, the best way to do that is to have the <code>CMD</code> run a script. <code>CMD</code> requires that you tell it where to run the command, unlike <code>RUN</code>. So example <code>CMD</code> commands would be:</p> </li> </ul> <pre><code>  CMD [\"python\", \"./app.py\"]\n\n  CMD [\"/bin/bash\", \"echo\", \"Hello World\"]\n</code></pre> <ul> <li><code>EXPOSE</code> creates a hint for users of an image which ports provide services. It is included in the information which  can be retrieved via <code>$ docker inspect &lt;container-id&gt;</code>.     </li> </ul> <p>Note: The <code>EXPOSE</code> command does not actually make any ports accessible to the host! Instead, this requires  publishing ports by means of the <code>-p</code> flag when using <code>$ docker run</code>.  </p> <ul> <li><code>WORKDIR</code> sets the working directory for any <code>RUN</code>, <code>CMD</code>, <code>ENTRYPOINT</code>, <code>COPY</code> and <code>ADD</code> instructions that follow it. It's good practice to set this explicitly rather than relying on the default (<code>/</code>).</li> </ul> <p>Note: To push your image to a registry, use the <code>docker push</code> command (not a Dockerfile instruction). Example: <code>docker push username/imagename:tag</code></p> <p>Note: If you want to learn more about Dockerfiles, check out Best practices for writing Dockerfiles.</p>"},{"location":"1_3b_handson_docker.html#236-build-context","title":"2.3.6 Build context","text":"<p>When you run <code>docker build</code>, Docker sends all files in the current directory to the Docker daemon. This is called the build context. Keep your build directory clean to speed up builds.</p> <p>Tip</p> <p>For details on excluding files with <code>.dockerignore</code>, see the Advanced section at the end.</p> <p>What You Learned</p> <ul> <li>Running web apps: Use <code>-d</code> for detached mode, <code>-p host:container</code> for port mapping</li> <li>Building images: Write a <code>Dockerfile</code>, then <code>docker build -t name:tag .</code></li> <li>Dockerfile instructions: <code>FROM</code>, <code>RUN</code>, <code>COPY</code>, <code>EXPOSE</code>, <code>CMD</code> and their purposes</li> <li>Build context &amp; .dockerignore: Control what gets sent to the Docker daemon</li> </ul>"},{"location":"1_3b_handson_docker.html#3-running-cli-apps-with-docker-passing-data-at-runtime","title":"3. Running CLI Apps with Docker: Passing Data at Runtime","text":"<p>In Section 2, you built a static website where all content was baked into the image. Now we'll package CLI applications that process data passed at runtime - the pattern used for ML training.</p> Aspect Static Web (Section 2) CLI App (Section 3) Content Baked into image at build time Passed at runtime via volumes Rebuild needed? Yes, for any content change No - same image, different data Use case Web servers, APIs Training scripts, data processing"},{"location":"1_3b_handson_docker.html#31-volume-mounting-sharing-files-with-containers","title":"3.1 Volume Mounting: Sharing Files with Containers","text":"<p>The <code>-v</code> flag connects a folder on your host to a path inside the container, which allows to share files at runtime.</p> <pre><code>docker run -v /path/on/host:/path/in/container my-image\n</code></pre> <p>For example with <code>./</code> in host and <code>/app/</code> in container,</p> <pre><code>Host Machine              Container\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ./configs/   \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 /app/configs \u2502\n\u2502 ./outputs/   \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 /app/outputs \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Files are shared bidirectionally (changes appear on both sides)</li> <li>Container paths must be absolute (<code>/app/configs</code>, not <code>configs</code>)</li> <li>No image rebuild needed - just run with different volumes</li> </ul> <p>Now Let's Build This</p> <p>In the following exercises, you'll create a training CLI, package it in Docker, and run it with different config files without rebuilding the image.</p>"},{"location":"1_3b_handson_docker.html#32-a-local-cli-application","title":"3.2 A local CLI application","text":"<ul> <li>In the <code>docker/app.py</code>, I provided you with the following code :</li> </ul> <pre><code>import time\nfrom pathlib import Path\nfrom typing import Annotated, Optional\n\nimport typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef say_hello(name: str):\n    typer.echo(f\"Hello {name}\")\n\n\n@app.command()\ndef run_training(\n    config: Annotated[\n        Path,\n        typer.Option(\n            exists=True,\n            file_okay=True,\n            dir_okay=False,\n            writable=False,\n            readable=True,\n            resolve_path=True,\n        ),\n    ],\n    output_dir: Annotated[\n        Path,\n        typer.Option(\n            dir_okay=True,\n            writable=True,\n            readable=True,\n            resolve_path=True,\n            file_okay=False,\n        ),\n    ],\n):\n    text = config.read_text()\n    print(f\"Config file contents: {text}\")\n\n    print(f\"Running training in {output_dir}...\")\n\n    time.sleep(10)\n\n    output_dir.mkdir(exist_ok=True,parents=True)\n\n    with open(output_dir / \"results.txt\", \"w\") as f:\n        f.write(\"Training successful !\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <ul> <li>You can the application locally using <code>pip install typer</code>then <code>python app.py say-hello {my name}</code> or <code>python app.py run-training --config {my config} --output-dir {somewhere}</code></li> </ul>"},{"location":"1_3b_handson_docker.html#33-packaging-it-in-a-dockerfile","title":"3.3 Packaging it in a Dockerfile","text":"<p>We will now package it in a docker file</p> <ul> <li>A provided a <code>Dockerfile</code> in the <code>docker/</code> folder. Compared to the previous one, here are the key differences:</li> <li>Replaced <code>CMD [\"python\", \"/usr/src/app/app.py\"]</code></li> <li>With <code>ENTRYPOINT [\"python\", \"/usr/src/app/app.py\"]</code></li> </ul> <p>CMD vs ENTRYPOINT</p> <ul> <li>CMD: Arguments in <code>docker run myimage arg1</code> replace the entire CMD</li> <li>ENTRYPOINT: Arguments in <code>docker run myimage arg1</code> are appended to ENTRYPOINT</li> </ul> <p>For CLI apps, use ENTRYPOINT so users pass arguments naturally: <pre><code>docker run myimage say-hello Alice   # Runs: python app.py say-hello Alice\n</code></pre></p> <p>See also: CMD vs ENTRYPOINT deep dive</p> <ul> <li>Build the docker image for that application using what you learned.</li> </ul>"},{"location":"1_3b_handson_docker.html#34-running-the-cli-app","title":"3.4 Running the CLI App","text":"<ul> <li>Now to run the CLI you just have to pass the arguments when running the docker <code>docker run --rm {your image} {your args}</code>. Try it with <code>docker run --rm {your image} say-hello {your name}</code></li> </ul> <p>Now you know how to pass arguments to CLI applications in docker containers.</p>"},{"location":"1_3b_handson_docker.html#35-mounting-volumes","title":"3.5 Mounting volumes","text":"<p>Warning</p> <p>once you have built your container and it works, don't rebuild it again ! We will test the volume mounting options now</p> <ul> <li>In order to pass a config file, or data to your docker, you need to make it available to your docker. To do that, we have to mount volumes</li> </ul> <p>Create a dummy config file (<code>config.txt</code>) in a folder (e.g., <code>configs/</code>) and an empty <code>outputs/</code> folder. Then mount them when you run the container:</p> <pre><code>docker run --rm \\\n  -v $(pwd)/configs:/app/configs \\\n  -v $(pwd)/outputs:/app/outputs \\\n  {your image} \\\n  run-training --config /app/configs/config.txt --output-dir /app/outputs/\n</code></pre> <p>Container Paths vs Host Paths</p> <p>The paths after <code>run-training</code> are container paths (where Docker sees the files), not your local paths:</p> <ul> <li><code>$(pwd)/configs</code> \u2192 your local folder (host)</li> <li><code>/app/configs</code> \u2192 where Docker mounts it (container)</li> </ul> <p>Your CLI receives <code>/app/configs/config.txt</code> because that's where the file exists inside the container.</p> <p>You've Mastered This When:</p> <ol> <li>You run training with different config files without rebuilding the image</li> <li>Output files appear in your local <code>outputs/</code> folder after the container exits</li> <li>You understand why the CLI uses <code>/app/configs</code> (container path) not <code>./configs</code> (host path)</li> </ol>"},{"location":"1_3b_handson_docker.html#36-troubleshooting-volume-mounts","title":"3.6 Troubleshooting Volume Mounts","text":"Problem Likely Cause Solution \"File not found\" in container Wrong mount path Verify with <code>docker run --rm -v $(pwd)/configs:/app/configs myimage ls /app/configs</code> Changes don't appear on host Volume not mounted Check <code>-v</code> syntax is <code>host:container</code>, not reversed Permission denied Host dir permissions Run <code>chmod -R 755 ./outputs</code> or check file ownership <p>What You Learned</p> <ul> <li>ENTRYPOINT vs CMD: <code>ENTRYPOINT</code> for CLI apps that accept arguments</li> <li>Volume mounting: <code>-v host_path:container_path</code> connects local folders to containers</li> <li>Path distinction: Host paths for <code>-v</code>, container paths for your CLI</li> <li>ML training pattern: Mount config/data in, mount outputs out, keep image unchanged</li> </ul>"},{"location":"1_3b_handson_docker.html#4-containers-registry","title":"4. Containers Registry","text":"<p>Remember Container Registries? Here are some explainers</p> <p>The main container registry is dockerhub, https://hub.docker.com/</p> <p>All docker engines that have access to the internet have access to this main hub, and this is where we pulled our base images from before</p> <p>Example, the Python Image</p> <p>Google Cloud has an Artifacts Registry per project, which ensures the docker images you build are accessible for the people who have access to your project only.</p> <p>We will follow this tutorial to push our images to artifact registry</p> <ul> <li> <p>First, create a Docker Artifact registry using this tutorial, example <code>fch-sdd2425-artifacts-registry</code> (that's mine, name it with your name). Set the repository in <code>multi-region/europe</code></p> </li> <li> <p>Pushing our images requires authenticating, <code>gcloud auth configure-docker europe-docker.pkg.dev</code></p> </li> <li> <p>Pushing our images requires tagging them in a specific way : <code>europe-docker.pkg.dev/${PROJECT_ID}/${REPO_ID}/${IMAGE}:${TAG}</code></p> </li> <li> <p>Use the docker cli to tag your previous <code>myfirstapp</code> image to the right namespace</p> </li> </ul> <p><code>docker tag myfirstapp:1.0 europe-docker.pkg.dev/${PROJECT_ID}/${REPO_ID}/myfirstapp:1.0</code></p> <ul> <li>Upload it on container registry </li> </ul> <p><code>docker push europe-docker.pkg.dev/${PROJECT_ID}/${REPO_ID}/[IMAGE]:[TAG]</code></p> <p>Hint</p> <p>to get your project id: <code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)</code> to get your artifact repository id look at this page, you can get your project id this way as well</p> <ul> <li>Go to your artifact registry https://console.cloud.google.com/artifacts, you should see your docker image :)</li> </ul> <p>What You Learned</p> <ul> <li>Container registries: Central storage for Docker images (like GitHub for code)</li> <li>Docker Hub vs private registries: Public images vs organization-private images</li> <li>GCP Artifact Registry: Google's managed container registry service</li> <li>Tag and push workflow: <code>docker tag</code> to rename, <code>docker push</code> to upload</li> <li>Image naming convention: <code>registry/project/image:tag</code> format</li> </ul>"},{"location":"1_3b_handson_docker.html#what-youve-learned","title":"What You've Learned","text":"<p>Core Docker Skills</p> <p>You now know how to:</p> <ul> <li>Pull and run containers from Docker Hub</li> <li>Build custom images with Dockerfiles</li> <li>Mount volumes to pass data at runtime</li> <li>Push images to a container registry</li> </ul>"},{"location":"1_3b_handson_docker.html#whats-next","title":"What's Next","text":"<p>In Day 2, you'll combine Docker with GCP to deploy containerized ML models to the cloud.</p> <p>The following sections are Advanced - useful reference material that you can explore after completing the core content.</p>"},{"location":"1_3b_handson_docker.html#advanced-pre-built-data-science-environments","title":"Advanced - Pre-built Data Science Environments","text":"<p>Docker images like Jupyter Docker Stacks provide fully configured data science environments. These images combine:</p> <ul> <li>Volume mounting: Connect your local notebooks to the container</li> <li>Port forwarding: Access Jupyter in your browser</li> </ul> <p>Example usage: <pre><code>docker run --rm -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/scipy-notebook\n</code></pre></p> <p>This pattern\u2014pulling a pre-configured image and mounting your data\u2014is useful for:</p> <ul> <li>Standardizing team development environments</li> <li>Quickly onboarding new team members</li> <li>Testing dependency upgrades without affecting your system</li> </ul> <p>Note</p> <p>These images can be large (several GB). Try this at home rather than during class to avoid network congestion.</p>"},{"location":"1_3b_handson_docker.html#advanced-docker-compose-preview","title":"Advanced - Docker Compose Preview","text":"<p>Docker Compose manages multi-container applications with a single YAML file. Instead of running multiple <code>docker run</code> commands, you define your entire stack in <code>docker-compose.yml</code>:</p> <pre><code>services:\n  web:\n    image: myapp:1.0\n    ports:\n      - \"8080:5000\"\n  redis:\n    image: redis:alpine\n</code></pre> <p>Then start everything with: <pre><code>docker compose up\n</code></pre></p> <p>You'll use Docker Compose on Day 2 when deploying a multi-container ML application (API backend + web frontend). For now, just know it exists.</p> <p>Further reading: Docker Compose Getting Started</p>"},{"location":"1_3b_handson_docker.html#advanced-cloud-build","title":"Advanced - Cloud Build","text":"<p>Google Cloud Build builds Docker images in the cloud, without needing Docker installed locally. This is useful for CI/CD pipelines.</p> <p>Instead of: <pre><code>docker build -t myimage .\ndocker push europe-docker.pkg.dev/...\n</code></pre></p> <p>You run: <pre><code>gcloud builds submit --tag europe-docker.pkg.dev/$PROJECT_ID/$REPO/myimage .\n</code></pre></p> <p>Cloud Build pulls your code, builds the image on Google's infrastructure, and pushes to Artifact Registry automatically. You'll see this in action if you continue with GCP-based ML deployments.</p>"},{"location":"1_3b_handson_docker.html#advanced-going-further","title":"Advanced - Going Further","text":"<p>https://container.training/</p>"},{"location":"1_3b_handson_docker.html#advanced-docker-build-context-and-dockerignore","title":"Advanced - Docker Build Context and .dockerignore","text":"<p>When you run <code>docker build</code>, Docker sends all files in the current directory (the build context) to the Docker daemon. This can slow down builds and bloat your image if unnecessary files are included.</p>"},{"location":"1_3b_handson_docker.html#the-build-context","title":"The build context","text":"<p>The build context is everything in the directory you specify when running <code>docker build</code>. For example:</p> <pre><code>docker build -t myapp:1.0 .\n</code></pre> <p>The <code>.</code> means \"use the current directory as the build context\". Docker will send all files in this directory to the daemon, which can be slow if you have large files (datasets, model weights, etc.).</p>"},{"location":"1_3b_handson_docker.html#using-dockerignore","title":"Using .dockerignore","text":"<p>Create a <code>.dockerignore</code> file to exclude files from the build context. This works like <code>.gitignore</code>:</p> <pre><code># .dockerignore example\n.git\n__pycache__\n*.pyc\n.env\ndata/\nmodels/\n*.tar.gz\nnode_modules/\n.vscode/\n</code></pre> <p>Tip</p> <p>Always create a <code>.dockerignore</code> file in your project. It speeds up builds and prevents accidentally including sensitive files (like <code>.env</code> with secrets) in your image.</p>"},{"location":"1_3b_handson_docker.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"1_3b_handson_docker.html#debugging-containers","title":"Debugging Containers","text":"<p>When things go wrong, these commands will help you troubleshoot:</p>"},{"location":"1_3b_handson_docker.html#view-container-logs","title":"View container logs","text":"<pre><code># See logs from a running or stopped container\ndocker logs &lt;container-name-or-id&gt;\n\n# Follow logs in real-time (like tail -f)\ndocker logs -f &lt;container-name-or-id&gt;\n\n# Show only the last 100 lines\ndocker logs --tail 100 &lt;container-name-or-id&gt;\n</code></pre>"},{"location":"1_3b_handson_docker.html#get-a-shell-inside-a-running-container","title":"Get a shell inside a running container","text":"<pre><code># Open an interactive shell in a running container\ndocker exec -it &lt;container-name-or-id&gt; /bin/sh\n\n# Or use bash if available\ndocker exec -it &lt;container-name-or-id&gt; /bin/bash\n</code></pre>"},{"location":"1_3b_handson_docker.html#inspect-container-details","title":"Inspect container details","text":"<pre><code># See detailed container configuration (ports, volumes, env vars, etc.)\ndocker inspect &lt;container-name-or-id&gt;\n\n# See resource usage (CPU, memory)\ndocker stats &lt;container-name-or-id&gt;\n</code></pre>"},{"location":"1_3b_handson_docker.html#debug-a-failed-build","title":"Debug a failed build","text":"<p>If your build fails, you can run a container from the last successful layer:</p> <pre><code># Find the last successful layer ID in the build output, then:\ndocker run -it &lt;layer-id&gt; /bin/sh\n</code></pre> <p>Note</p> <p>These debugging skills are essential when deploying ML models. Container logs are often the first place to check when a model serving endpoint fails.</p>"},{"location":"1_4a_gcp_workflow.html","title":"GCP Compute Workflow Hands-on","text":""},{"location":"1_4a_gcp_workflow.html#0-overview","title":"0. Overview","text":"<p>Abstract</p> <p>In this hands-on, you will learn a core data science workflow: offloading compute-intensive training to a cloud VM while keeping your development environment lightweight.</p> <p>You will:</p> <ul> <li>Create a Deep Learning VM on Google Compute Engine</li> <li>Run PyTorch training remotely via SSH</li> <li>Upload results to Google Cloud Storage</li> <li>Pull results back to your Codespace for analysis</li> </ul> <p>Cost Warning</p> <p>GCP resources cost money. A <code>n1-standard-2</code> VM costs ~$0.10/hour.</p> <p>CRITICAL: Always verify training is complete before deleting your VM, then delete it immediately. Leaving a VM running overnight wastes money and your free credits.</p> <p>Tip</p> <p>When replacing <code>{something}</code> in commands, don't include the brackets. Write <code>yourname</code>, not <code>{yourname}</code>.</p>"},{"location":"1_4a_gcp_workflow.html#the-problem","title":"The Problem","text":"<p>Your GitHub Codespace has 2 CPU cores and 8GB RAM. You need to train a CNN on MNIST - doable locally, but imagine this was a ResNet on ImageNet. How do data scientists handle compute-intensive training when their laptop or dev environment isn't enough?</p> <p>The solution: Offload training to a cloud VM, save results to cloud storage, pull back for analysis.</p>"},{"location":"1_4a_gcp_workflow.html#the-workflow","title":"The Workflow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    CODESPACE    \u2502         \u2502    GCE VM       \u2502         \u2502      GCS        \u2502\n\u2502  (your laptop)  \u2502         \u2502 (training box)  \u2502         \u2502 (cloud storage) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                           \u2502                           \u2502\n         \u2502  1. Create VM             \u2502                           \u2502\n         \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502                           \u2502\n         \u2502                           \u2502                           \u2502\n         \u2502  2. SSH + run training    \u2502                           \u2502\n         \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502                           \u2502\n         \u2502                           \u2502  3. Upload model+metrics  \u2502\n         \u2502                           \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502\n         \u2502                           \u2502                           \u2502\n         \u2502  4. Delete VM             \u2502                           \u2502\n         \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502                           \u2502\n         \u2502                           \u2502                           \u2502\n         \u2502  5. Pull results from GCS                             \u2502\n         \u2502 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n         \u2502                           \u2502                           \u2502\n         \u2502  6. Analyze in Jupyter    \u2502                           \u2502\n         \u25bc                           \u2502                           \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502                           \u2502\n   \u2502 Notebook \u2502                      \u2502                           \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502                           \u2502\n</code></pre> <p>Here's the same workflow as a diagram:</p> <p></p>"},{"location":"1_4a_gcp_workflow.html#prerequisites","title":"Prerequisites","text":"<p>Before starting, you should have:</p> <ul> <li>Completed the GCP Hands-on (gcloud configured, project access)</li> <li>Access to the GitHub Codespace</li> </ul>"},{"location":"1_4a_gcp_workflow.html#1-key-concepts","title":"1. Key Concepts","text":"<p>Before diving in, let's understand the two GCP services we'll use.</p> <p>What is Google Compute Engine (GCE)?</p> <p>Google Compute Engine (GCE) is GCP's virtual machine service. You can:</p> <ul> <li>Create VMs with specific CPU, RAM, and disk configurations</li> <li>Choose from pre-built images (including Deep Learning VMs with PyTorch pre-installed)</li> <li>Pay only for the time the VM is running (billed per second)</li> <li>Access VMs via SSH from anywhere</li> </ul> <p>Think of it as renting a computer in Google's data center. You control it entirely, but Google handles the physical hardware, networking, and maintenance.</p> <p>Key difference from your laptop: GCE VMs are ephemeral\u2014designed to be created, used, and deleted. Don't treat them as permanent storage.</p> <p>What is Google Cloud Storage (GCS)?</p> <p>Google Cloud Storage (GCS) is GCP's object storage service. You can:</p> <ul> <li>Store any file (models, datasets, logs) in \"buckets\"</li> <li>Access files from anywhere (VMs, your laptop, other GCP services)</li> <li>Pay only for storage used and data transferred</li> <li>Files persist until you delete them\u2014independent of any VM</li> </ul> <p>Think of it as a cloud hard drive with unlimited capacity. Unlike a VM's disk, GCS data survives VM deletion.</p> <p>Why use GCS for ML? Your trained model and metrics live in GCS, so you can delete the expensive VM immediately after training and still have all your results.</p>"},{"location":"1_4a_gcp_workflow.html#2-setup","title":"2. Setup","text":""},{"location":"1_4a_gcp_workflow.html#set-environment-variables","title":"Set Environment Variables","text":"<p>First, configure your environment with a unique run identifier:</p> <pre><code># Project and shared bucket (already created by instructor)\nexport PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\nexport GCS_BUCKET=\"gs://isae-sdd-de-2526\"\n\n# Unique run identifier: username + timestamp\nexport RUN_ID=\"${USER}-$(date +%Y%m%d-%H%M%S)\"\nexport INSTANCE_NAME=\"training-vm-${RUN_ID}\"\nexport GCS_OUTPUT=\"${GCS_BUCKET}/runs/${RUN_ID}\"\n\n# Display configuration\necho \"Project: ${PROJECT_ID}\"\necho \"Run ID: ${RUN_ID}\"\necho \"Results will be saved to: ${GCS_OUTPUT}\"\n</code></pre>"},{"location":"1_4a_gcp_workflow.html#verify-bucket-access","title":"Verify Bucket Access","text":"<p>The shared bucket is pre-created by the instructor. Verify you can access it:</p> <pre><code># The bucket already exists - verify you can access it\ngcloud storage ls ${GCS_BUCKET}\n</code></pre> <p>Checkpoint 1</p> <p>You should see the bucket contents listed (may be empty or have other students' runs). If you get a permission error, check with the instructor.</p>"},{"location":"1_4a_gcp_workflow.html#3-create-vm-and-run-training","title":"3. Create VM and Run Training","text":""},{"location":"1_4a_gcp_workflow.html#create-the-deep-learning-vm","title":"Create the Deep Learning VM","text":"<pre><code>gcloud compute instances create ${INSTANCE_NAME} \\\n    --zone=europe-west1-b \\\n    --image-family=pytorch-latest-cpu \\\n    --image-project=deeplearning-platform-release \\\n    --machine-type=n1-standard-2 \\\n    --scopes=storage-rw \\\n    --boot-disk-size=50GB\n</code></pre> <p>Key flags explained:</p> Flag Purpose <code>--image-family=pytorch-latest-cpu</code> Pre-installed PyTorch, no GPU needed <code>--scopes=storage-rw</code> VM can write to GCS without extra auth <code>--machine-type=n1-standard-2</code> 2 vCPUs, 7.5 GB RAM <p>Checkpoint 2</p> <p>VM creation takes ~60 seconds. Run <code>gcloud compute instances list</code> - your VM should show <code>RUNNING</code> status.</p>"},{"location":"1_4a_gcp_workflow.html#copy-the-training-script","title":"Copy the Training Script","text":"<pre><code>gcloud compute scp train.py ${INSTANCE_NAME}:~ --zone=europe-west1-b\n</code></pre>"},{"location":"1_4a_gcp_workflow.html#ssh-into-the-vm-and-run-training","title":"SSH into the VM and Run Training","text":"<pre><code>gcloud compute ssh ${INSTANCE_NAME} --zone=europe-west1-b\n</code></pre> <p>Once inside the VM, run training with <code>nohup</code> to keep it running even if SSH disconnects:</p> <pre><code># Use nohup to ensure training continues even if SSH disconnects\nnohup python train.py \\\n    --epochs 5 \\\n    --output-gcs ${GCS_OUTPUT} \\\n    &gt; training.log 2&gt;&amp;1 &amp;\n\n# Monitor progress\ntail -f training.log\n# Press Ctrl+C to stop watching (training continues in background)\n</code></pre> <p>Why nohup?</p> <p>If your SSH connection drops (network hiccup, laptop sleeps), the training process continues. Without nohup, a disconnection would kill your training mid-run.</p> <p>For longer interactive sessions, you could also use tmux which you learned about earlier.</p> <p>Checkpoint 3</p> <p>Training is complete when you see <code>Results uploaded to gs://...</code> in the log. Exit SSH with <code>exit</code>.</p>"},{"location":"1_4a_gcp_workflow.html#4-verify-completion-cleanup-retrieve-results","title":"4. Verify Completion, Cleanup &amp; Retrieve Results","text":""},{"location":"1_4a_gcp_workflow.html#verify-training-completed","title":"Verify Training Completed","text":"<p>CRITICAL: Verify before deleting!</p> <p>Never delete your VM until you've confirmed results are in GCS. Once deleted, any data on the VM is gone forever.</p> <p>From your Codespace (not the VM), check if results exist:</p> <pre><code># Check if results exist in GCS (training script uploads on completion)\ngcloud storage ls ${GCS_OUTPUT}/metrics.json\n</code></pre> <p>If you see the file listed, training is complete. If you get \"One or more URLs matched no objects\", training is still running.</p> <p>Wait for completion (if needed):</p> <pre><code># Poll until metrics.json appears (check every 30 seconds)\nwhile ! gcloud storage ls ${GCS_OUTPUT}/metrics.json 2&gt;/dev/null; do\n    echo \"$(date): Training still in progress...\"\n    sleep 30\ndone\necho \"Training complete! Results ready in ${GCS_OUTPUT}\"\n</code></pre> <p>Checkpoint 4a</p> <p>You should see <code>gs://isae-sdd-de-2526/runs/yourname-YYYYMMDD-HHMMSS/metrics.json</code> listed.</p>"},{"location":"1_4a_gcp_workflow.html#list-all-results","title":"List All Results","text":"<pre><code>gcloud storage ls ${GCS_OUTPUT}/\n</code></pre> <p>Expected output: <pre><code>gs://isae-sdd-de-2526/runs/yourname-20260111-143052/model.pt\ngs://isae-sdd-de-2526/runs/yourname-20260111-143052/metrics.json\n</code></pre></p>"},{"location":"1_4a_gcp_workflow.html#pull-results-to-codespace","title":"Pull Results to Codespace","text":"<pre><code>mkdir -p ./results/${RUN_ID}\ngcloud storage cp ${GCS_OUTPUT}/* ./results/${RUN_ID}/\n</code></pre> <p>Checkpoint 5</p> <p>Run <code>ls ./results/${RUN_ID}/</code> - you should see <code>model.pt</code> and <code>metrics.json</code>.</p>"},{"location":"1_4a_gcp_workflow.html#delete-the-vm","title":"Delete the VM","text":"<p>Only after verifying results are in GCS!</p> <pre><code>gcloud compute instances delete ${INSTANCE_NAME} --zone=europe-west1-b --quiet\n</code></pre> <p>Checkpoint 4b</p> <p>Run <code>gcloud compute instances list</code> - your VM should no longer appear.</p>"},{"location":"1_4a_gcp_workflow.html#5-analyze-results-in-jupyter","title":"5. Analyze Results in Jupyter","text":"<p>Open the <code>analyze.ipynb</code> notebook in your Codespace.</p>"},{"location":"1_4a_gcp_workflow.html#cell-1-load-metrics","title":"Cell 1: Load Metrics","text":"<pre><code>import json\nimport os\nimport matplotlib.pyplot as plt\n\n# Find the most recent run (or specify RUN_ID manually)\nresults_dir = \"./results\"\nruns = sorted(os.listdir(results_dir))\nrun_id = runs[-1]  # Most recent run\nrun_path = os.path.join(results_dir, run_id)\nprint(f\"Analyzing run: {run_id}\")\n\nwith open(os.path.join(run_path, \"metrics.json\")) as f:\n    metrics = json.load(f)\n\nprint(f\"Final accuracy: {metrics['test_accuracy'][-1]:.2f}%\")\nprint(f\"Final loss: {metrics['test_loss'][-1]:.4f}\")\n</code></pre>"},{"location":"1_4a_gcp_workflow.html#cell-2-plot-training-curves","title":"Cell 2: Plot Training Curves","text":"<pre><code>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.plot(metrics['epoch'], metrics['test_loss'])\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Test Loss over Training')\n\nax2.plot(metrics['epoch'], metrics['test_accuracy'])\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Test Accuracy over Training')\n\nplt.tight_layout()\nplt.savefig(os.path.join(run_path, 'training_curves.png'))\nplt.show()\n</code></pre>"},{"location":"1_4a_gcp_workflow.html#cell-3-inspect-model","title":"Cell 3: Inspect Model","text":"<pre><code>import torch\nmodel_state = torch.load(os.path.join(run_path, 'model.pt'), map_location='cpu')\nprint(f\"Model layers: {list(model_state.keys())}\")\nprint(f\"Total parameters: {sum(p.numel() for p in model_state.values()):,}\")\n</code></pre> <p>Checkpoint 6 (Final)</p> <p>You should see training curves plotted and ~93-98% accuracy after 5 epochs.</p>"},{"location":"1_4a_gcp_workflow.html#6-summary","title":"6. Summary","text":"<p>What You Learned</p> <ul> <li>Deep Learning VMs: Pre-configured GCE images with ML frameworks ready</li> <li>Remote training pattern: SSH \u2192 run \u2192 upload to GCS \u2192 delete VM</li> <li>GCS as artifact storage: Durable, accessible from anywhere, decoupled from compute</li> <li>Cloud workflow: Compute is ephemeral, storage is persistent</li> </ul>"},{"location":"1_4a_gcp_workflow.html#7-bonus-automation-script","title":"7. Bonus: Automation Script","text":"<p>Every command you ran could be a script. Here's what a fully automated <code>train_remote.sh</code> looks like:</p> <pre><code>#!/bin/bash\nset -e  # Exit on any error\n\n# Configuration\nGCS_BUCKET=\"gs://isae-sdd-de-2526\"\nRUN_ID=\"${USER}-$(date +%Y%m%d-%H%M%S)\"\nINSTANCE_NAME=\"training-vm-${RUN_ID}\"\nZONE=\"europe-west1-b\"\nGCS_OUTPUT=\"${GCS_BUCKET}/runs/${RUN_ID}\"\nEPOCHS=${1:-5}  # Default 5 epochs, or pass as argument\n\necho \"==&gt; Run ID: ${RUN_ID}\"\necho \"==&gt; Results will be saved to: ${GCS_OUTPUT}\"\n\necho \"==&gt; Creating VM ${INSTANCE_NAME}...\"\ngcloud compute instances create ${INSTANCE_NAME} \\\n    --zone=${ZONE} \\\n    --image-family=pytorch-latest-cpu \\\n    --image-project=deeplearning-platform-release \\\n    --machine-type=n1-standard-2 \\\n    --scopes=storage-rw \\\n    --boot-disk-size=50GB\n\n# Wait for SSH to become available (more reliable than sleep)\necho \"==&gt; Waiting for SSH to become available...\"\nuntil gcloud compute ssh ${INSTANCE_NAME} --zone=${ZONE} --command=\"echo ready\" 2&gt;/dev/null; do\n    echo \"    Waiting for VM...\"\n    sleep 5\ndone\n\necho \"==&gt; Copying training script...\"\ngcloud compute scp train.py ${INSTANCE_NAME}:~ --zone=${ZONE}\n\necho \"==&gt; Starting training in background...\"\ngcloud compute ssh ${INSTANCE_NAME} --zone=${ZONE} --command \\\n    \"nohup python train.py --epochs ${EPOCHS} --output-gcs ${GCS_OUTPUT} &gt; training.log 2&gt;&amp;1 &amp;\"\n\n# Poll for completion by checking if metrics.json exists in GCS\necho \"==&gt; Waiting for training to complete...\"\nwhile ! gcloud storage ls ${GCS_OUTPUT}/metrics.json &amp;&gt; /dev/null; do\n    echo \"    $(date): Training in progress...\"\n    sleep 30\ndone\n\necho \"==&gt; Training complete! Deleting VM...\"\ngcloud compute instances delete ${INSTANCE_NAME} --zone=${ZONE} --quiet\n\necho \"==&gt; Done! Results at ${GCS_OUTPUT}\"\necho \"==&gt; Download with: gcloud storage cp ${GCS_OUTPUT}/* ./results/${RUN_ID}/\"\n</code></pre> <p>Now launching a training run is one command:</p> <pre><code>./train_remote.sh 10  # Train for 10 epochs\n</code></pre> <p>This is how reproducible ML pipelines start. Tools like Terraform and Pulumi formalize this further, letting you version control your infrastructure alongside your code.</p>"},{"location":"1_4a_gcp_workflow.html#8-looking-ahead-mlops","title":"8. Looking Ahead: MLOps","text":"<p>You just did manually what MLOps platforms automate:</p> What You Did Production Tools Create VM, run training Vertex AI (GCP), SageMaker (AWS) Track metrics.json MLflow, Weights &amp; Biases Script the workflow Kubeflow, Airflow <p>The pattern you learned is the foundation\u2014cloud platforms add automation on top.</p>"},{"location":"1_4b_docker_workflow.html","title":"Bureau d'\u00e9tudes Docker - Build, Ship, Run","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_4b_docker_workflow.html#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this workshop, you will be able to:</p> <ol> <li>Understand Dockerfile structure - Read and explain what each instruction in a Dockerfile does</li> <li>Build Docker images - Use <code>docker build</code> with proper tagging conventions for a container registry</li> <li>Push/Pull from a registry - Authenticate to Google Artifact Registry and exchange images with teammates</li> <li>Run containers with configuration - Map ports, set environment variables, and understand container lifecycle</li> </ol> <p></p> <p>Warning</p> <p>Please read all the text in the question before executing the step-by-step instructions because there might be help or indications after the instructions.</p>"},{"location":"1_4b_docker_workflow.html#prerequisites","title":"Prerequisites","text":"<p>Before starting this workshop, you should have:</p> <ul> <li>Set up your GitHub Codespace (from the previous session)</li> <li>Installed and configured the <code>gcloud</code> CLI (from the previous session)</li> <li>Manipulated Google Cloud Storage in the previous session</li> </ul> <p>Tip</p> <p>Use Google Chrome without any ad blockers if you have any issues, or use the local VSCode + Codespace extension. If you have connectivity issues, switch your wi-fi connection between eduroam (preferred), isae-edu, or a 4G hotspot.</p>"},{"location":"1_4b_docker_workflow.html#team-setup","title":"Team Setup","text":"<p>You should be in teams of 2-5 people.</p> <p>Each team member picks a different mascot and remembers it:</p> <ul> <li>\ud83d\udc08 cat</li> <li>\ud83d\udc15 dog</li> <li>\ud83d\udc7d yoda</li> <li>\ud83e\udd89 owl</li> <li>\ud83d\udc3c panda</li> </ul> <p>Find a group name for your team - you will need it for tagging your images.</p> <p>Shared Container Registry</p> <p>A container registry has been pre-created for this workshop. You will push and pull images from:</p> <pre><code>europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker\n</code></pre>"},{"location":"1_4b_docker_workflow.html#phase-1-build","title":"Phase 1: BUILD","text":""},{"location":"1_4b_docker_workflow.html#11-start-development-environment-github-codespace","title":"1.1 - Start Development Environment (GitHub Codespace)","text":"<p>Launch your GitHub Codespace from the preconfigured repository: https://github.com/fchouteau/isae-cloud-computing-codespace</p> <p>Ensure that the <code>gcloud</code> CLI is installed and configured (run <code>gcloud init</code> like last time).</p>"},{"location":"1_4b_docker_workflow.html#12-get-resources-from-google-cloud-storage","title":"1.2 - Get Resources from Google Cloud Storage","text":"<p>From your GitHub Codespace, download your mascot resources. First, set your mascot:</p> <pre><code>export MASCOT=&lt;your chosen mascot&gt;  # cat, dog, owl, panda, or yoda\n</code></pre> <p>Only download your mascot (no cheating - this will cause confusion later!)</p> <pre><code>gsutil -m cp -r gs://fchouteau-isae-cloud/be/${MASCOT} .\ncd ${MASCOT}\n</code></pre> <p>You should see a file structure like this:</p> <pre><code>yoda/\n\u251c\u2500\u2500 app.py\n\u251c\u2500\u2500 AUTHOR.txt\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 favicon.ico\n\u251c\u2500\u2500 imgs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.gif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2.gif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 3.gif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4.gif\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 5.gif\n\u2514\u2500\u2500 template.html.jinja2\n</code></pre>"},{"location":"1_4b_docker_workflow.html#13-build-your-docker-image","title":"1.3 - Build your Docker image","text":"<p>Question</p> <ul> <li>Look at the <code>Dockerfile</code> (<code>cat Dockerfile</code>), what does each instruction do?</li> <li>Look at <code>app.py</code> (<code>cat app.py</code>). What is Flask? What does this app do?</li> </ul> <p>Edit the file <code>AUTHOR.txt</code> to add your name instead of the placeholder.</p> <p>Danger</p> <p>On which port is your Flask app running? (check the <code>Dockerfile</code>) Note it carefully - you will need to communicate it to your teammates!</p> <p>Set your group name and build the image:</p> <pre><code>export GROUPNAME=&lt;your team name&gt;\n\ndocker build -t europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker/${GROUPNAME}-${MASCOT}:1.0 .\n</code></pre> <p>Verify with <code>docker images</code> - you should see your image listed.</p> <p>Question</p> <p>Describe concisely to your past self what is a <code>Docker Image</code></p> <p>Checkpoint</p> <p>Wait for instructor confirmation that everyone has built their image before moving to the SHIP phase.</p>"},{"location":"1_4b_docker_workflow.html#phase-2-ship","title":"Phase 2: SHIP","text":""},{"location":"1_4b_docker_workflow.html#21-authenticate-to-the-container-registry","title":"2.1 - Authenticate to the Container Registry","text":"<p>Configure Docker to authenticate with Google Artifact Registry:</p> <pre><code>gcloud auth configure-docker europe-docker.pkg.dev\n</code></pre>"},{"location":"1_4b_docker_workflow.html#22-push-your-docker-image","title":"2.2 - Push your Docker image","text":"<p>Push your image to the shared registry:</p> <pre><code>docker push europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker/${GROUPNAME}-${MASCOT}:1.0\n</code></pre> <p>Question</p> <p>What is a Container Registry? Why not just share Dockerfiles instead of images?</p> Answer <p>Pre-built images ensure consistency across environments, faster deployment (no build step needed), and no build dependencies required on the target machine.</p>"},{"location":"1_4b_docker_workflow.html#23-communicate-with-your-team","title":"2.3 - Communicate with your team","text":"<p>Share with your teammates:</p> <ul> <li>Your image name (e.g., <code>team1-yoda:1.0</code>)</li> <li>Your port number (from your Dockerfile)</li> </ul> <p>Checkpoint</p> <p>Wait for instructor confirmation that all images are pushed before moving to the RUN phase.</p>"},{"location":"1_4b_docker_workflow.html#phase-3-run","title":"Phase 3: RUN","text":""},{"location":"1_4b_docker_workflow.html#31-pull-your-teammates-image","title":"3.1 - Pull your teammate's image","text":"<p>Get the image name and port number from one of your teammates, then pull their image:</p> <pre><code>export TEAMMATE_MASCOT=&lt;teammate's mascot&gt;\n\ndocker pull europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker/${GROUPNAME}-${TEAMMATE_MASCOT}:1.0\n</code></pre> <p>Verify with <code>docker images</code> - you should see your teammate's image.</p>"},{"location":"1_4b_docker_workflow.html#32-run-the-container","title":"3.2 - Run the container","text":"<p>Run your teammate's container with port mapping and your name as an environment variable:</p> <pre><code>docker run -d -p 8080:&lt;TEAMMATE_PORT&gt; -e USER=\"&lt;your name&gt;\" \\\n  europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker/${GROUPNAME}-${TEAMMATE_MASCOT}:1.0\n</code></pre> <p>Replace <code>&lt;TEAMMATE_PORT&gt;</code> with the port number your teammate told you (from their Dockerfile).</p>"},{"location":"1_4b_docker_workflow.html#33-access-via-codespace-port-forwarding","title":"3.3 - Access via Codespace Port Forwarding","text":"<ol> <li>Open the \"Ports\" tab in VS Code (bottom panel)</li> <li>Port 8080 should appear automatically when the container starts</li> <li>Click the globe icon or \"Open in Browser\" to access the webapp</li> </ol> <p>Tip</p> <p>You can also publicly share the Codespace preview link so that other people can see your results.</p>"},{"location":"1_4b_docker_workflow.html#34-container-management","title":"3.4 - Container Management","text":"<p>Useful commands to manage your containers:</p> <pre><code>docker ps          # See running containers\ndocker logs &lt;id&gt;   # Check logs if issues\ndocker stop &lt;id&gt;   # Stop a container\n</code></pre>"},{"location":"1_4b_docker_workflow.html#success-criteria","title":"Success Criteria","text":"<p>Your running webapp must show:</p> Checkpoint What it proves Displays teammate's mascot (not your own) You pulled the correct image Shows author name (teammate's name) Image was built by your teammate Shows your name You configured the USER env var at runtime <p>Bug</p> <p>If any of the three items above are missing, use the troubleshooting guide below!</p>"},{"location":"1_4b_docker_workflow.html#troubleshooting-guide","title":"Troubleshooting Guide","text":"Problem Likely cause Fix Wrong mascot Pulled wrong image Check image name, repull Author shows placeholder Teammate forgot AUTHOR.txt Teammate rebuilds &amp; repushes Your name missing Forgot <code>-e USER=...</code> Stop container, rerun with env var Page not loading Wrong port mapping Check teammate's port, fix <code>-p</code> flag <p>Example</p> <p>Try to refresh the webpage to make more gifs appear!</p> <p>Share your result on Slack</p>"},{"location":"1_4b_docker_workflow.html#whats-next","title":"What's Next?","text":"<p>You've successfully built, shipped, and run Docker containers. But consider these limitations:</p> Limitation Why it matters Only accessible via your Codespace No public URL - others can't see your app Ephemeral When Codespace stops, container dies No scaling One container handles all requests Manual process You ran <code>docker run</code> by hand <p>Coming up: Deployment</p> <p>In the Deployment session, we'll solve these problems: deploy containers to real cloud infrastructure with public URLs, automatic restarts, and managed scaling.</p>"},{"location":"1_4b_docker_workflow.html#cleanup","title":"Cleanup","text":"<p>Stop and remove your containers (optional but good practice):</p> <pre><code># Stop running containers\ndocker ps -q | xargs -r docker stop\n\n# Remove containers\ndocker ps -aq | xargs -r docker rm\n\n# (Optional) Remove local images\ndocker images -q | xargs -r docker rmi\n</code></pre> <p>Note</p> <p>Codespaces are ephemeral, so cleanup is optional. Everything will be removed when the Codespace is deleted.</p>"},{"location":"1_4b_docker_workflow.html#congratulations","title":"Congratulations!","text":"<p>Success</p> <p>You have successfully completed the Build-Ship-Run workflow! You now understand the fundamental Docker concepts that power modern application deployment.</p>"},{"location":"1_4c_deployment_workflow.html","title":"Bureau d'etudes Cloud &amp; Docker Partie 3","text":""},{"location":"1_4c_deployment_workflow.html#31-introduction","title":"3.1 - Introduction","text":"<p>In the previous sessions, you learned Docker basics: building images from Dockerfiles, running containers, and pushing images to a registry.</p> <p>In this session, we focus on deployment: taking a containerized application and running it on a cloud VM that's accessible from the internet. You won't need to write any Dockerfile - the image is already built. The goal is to understand the deployment workflow.</p>"},{"location":"1_4c_deployment_workflow.html#32-the-application","title":"3.2 - The Application","text":"<p>The application files are already available in the <code>be-docker-deploy/</code> folder of your codespace. This is a Streamlit app that visualizes neural network activation functions - useful reference material after your Deep Learning class.</p> <p>Let's test it locally first:</p> <pre><code>cd be-docker-deploy\npip install -r requirements.txt\nstreamlit run app.py\n</code></pre> <p>Access the app on port 8501 in your codespace (click the port forwarding notification or use the Ports tab).</p> <p>Explore the interface, then quit the server with <code>Ctrl+C</code>.</p> <p>Question</p> <p>What does this application visualize? How could it help you understand neural networks?</p>"},{"location":"1_4c_deployment_workflow.html#33-the-docker-image","title":"3.3 - The Docker Image","text":"<p>For this exercise, the Docker image has been pre-built and pushed to our shared Artifact Registry. You already learned how to build and push images in the previous workshops - this session focuses purely on deployment.</p> <p>The pre-built image is:</p> <pre><code>europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker/demo-streamlit-activation-function:1.0\n</code></pre> <p>For teachers</p> <p>If you need to rebuild the image, the source files are in <code>be-docker-deploy/</code>:</p> <pre><code>cd be-docker-deploy\ndocker build -t europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker/demo-streamlit-activation-function:1.0 .\ndocker push europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker/demo-streamlit-activation-function:1.0\n</code></pre>"},{"location":"1_4c_deployment_workflow.html#34-deploy-to-gce-vm","title":"3.4 - Deploy to GCE VM","text":"<p>We'll create a VM that automatically runs our container at boot - no SSH required.</p> <p>Run this command, replacing <code>FIRSTNAME</code> with your unique identifier (e.g., <code>streamlit-vm-marie</code>):</p> <pre><code>gcloud compute instances create-with-container streamlit-vm-FIRSTNAME \\\n    --project=isae-sdd-481407 \\\n    --zone=europe-west9-a \\\n    --machine-type=e2-small \\\n    --image=projects/cos-cloud/global/images/cos-stable-117-18613-75-72 \\\n    --boot-disk-size=10GB \\\n    --container-image=europe-docker.pkg.dev/isae-sdd-481407/isae-sdd-de-2526-docker/demo-streamlit-activation-function:1.0 \\\n    --container-restart-policy=always \\\n    --tags=http-server-8501\n</code></pre> <p>Key differences from previous VM creation:</p> Flag Purpose <code>--container-image</code> Deploys the container automatically at boot <code>--image=.../cos-...</code> Uses Container-Optimized OS (lightweight, designed for containers) <code>--tags=http-server-8501</code> Labels the VM so firewall rules can target it <code>--container-restart-policy=always</code> Restarts the container if it crashes"},{"location":"1_4c_deployment_workflow.html#35-expose-to-the-web","title":"3.5 - Expose to the Web","text":"<p>By default, GCP blocks all incoming traffic to VMs. A firewall rule opens specific ports.</p> <p>A firewall rule for port 8501 already exists in our project:</p> <pre><code># This rule already exists - no need to run it\n# gcloud compute firewall-rules create allow-8501 \\\n#     --allow=tcp:8501 \\\n#     --target-tags=http-server-8501\n</code></pre> <p>How it works:</p> <ul> <li>The rule <code>allow-8501</code> permits TCP traffic on port 8501</li> <li><code>--target-tags=http-server-8501</code> applies only to VMs with that tag</li> <li>Your VM has this tag (we set it with <code>--tags</code> above)</li> </ul> <p>Get your VM's external IP:</p> <pre><code>gcloud compute instances list --filter=\"name=streamlit-vm-FIRSTNAME\"\n</code></pre> <p>Or find it in the Google Cloud Console: Compute Engine &gt; VM instances</p> <p>Access your app:</p> <p>Open your browser and go to: <code>http://EXTERNAL_IP:8501</code></p> <p>Warning</p> <p>This won't work on ISAE wifi due to network restrictions. Try eduroam or mobile data.</p>"},{"location":"1_4c_deployment_workflow.html#36-going-to-production-conceptual","title":"3.6 - Going to Production (Conceptual)","text":""},{"location":"1_4c_deployment_workflow.html#current-situation","title":"Current Situation","text":"<p>Accessing via <code>http://IP:8501</code> works for learning, but it isn't production-ready:</p> <ul> <li>IP addresses are hard to remember</li> <li>IPs can change when VMs are recreated</li> <li>No encryption (HTTP, not HTTPS)</li> <li>Non-standard port (8501 instead of 80/443)</li> </ul>"},{"location":"1_4c_deployment_workflow.html#sslipio-trick","title":"sslip.io Trick","text":"<p>For a slightly more readable URL, you can use sslip.io - a free DNS service that resolves hostnames to embedded IP addresses.</p> <p>If your VM's IP is <code>35.205.123.45</code>, you can access it via:</p> <pre><code>http://35-205-123-45.sslip.io:8501\n</code></pre> <p>How it works: sslip.io resolves <code>35-205-123-45.sslip.io</code> to <code>35.205.123.45</code>.</p> <p>You still need to specify port 8501, but at least you have a hostname instead of raw numbers.</p>"},{"location":"1_4c_deployment_workflow.html#whats-missing-for-production","title":"What's Missing for Production","text":"<p>A production setup typically looks like this:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   User   \u2502\u2500\u2500\u2500\u25b6\u2502  Load Balancer  \u2502\u2500\u2500\u2500\u25b6\u2502   VM    \u2502\u2500\u2500\u2500\u25b6\u2502 Container \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   (HTTPS:443)   \u2502    \u2502 (:8501) \u2502    \u2502  (:8501)  \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 SSL Certificate\u2502\n                \u2502 Custom Domain  \u2502\n                \u2502 (myapp.com)    \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>What we're missing:</p> <ul> <li>HTTPS: Encrypted connection (SSL/TLS certificate)</li> <li>Standard ports: 80 (HTTP) or 443 (HTTPS) instead of 8501</li> <li>Custom domain: <code>myapp.com</code> instead of IP address</li> <li>Load balancing: Distribute traffic, handle failures</li> </ul>"},{"location":"1_4c_deployment_workflow.html#why-were-not-doing-this","title":"Why We're Not Doing This","text":"<p>Setting up production infrastructure requires:</p> <ul> <li>A domain name (cost + DNS configuration)</li> <li>SSL certificates (Let's Encrypt or paid)</li> <li>Load balancer configuration (complexity + cost)</li> </ul> <p>For learning purposes, accessing via IP:port is sufficient.</p>"},{"location":"1_4c_deployment_workflow.html#whats-next","title":"What's Next","text":"<p>In the next session, we'll discover Cloud Run which provides:</p> <ul> <li>Automatic HTTPS with a public URL</li> <li>No infrastructure to manage</li> <li>Auto-scaling based on traffic</li> <li>Pay only when your app is running</li> </ul>"},{"location":"1_4c_deployment_workflow.html#37-cleanup","title":"3.7 - Cleanup","text":"<p>Delete your VM to avoid unnecessary charges:</p> <pre><code>gcloud compute instances delete streamlit-vm-FIRSTNAME --zone=europe-west9-a\n</code></pre> <p>Confirm the deletion when prompted.</p> <p>Note</p> <p>The firewall rule stays - it's shared across all students and doesn't cost anything.</p>"},{"location":"1_4c_deployment_workflow.html#38-conclusion","title":"3.8 - Conclusion","text":"<p>Success</p> <p>You have successfully deployed a containerized webapp to a public VM!</p> <p>What you learned:</p> <ul> <li>Deploying containers to GCE VMs with Container-Optimized OS</li> <li>How firewall rules control network access</li> <li>Using pre-built images from Artifact Registry</li> <li>Conceptual understanding of production infrastructure (load balancers, domains, HTTPS)</li> </ul> <p>Next steps:</p> <ul> <li>Finish previous hands-on if time remains (docker-compose section)</li> <li>Next session: Cloud Run for fully managed deployments</li> </ul>"},{"location":"1_5a_deployment.html","title":"Deploy your ML model into production","text":""},{"location":"1_5a_deployment.html#slides","title":"Slides","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_5b_deployment_be.html","title":"Deploy your ML model into production","text":""},{"location":"1_5b_deployment_be.html#objectives","title":"Objectives","text":"<p>By the end of this workshop, you will be able to:</p> <ol> <li>Explain APIs and microservices - Understand what an API is and why we separate frontend from backend</li> <li>Run multi-container applications - Use docker-compose to orchestrate services locally</li> <li>Deploy to Cloud Run - Use the gcloud CLI to deploy a container with a public HTTPS URL</li> <li>Connect distributed services - Point a local frontend to a cloud-hosted backend API</li> </ol>"},{"location":"1_5b_deployment_be.html#workshop-overview","title":"Workshop Overview","text":"<p>We will convert this notebook into two containerized services:</p> <ul> <li>A backend server that receives images and returns YOLO object detection predictions</li> <li>A frontend UI that lets you upload images and visualize the predictions</li> </ul> <p>The Docker images are pre-built - this workshop focuses on understanding the architecture and deployment workflow.</p> <p>What we will cover:</p> <ol> <li>Key concepts: APIs and microservices</li> <li>Testing the backend locally</li> <li>Testing the frontend locally</li> <li>Using docker-compose for multi-container orchestration</li> <li>Deploying the backend to Google Cloud Run</li> <li>Connecting everything together</li> </ol>"},{"location":"1_5b_deployment_be.html#1-key-concepts","title":"1 - Key Concepts","text":"<p>Before diving into the hands-on, let's understand two important concepts.</p>"},{"location":"1_5b_deployment_be.html#what-is-an-api","title":"What is an API?","text":"<p>An API (Application Programming Interface) defines how software components communicate. For web applications, this means:</p> <ul> <li>Client sends a request: HTTP method + URL + optional data</li> <li>Server processes and responds: Status code + data (usually JSON)</li> </ul> <p>In this workshop, the backend exposes an API with endpoints like:</p> Endpoint Method Input Output <code>/health</code> GET None <code>{\"status\": \"ok\"}</code> <code>/predict</code> POST Base64 image <code>{\"detections\": [...], \"time\": 0.5}</code> <p>The frontend doesn't need to know how the model works - it just sends images and receives predictions through the API contract.</p>"},{"location":"1_5b_deployment_be.html#why-microservices","title":"Why Microservices?","text":"<p>Our application has two components:</p> Component Role Technology Backend Receives images, runs YOLO model, returns predictions FastAPI Frontend User interface for uploading images and displaying results Streamlit <p>Why separate them instead of one monolithic application?</p> <ul> <li>Independent scaling - The backend needs more CPU/memory for ML inference; the frontend is lightweight</li> <li>Independent updates - Change the UI without touching the model, or vice versa</li> <li>Technology flexibility - Use the best framework for each job</li> <li>Team autonomy - Different developers can work on each component</li> </ul> <p>When to use microservices</p> <p>Microservices add complexity (networking, deployment coordination). For small projects, a monolith is often simpler. We use microservices here to demonstrate the pattern you'll encounter in production systems.</p>"},{"location":"1_5b_deployment_be.html#2-setup","title":"2 - Setup","text":""},{"location":"1_5b_deployment_be.html#select-your-gcp-project","title":"Select your GCP Project","text":"<p>Select your personal Google Cloud Platform project (the same one from previous sessions).</p>"},{"location":"1_5b_deployment_be.html#start-your-github-codespace","title":"Start your GitHub Codespace","text":"<p>If you already have a GitHub Codespace from previous sessions, relaunch it from the Codespaces interface.</p> <p>Otherwise, start a new one from https://github.com/fchouteau/isae-cloud-computing-codespace</p> <p></p>"},{"location":"1_5b_deployment_be.html#verify-gcloud-configuration","title":"Verify gcloud configuration","text":"<pre><code>gcloud config get-value project\n</code></pre> <p>If not configured, run <code>gcloud init</code> as you did in the previous session.</p>"},{"location":"1_5b_deployment_be.html#navigate-to-the-workshop-folder","title":"Navigate to the workshop folder","text":"<pre><code>cd tp-deployment\nls\n</code></pre> <p>You should see:</p> <pre><code>tp-deployment/\n\u251c\u2500\u2500 backend/           # FastAPI server with YOLO model\n\u251c\u2500\u2500 frontend/          # Streamlit user interface\n\u251c\u2500\u2500 docker-compose.yml # Multi-container configuration\n\u2514\u2500\u2500 test-images/       # Sample images for testing\n</code></pre>"},{"location":"1_5b_deployment_be.html#3-understanding-and-testing-the-backend","title":"3 - Understanding and Testing the Backend","text":"<p>The <code>backend/</code> folder contains a FastAPI application that serves the YOLO object detection model.</p>"},{"location":"1_5b_deployment_be.html#explore-the-code","title":"Explore the code","text":"<p>Open <code>backend/app.py</code> and look for:</p> <ul> <li>The FastAPI app declaration</li> <li>The <code>/predict</code> route - receives an image, runs inference, returns detections</li> <li>The <code>/health</code> route - simple endpoint to check if the server is alive</li> </ul> <pre><code>@app.post(\n    \"/predict\",\n    description=\"Send a base64 encoded image + the model name, get detections\",\n    response_description=\"Detections + Processing time\",\n    response_model=Result,\n)\n</code></pre> <p>What is FastAPI?</p> <p>FastAPI is a modern Python web framework for building APIs. It automatically generates interactive documentation from your code and validates request/response data.</p>"},{"location":"1_5b_deployment_be.html#run-the-backend-locally","title":"Run the backend locally","text":"<p>Launch the backend container:</p> <pre><code>docker run --rm -p 8000:8000 eu.gcr.io/third-ridge-138414/yolo-v5:1.2\n</code></pre> <p>This starts a container exposing port 8000.</p>"},{"location":"1_5b_deployment_be.html#test-the-api","title":"Test the API","text":"<p>Option 1: Browser</p> <p>Open port 8000 in your Codespace. You should see:</p> <pre><code>\"YOLO-V5 WebApp created with FastAPI\"\n</code></pre> <p>Navigate to <code>/docs</code> to see the interactive API documentation:</p> <p></p> <p>Option 2: Test script</p> <p>In a new terminal, run the test script:</p> <pre><code>cd backend\npython test_webapp.py\n</code></pre> <p>This sends test requests to the API and displays the detection results for <code>cats.png</code>.</p> <p>Checkpoint</p> <p>You should see test results and cat detections printed. Keep this container running for the next section.</p>"},{"location":"1_5b_deployment_be.html#4-understanding-and-testing-the-frontend","title":"4 - Understanding and Testing the Frontend","text":"<p>Interacting with the backend via scripts isn't user-friendly. The <code>frontend/</code> folder contains a Streamlit application that provides a visual interface.</p>"},{"location":"1_5b_deployment_be.html#explore-the-code_1","title":"Explore the code","text":"<p>Open <code>frontend/app.py</code> and look for:</p> <ul> <li>The backend URL configuration (user can change it in the UI)</li> <li>The \"IS ALIVE\" button - calls <code>/health</code> to check if the backend is reachable</li> <li>The image upload widget and \"PREDICT\" button - sends the image to <code>/predict</code></li> </ul>"},{"location":"1_5b_deployment_be.html#run-the-frontend-locally","title":"Run the frontend locally","text":"<p>Open a new terminal (keep the backend running in the first one):</p> <pre><code>docker run --rm -p 8501:8501 --network=\"host\" eu.gcr.io/third-ridge-138414/yolo-v5-streamlit:1.5\n</code></pre> <p>Why <code>--network=\\\"host\\\"</code>?</p> <p>This lets the frontend container access <code>localhost:8000</code> where the backend is running. Without it, containers are on isolated networks and cannot communicate.</p>"},{"location":"1_5b_deployment_be.html#test-the-full-flow","title":"Test the full flow","text":"<ol> <li>Open port 8501 in your Codespace</li> <li>The backend URL should default to <code>http://localhost:8000</code></li> <li>Click \"IS ALIVE\" - should confirm the backend is reachable</li> <li>Upload an image (try one with people, cars, or animals)</li> <li>Click \"PREDICT\" - you should see detection boxes drawn on the image</li> </ol> <p>Checkpoint</p> <p>You can now send images through the frontend and see YOLO detections. Stop both containers (Ctrl+C in each terminal) before continuing.</p>"},{"location":"1_5b_deployment_be.html#5-multi-container-orchestration-with-docker-compose","title":"5 - Multi-Container Orchestration with docker-compose","text":"<p>Running two containers manually works, but it's tedious:</p> <ul> <li>Two terminals to manage</li> <li>Remember port mappings for each service</li> <li>Handle networking between containers</li> <li>Start and stop each individually</li> </ul> <p>Imagine an application with 5 containers...</p>"},{"location":"1_5b_deployment_be.html#the-solution-docker-compose","title":"The solution: docker-compose","text":"<p>Docker Compose lets you define all services in a single YAML file and manage them with one command.</p> <p>Open <code>docker-compose.yml</code>:</p> <pre><code>version: '3'\nservices:\n  yolo:\n    image: \"eu.gcr.io/third-ridge-138414/yolo-v5:1.2\"\n    ports:\n      - \"8000:8000\"\n    hostname: yolo\n  streamlit:\n    image: \"eu.gcr.io/third-ridge-138414/yolo-v5-streamlit:1.5\"\n    ports:\n      - \"8501:8501\"\n    hostname: streamlit\n</code></pre> <p>Key benefits:</p> <ul> <li>Both services start with one command</li> <li>They automatically share a network (no <code>--network=\"host\"</code> needed)</li> <li>Services can reach each other by hostname (<code>yolo</code>, <code>streamlit</code>)</li> </ul>"},{"location":"1_5b_deployment_be.html#run-with-docker-compose","title":"Run with docker-compose","text":"<pre><code>docker-compose up\n</code></pre> <p>Both services start and logs are interleaved in your terminal.</p>"},{"location":"1_5b_deployment_be.html#test-the-application","title":"Test the application","text":"<ol> <li>Open port 8501 in your Codespace (frontend)</li> <li>Set the backend URL to <code>http://yolo:8000</code> (use the hostname, not localhost)</li> <li>Test \"IS ALIVE\" and run a prediction</li> </ol> <p>Hostname vs localhost</p> <p>When using docker-compose, services communicate via their hostnames defined in the YAML file. Use <code>http://yolo:8000</code>, not <code>http://localhost:8000</code>.</p>"},{"location":"1_5b_deployment_be.html#stop-the-services","title":"Stop the services","text":"<pre><code>docker-compose down\n</code></pre> <p>docker-compose vs Kubernetes</p> <p>docker-compose is great for local development and simple deployments. For production at scale, orchestrators like Kubernetes provide service discovery, load balancing, and self-healing across multiple machines.</p>"},{"location":"1_5b_deployment_be.html#6-deploying-to-google-cloud-run","title":"6 - Deploying to Google Cloud Run","text":"<p>We've tested locally. Now let's deploy the backend to the cloud so anyone can access it with a real URL.</p> <p>Why not deploy to a VM?</p> <p>You could deploy to a VM using <code>gcloud compute instances create-with-container</code> as you learned in the previous session. However, this approach has limitations:</p> Limitation Problem No domain name You get a raw IP address like <code>35.205.x.x:8000</code> No HTTPS Traffic is unencrypted Manual scaling One VM handles all requests, or you manually add more Always running You pay even when nobody is using it Infrastructure burden Firewall rules, restarts, updates are your responsibility <p>Cloud Run solves all of these automatically.</p>"},{"location":"1_5b_deployment_be.html#what-is-cloud-run","title":"What is Cloud Run?","text":"<p>Cloud Run is a fully managed platform for running containers. You provide a container image, Cloud Run handles everything else:</p> Feature What you get Public URL <code>https://your-service-xxxxx.run.app</code> HTTPS Automatic SSL certificate, no configuration Scaling Scales up with traffic, down to zero when idle No infrastructure No VMs, no firewall rules, no load balancer setup"},{"location":"1_5b_deployment_be.html#deploy-the-backend","title":"Deploy the backend","text":"<p>Make sure gcloud is configured:</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\necho \"Deploying to project: ${PROJECT_ID}\"\n</code></pre> <p>Deploy the YOLO backend:</p> <pre><code>gcloud run deploy yolo-backend \\\n    --image=eu.gcr.io/third-ridge-138414/yolo-v5:1.2 \\\n    --platform=managed \\\n    --region=europe-west1 \\\n    --allow-unauthenticated \\\n    --port=8000 \\\n    --memory=16Gi\n</code></pre> <p>Flags explained:</p> Flag Purpose <code>--image</code> The container image to deploy <code>--platform=managed</code> Use fully managed Cloud Run (not Kubernetes) <code>--region=europe-west1</code> Deploy in Europe (close to users) <code>--allow-unauthenticated</code> Public access without login <code>--port=8000</code> The port your container listens on <code>--memory=16Gi</code> YOLO model needs more RAM than default"},{"location":"1_5b_deployment_be.html#get-your-service-url","title":"Get your service URL","text":"<p>After deployment completes, gcloud displays the service URL:</p> <pre><code>Service URL: https://yolo-backend-xxxxx-ew.a.run.app\n</code></pre> <p>Test the deployed backend:</p> <ol> <li>Open the URL in your browser - you should see the FastAPI welcome message</li> <li>Add <code>/docs</code> to the URL - you should see the interactive API documentation</li> </ol> <p>Checkpoint</p> <p>Your backend is now running in the cloud with a real HTTPS URL. Anyone on the internet can access it.</p>"},{"location":"1_5b_deployment_be.html#connect-your-local-frontend-to-the-cloud-backend","title":"Connect your local frontend to the cloud backend","text":"<p>Now run the frontend locally, pointing to your Cloud Run backend:</p> <pre><code>docker run --rm -p 8501:8501 eu.gcr.io/third-ridge-138414/yolo-v5-streamlit:1.5\n</code></pre> <p>Note: No <code>--network=\"host\"</code> needed - we're connecting to the internet, not localhost.</p> <ol> <li>Open port 8501 in your Codespace</li> <li>Set the backend URL to your Cloud Run URL (e.g., <code>https://yolo-backend-xxxxx-ew.a.run.app</code>)</li> <li>Click \"IS ALIVE\" - should confirm the cloud backend is reachable</li> <li>Upload an image and run a prediction</li> </ol> <p>Your request now travels from your Codespace, through the internet, to Google's infrastructure, runs inference on the YOLO model, and returns the results.</p> <p>Scaling (preview)</p> <p>Cloud Run can automatically scale based on traffic - spinning up more instances when busy, scaling to zero when idle. We'll explore these settings in a future session. For now, the defaults work well.</p>"},{"location":"1_5b_deployment_be.html#7-conclusion","title":"7 - Conclusion","text":"<p>Congratulations!</p> <p>You have deployed your first ML model to production with a real HTTPS URL!</p>"},{"location":"1_5b_deployment_be.html#what-you-learned","title":"What you learned","text":"Concept What you did APIs Understood REST endpoints (<code>POST /predict</code>, <code>GET /health</code>) Microservices Separated frontend and backend into independent containers docker-compose Orchestrated multiple containers locally with one command Cloud Run Deployed a container and got a public HTTPS URL instantly"},{"location":"1_5b_deployment_be.html#architecture-you-built","title":"Architecture you built","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Local (Codespace)  \u2502              \u2502       Google Cloud Run       \u2502\n\u2502                     \u2502              \u2502                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    HTTPS     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   Frontend    \u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  \u2502   Backend (YOLO API)   \u2502  \u2502\n\u2502  \u2502  (Streamlit)  \u2502  \u2502              \u2502  \u2502                        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502              \u2502  \u2502  yolo-backend-xxx.run  \u2502  \u2502\n\u2502                     \u2502              \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"1_5b_deployment_be.html#cleanup","title":"Cleanup","text":"<p>Delete your Cloud Run service to avoid any charges:</p> <pre><code>gcloud run services delete yolo-backend --region=europe-west1 --quiet\n</code></pre> <p>Don't forget!</p> <p>Always clean up cloud resources when you're done. Cloud Run has a generous free tier, but it's good practice to delete unused services.</p>"},{"location":"1_5b_deployment_be.html#whats-next","title":"What's next","text":"<ul> <li>Scaling: Configure min/max instances and concurrency limits</li> <li>CI/CD: Automatically deploy when you push to git</li> <li>Custom domains: Use your own domain instead of <code>.run.app</code></li> <li>Full deployment: Deploy the frontend to Cloud Run too</li> </ul>"},{"location":"1_6_conclusion.html","title":"Take-away messages &amp; reading list","text":""},{"location":"1_6_conclusion.html#take-away-messages","title":"Take-away messages","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_6_conclusion.html#readings","title":"Readings","text":""},{"location":"1_6_conclusion.html#about-cloud-computing","title":"About Cloud Computing","text":"<ul> <li> <p>Buyya, R., Srirama, S. N., Casale, G., Calheiros, R., Simmhan, Y., Varghese, B., ... &amp; Toosi, A. N. (2018). A manifesto for future generation cloud computing: Research directions for the next decade. ACM computing surveys (CSUR), 51(5), 1-38.</p> </li> <li> <p>On sustainable data centers and energy use (intro)</p> </li> <li> <p>The NIST Definitions of Cloud Computing</p> </li> <li> <p>Open Data: Open Sentinel 2 archive on AWS</p> </li> <li> <p>Environmental Impact of Cloud vs On Premise</p> </li> <li> <p>Environmental Impact of cloud vs on-premise medium blog post</p> </li> <li> <p>Paper from Natural Resources Defense Council on Cloud vs On-Premise</p> </li> <li> <p>Anecdotes about Cloud Computing</p> </li> </ul>"},{"location":"1_6_conclusion.html#about-containers","title":"About Containers","text":"<ul> <li> <p>Docker whitepaper: Docker and the way of the Devops</p> </li> <li> <p>What exactly is Docker ? Simple explanation from a medium blog post</p> </li> </ul>"},{"location":"1_6_conclusion.html#about-orchestration","title":"About Orchestration","text":"<ul> <li> <p>Verma, A., Pedrosa, L., Korupolu, M., Oppenheimer, D., Tune, E., &amp; Wilkes, J. (2015, April). Large-scale cluster management at Google with Borg. In Proceedings of the Tenth European Conference on Computer Systems (pp. 1-17).</p> </li> <li> <p>Kubernetes Comic to learn about Kubernetes in a fun way https://cloud.google.com/kubernetes-engine/kubernetes-comic</p> </li> </ul>"},{"location":"1_7_appendixes.html","title":"Appendixes","text":"<p>Info</p> <p>This contains useful appendixes and links for technical notions that you may find useful.</p>"},{"location":"1_8_deployment_tp_long.html","title":"Deployment : Deploy your ML model in production (Version Longue de janvier 2023)","text":""},{"location":"1_8_deployment_tp_long.html#objectives","title":"Objectives","text":"<p>Your first ML model in production !</p> <ul> <li>A model behind a Restful API, packaged in a docker</li> <li>A frontend using streamlit, packaged in a docker</li> <li>Deploy a multi-container application using docker compose</li> <li>Deploy the model in the docker image</li> <li>Send it to your friends !</li> </ul> <p>Regardons ce notebook</p> <p>Il effectue les op\u00e9rations suivantes:</p> <ul> <li>Chargement d'un mod\u00e8le</li> <li>Chargement d'une image</li> <li>D\u00e9tection des \"objets\" sur l'image</li> <li>Dessin des d\u00e9tections sur l'image</li> <li>Affichage</li> </ul> <p>L'objectif est de convertir ce notebook en deux applications :</p> <ul> <li>L'une qui \"sert\" les pr\u00e9dictions d'un mod\u00e8le (le serveur)</li> <li>L'une qui permet \u00e0 un utilisateur d'interagir facilement avec le mod\u00e8le en mettant en ligne sa propre image (le \"client\")</li> </ul> <p>Nous allons d\u00e9velopper tout cela dans l'environnement de d\u00e9veloppement (codespaces)</p> <p>Puis d\u00e9ployer le mod\u00e8le dans l'environnement GCP</p>"},{"location":"1_8_deployment_tp_long.html#team-composition","title":"Team Composition","text":"<p>C'est mieux d'\u00eatre en bin\u00f4me pour s'entraider :)</p>"},{"location":"1_8_deployment_tp_long.html#configuration-du-codespace","title":"Configuration du codespace","text":"<p>Nous allons utiliser github codespaces comme environnement de d\u00e9veloppement,</p> <p>Repartir de https://github.com/github/codespaces-blank</p> <p>Puis configurer ce codespace avec le google cloud sdk et configurer le projet <code>isae-sdd</code></p> <p>Hint</p> <pre><code># Rappels : Installation du google cloud sdk\n# https://cloud.google.com/sdk/docs/install#linux\ncurl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-416.0.0-linux-x86_64.tar.gz\ntar -xf google-cloud-cli-416.0.0-linux-x86.tar.gz\n./google-cloud-sdk/install.sh\n# Type yes to add to path !\nexport PATH=./google-cloud-sdk/bin:$PATH\ngcloud init\n# login and copy the token\n# configure isae-sdd then compute zone 17\ngcloud auth configure-docker\n</code></pre> <p>Voir les tps pr\u00e9c\u00e9dents</p> <p>Maintenant, depuis ce codespace, ouvrez un terminal et r\u00e9cup\u00e9rez les fichiers suivants :</p> <pre><code>gsutil cp -r gs://fchouteau-isae-cloud/deployment/* .\n</code></pre> <p>Hint</p> <p>Si vous tombez \u00e0 court de stockage dans le TP, lancez <code>docker system prune</code> pour nettoyer le cache docker</p>"},{"location":"1_8_deployment_tp_long.html#1-converting-a-prediction-notebook-into-a-webapplication","title":"1 - Converting a prediction notebook into a webapplication","text":"<p>Placez vous dans le dossier <code>model</code> nouvellement cr\u00e9\u00e9</p>"},{"location":"1_8_deployment_tp_long.html#objectif","title":"Objectif","text":"<p>Packager un mod\u00e8le de machine learning derri\u00e8re une webapplication pour pouvoir la d\u00e9ployer sur le web et servir des pr\u00e9dictions \u00e0 des utilisateurs</p> <p>Le mod\u00e8le: Un d\u00e9tecteur d'objets sur des photographies \"standard\" suppos\u00e9 marcher en temps r\u00e9el, qui sort des \"bounding boxes\" autour des objets d\u00e9tect\u00e9 dans des images</p> <p></p> <p>Remarque : Le papier vaut la lecture https://pjreddie.com/media/files/papers/YOLOv3.pdf</p> <p>On r\u00e9cup\u00e8re la version disponible sur torchhub https://pytorch.org/hub/ultralytics_yolov5/ qui correspond au repository suivant https://github.com/ultralytics/yolov5</p> <p>Voici une petite explication de l'historique de YOLO https://medium.com/towards-artificial-intelligence/yolo-v5-is-here-custom-object-detection-tutorial-with-yolo-v5-12666ee1774e</p> <p>On se propose ici d'encapsuler 3 versions du mod\u00e8le (S,M,L) qui sont 3 versions +/- complexes du mod\u00e8le YOLO-V5, afin de pouvoir comparer les performances et les r\u00e9sultats</p> <p></p>"},{"location":"1_8_deployment_tp_long.html#deroulement","title":"D\u00e9roulement","text":"<ul> <li>Transformer un notebook de pr\u00e9diction en \u201cWebApp\u201d en remplissant <code>app.stub.py</code> et en le renommant en <code>app.py</code></li> <li>Packager l'application sous forme d'une image docker</li> <li>Tester son image docker localement</li> <li>Uploader le docker sur Google Container Registry</li> </ul>"},{"location":"1_8_deployment_tp_long.html#developpement-de-apppy","title":"D\u00e9veloppement de app.py","text":"<p>Regardons le <code>app.stub.py</code> (que l'on renommera en <code>app.py</code>)</p> <pre><code>import base64\nimport io\nimport time\nfrom typing import List, Dict\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\n\nclass Input(BaseModel):\n    model: str\n    image: str\n\n\nclass Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n\n\nclass Result(BaseModel):\n    detections: List[Detection] = []\n    time: float = 0.0\n    model: str\n\n\n# !!!! FILL ME\ndef parse_predictions(prediction: np.ndarray, classes: [str]) -&gt; List[Detection]:\n    raise NotImplementedError\n\n\n# !!!! FILL ME\ndef load_model(model_name: str):\n    \"\"\"\"\"\"\n    raise NotImplementedError\n\n\nMODEL_NAMES = [\"yolov5s\", \"yolov5m\", \"yolov5l\"]\n\napp = FastAPI(\n    title=\"NAME ME\",\n    description=\"\"\"\n                DESCRIBE ME\n                \"\"\",\n    version=\"1.0\",\n)\n\n# !!!! FILL ME\n# This is a dictionnary that must contains a model for each key (model names), fill load model\n# example: for model_name in MODEL_NAMES: MODELS[model_name] = load_model(model_name)\n# You can also lazily load models only when they are called to avoid holding 3 models in memory\nMODELS = ...\n\n\n@app.get(\"/\", description=\"return the title\", response_description=\"FILL ME\", response_model=str)\ndef root() -&gt; str:\n    return app.title\n\n\n@app.get(\"/describe\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=str)\ndef describe() -&gt; str:\n    return app.description\n\n\n@app.get(\"/health\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=str)\ndef health() -&gt; str:\n    return \"HEALTH OK\"\n\n\n@app.get(\"/models\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=List[str])\ndef models() -&gt; [str]:\n    return MODEL_NAMES\n\n\n@app.post(\"/predict\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=Result)\ndef predict(inputs: Input) -&gt; Result:\n\n    # get correct model\n    model_name = inputs.model\n\n    if model_name not in MODEL_NAMES:\n        raise HTTPException(status_code=400, detail=\"wrong model name, choose between {}\".format(MODEL_NAMES))\n\n    # Get the model from the list of available models\n    model = MODELS.get(model_name)\n\n    # Get &amp; Decode image\n    try:\n        image = inputs.image.encode(\"utf-8\")\n        image = base64.b64decode(image)\n        image = Image.open(io.BytesIO(image))\n    except:\n        raise HTTPException(status_code=400, detail=\"File is not an image\")\n    # Convert from RGBA to RGB *to avoid alpha channels*\n    if image.mode == \"RGBA\":\n        image = image.convert(\"RGB\")\n\n    # Inference\n\n    # RUN THE PREDICTION, TIME IT\n    predictions = ...\n\n    # Post processing\n    classes = predictions.names\n    predictions = predictions.xyxy[0].numpy()\n\n    # Create a list of [DETECTIONS] objects that match the detection class above, using the parse_predictions method\n    detections = ...\n\n    result = Result(detections=..., time=..., model=...)\n\n    return result\n</code></pre> <p>Dans un premier temps, vous pouvez remplir la description des \"routes\" (i.e. des fonctions de l'application):</p> <pre><code>@app.get(\"/\", description=\"return the title\", response_description=\"FILL ME\", response_model=str)\ndef root() -&gt; str:\n    return app.title\n\n\n@app.get(\"/describe\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=str)\ndef describe() -&gt; str:\n    return app.description\n\n\n@app.get(\"/health\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=str)\ndef health() -&gt; str:\n    return \"HEALTH OK\"\n\n\n@app.get(\"/models\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=List[str])\ndef models() -&gt; [str]:\n    return MODEL_NAMES\n</code></pre> <p>Il y a deux fonctions \u00e0 compl\u00e9ter en s'inspirant du notebook <code>inference.ipynb</code>. Grace au typage de python, vous avez les types d'entr\u00e9e et de sortie des deux fonctions</p> <p>La premi\u00e8re prend un tableau de type (left, top, right, bottom, confidence, class_index) et une liste de noms de classes et cr\u00e9\u00e9e une liste d'objets <code>Detection</code> (voir le code pour la cr\u00e9ation des objets d\u00e9tection)</p> <pre><code># !!!! FILL ME\ndef parse_predictions(predictions: np.ndarray, classes: [str]) -&gt; List[Detection]:\n    raise NotImplementedError\n</code></pre> Hint <pre><code>def parse_prediction(prediction: np.ndarray, classes: [str]) -&gt; Detection:\nx0, y0, x1, y1, cnf, cls = prediction\ndetection = Detection(\n    x_min=int(x0),\n    y_min=int(y0),\n    x_max=int(x1),\n    y_max=int(y1),\n    confidence=round(float(cnf), 3),\n    class_name=classes[int(cls)],\n)\nreturn detection\n</code></pre> <p>La seconde fonction doit charger un mod\u00e8le via torchhub en fonction de son nom (voir le docker)</p> <pre><code># !!!! FILL ME\ndef load_model(model_name: str):\n    \"\"\"\"\"\"\n    raise NotImplementedError\n</code></pre> Hint <pre><code>def load_model(model_name: str) -&gt; Dict:\n    # Load model from torch\n    model = torch.hub.load(\"ultralytics/yolov5\", model_name, pretrained=True)\n    # Evaluation mode + Non maximum threshold\n    model = model.eval()\n\nreturn model\n</code></pre> <p>Ensuite, vous pouvez executer les fonctions de chargement de mod\u00e8le, par exemple</p> <pre><code># !!!! FILL ME\n# This is a dictionnary that must contains a model for each key (model names), fill load model\n# example: for model_name in MODEL_NAMES: MODELS[model_name] = load_model(model_name)\n# You can also lazily load models only when they are called to avoid holding 3 models in memory\nMODELS = {}\nfor model_name in MODEL_NAMES:\n    MODELS[model_name] = load_model(model_name)\n</code></pre> <p>Enfin, il s'agit d'\u00e9crire un code qui effectue une pr\u00e9diction \u00e0 partir d'une image PIL et de mesurer le temps (indice: <code>import time</code> et <code>t0 = time.time()</code> ...) de pr\u00e9diction</p> <pre><code># RUN THE PREDICTION, TIME IT\npredictions = ...\n# Post processing\nclasses = predictions.names\npredictions = predictions.xyxy[0].numpy()\n</code></pre> <p>Le r\u00e9sultat de predictions est un tableau numpy compos\u00e9 des colonnes <code>left, top, right, bottom, confidence, class_index</code></p> <p>Il s'agit ensuite de transformer ces predictions en <code>[Detection]</code></p> <pre><code>class Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n</code></pre> <pre><code># Create a list of [DETECTIONS] objects that match the detection class above, using the parse_predictions method\ndetections = parse_predictions(predictions, classes)\n</code></pre> Hint <pre><code># Inference\nt0 = time.time()\npredictions = model(image, size=640)  # includes NMS\nt1 = time.time()\nclasses = predictions.names\n\n# Post processing\npredictions = predictions.xyxy[0].numpy()\ndetections = [parse_prediction(prediction=pred, classes=classes) for pred in predictions]\n\nresult = Result(detections=detections, time=round(t1 - t0, 3), model=model_name)\n</code></pre>"},{"location":"1_8_deployment_tp_long.html#correction","title":"Correction","text":"<p><code>app.py</code></p> Hint <pre><code>import base64\nimport io\nimport time\nfrom typing import List, Dict\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\n\nclass Input(BaseModel):\n    model: str\n    image: str\n\n\nclass Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n\n\nclass Result(BaseModel):\n    detections: List[Detection] = []\n    time: float = 0.0\n    model: str\n\n\ndef parse_prediction(prediction: np.ndarray, classes: [str]) -&gt; Detection:\n    x0, y0, x1, y1, cnf, cls = prediction\n    detection = Detection(\n        x_min=int(x0),\n        y_min=int(y0),\n        x_max=int(x1),\n        y_max=int(y1),\n        confidence=round(float(cnf), 3),\n        class_name=classes[int(cls)],\n    )\n    return detection\n\n\ndef load_model(model_name: str) -&gt; Dict:\n    # Load model from torch\n    model = torch.hub.load(\"ultralytics/yolov5\", model_name, pretrained=True)\n    # Evaluation mode + Non maximum threshold\n    model = model.eval()\n\n    return model\n\n\n# %%\napp = FastAPI(\n    title=\"YOLO-V5 WebApp created with FastAPI\",\n    description=\"\"\"\n                Wraps 3 different yolo-v5 models under the same RESTful API\n                \"\"\",\n    version=\"1.1\",\n)\n\n# %%\nMODEL_NAMES = [\"yolov5s\", \"yolov5m\", \"yolov5l\"]\nMODELS = {}\n\n\n@app.get(\"/\", description=\"return the title\", response_description=\"title\", response_model=str)\ndef root() -&gt; str:\n    return app.title\n\n\n@app.get(\"/describe\", description=\"return the description\", response_description=\"description\", response_model=str)\ndef describe() -&gt; str:\n    return app.description\n\n\n@app.get(\"/version\", description=\"return the version\", response_description=\"version\", response_model=str)\ndef describe() -&gt; str:\n    return app.version\n\n\n@app.get(\"/health\", description=\"return whether it's alive\", response_description=\"alive\", response_model=str)\ndef health() -&gt; str:\n    return \"HEALTH OK\"\n\n\n@app.get(\n    \"/models\",\n    description=\"Query the list of models\",\n    response_description=\"A list of available models\",\n    response_model=List[str],\n)\ndef models() -&gt; [str]:\n    return MODEL_NAMES\n\n\n@app.post(\n    \"/predict\",\n    description=\"Send a base64 encoded image + the model name, get detections\",\n    response_description=\"Detections + Processing time\",\n    response_model=Result,\n)\ndef predict(inputs: Input) -&gt; Result:\n    global MODELS\n\n    # get correct model\n    model_name = inputs.model\n\n    if model_name not in MODEL_NAMES:\n        raise HTTPException(status_code=400, detail=\"wrong model name, choose between {}\".format(MODEL_NAMES))\n\n    # check load\n    if MODELS.get(model_name) is None:\n        MODELS[model_name] = load_model(model_name)\n\n    model = MODELS.get(model_name)\n\n    # Get Image\n    # Decode image\n    try:\n        image = inputs.image.encode(\"utf-8\")\n        image = base64.b64decode(image)\n        image = Image.open(io.BytesIO(image))\n    except:\n        raise HTTPException(status_code=400, detail=\"File is not an image\")\n    # Convert from RGBA to RGB *to avoid alpha channels*\n    if image.mode == \"RGBA\":\n        image = image.convert(\"RGB\")\n\n    # Inference\n    t0 = time.time()\n    predictions = model(image, size=640)  # includes NMS\n    t1 = time.time()\n    classes = predictions.names\n\n    # Post processing\n    predictions = predictions.xyxy[0].numpy()\n    detections = [parse_prediction(prediction=pred, classes=classes) for pred in predictions]\n\n    result = Result(detections=detections, time=round(t1 - t0, 3), model=model_name)\n\n    return result\n</code></pre>"},{"location":"1_8_deployment_tp_long.html#construire-le-docker","title":"Construire le docker","text":"<pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\ndocker build -t eu.gcr.io/${PROJECT_ID}/{you rname}{your app name}:{your version} -f Dockerfile . \n</code></pre>"},{"location":"1_8_deployment_tp_long.html#tester-le-docker","title":"Tester le docker","text":"<p>Vous pouvez lancer le docker localement et le tester  avec le notebook</p> <pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\ndocker run --rm -p 8000:8000 eu.gcr.io/${PROJECT_ID}/{your-name}-{your app name}:{your version}\n</code></pre> <p>Vous pouvez vous connecter \u00e0 votre appli via son ip publique sur le port 8000 depuis votre navigateur local</p> <p><code>http://{ip}:8000</code></p> <p>Essayez quelques routes :</p> <p><code>/models</code> <code>/docs</code></p>"},{"location":"1_8_deployment_tp_long.html#pusher-le-docker-sur-google-container-registry","title":"Pusher le docker sur google container registry","text":"<pre><code>gcloud auth configure-docker\ndocker push eu.gcr.io/${PROJECT_ID}/{your-name}-model:{your version}\n</code></pre> <p>Si vous devez mettre \u00e0 jour le docker, il faut incr\u00e9menter la version pour le d\u00e9ploiement</p>"},{"location":"1_8_deployment_tp_long.html#liens-utiles","title":"Liens Utiles","text":"<ul> <li>https://fastapi.tiangolo.com/</li> <li>https://requests.readthedocs.io/en/master/</li> <li>https://testdriven.io/blog/fastapi-streamlit/</li> </ul>"},{"location":"1_8_deployment_tp_long.html#2-making-a-companion-application","title":"2 - Making a companion application","text":"<p>Allez dans le dossier <code>streamlit</code></p>"},{"location":"1_8_deployment_tp_long.html#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er une application \"compagnon\" qui permet de faire des requ\u00eates \u00e0 un mod\u00e8le de fa\u00e7on ergonomique et de visualiser les r\u00e9sultats</p>"},{"location":"1_8_deployment_tp_long.html#deroulement_1","title":"D\u00e9roulement","text":"<ul> <li>Remplir <code>app.stub.py</code>, le renommer en <code>app.py</code> en remplissant les bons champs (s'aider des notebooks dans <code>app/</code>) et en cr\u00e9ant des jolies visualisations</li> <li>Packager l'application sous forme d'une image docker</li> <li>Tester son image docker localement</li> <li>Uploader le docker sur Google Container Registry</li> </ul>"},{"location":"1_8_deployment_tp_long.html#guide-de-developpement","title":"Guide de d\u00e9veloppement","text":"<p>Regardons le <code>APP.md</code></p> <ul> <li>Remplissez le fichier avec la description de votre application</li> </ul> <p>Regardons le <code>app.stub.py</code> </p> <pre><code>import requests\nimport streamlit as st\nfrom PIL import Image\nimport io\nimport base64\nfrom pydantic import BaseModel\nfrom typing import List\nimport random\n\n# ---- Functions ---\n\n\nclass Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n\n\nclass Result(BaseModel):\n    detections: List[Detection] = []\n    time: float = 0.0\n    model: str\n\n\n@st.cache(show_spinner=True)\ndef make_dummy_request(model_url: str, model: str, image: Image) -&gt; Result:\n    \"\"\"\n    This simulates a fake answer for you to test your application without having access to any other input from other teams\n    \"\"\"\n    # We do a dummy encode and decode pass to check that the file is correct\n    with io.BytesIO() as buffer:\n        image.save(buffer, format=\"PNG\")\n        buffer: str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        data = {\"model\": model, \"image\": buffer}\n\n    # We do a dummy decode\n    _image = data.get(\"image\")\n    _image = _image.encode(\"utf-8\")\n    _image = base64.b64decode(_image)\n    _image = Image.open(io.BytesIO(_image))  # type: Image\n    if _image.mode == \"RGBA\":\n        _image = _image.convert(\"RGB\")\n\n    _model = data.get(\"model\")\n\n    # We generate a random prediction\n    w, h = _image.size\n\n    detections = [\n        Detection(\n            x_min=random.randint(0, w // 2 - 1),\n            y_min=random.randint(0, h // 2 - 1),\n            x_max=random.randint(w // w, w - 1),\n            y_max=random.randint(h // 2, h - 1),\n            class_name=\"dummy\",\n            confidence=round(random.random(), 3),\n        )\n        for _ in range(random.randint(1, 10))\n    ]\n\n    # We return the result\n    result = Result(time=0.1, model=_model, detections=detections)\n\n    return result\n\n\n@st.cache(show_spinner=True)\ndef make_request(model_url: str, model: str, image: Image) -&gt; Result:\n    \"\"\"\n    Process our data and send a proper request\n    \"\"\"\n    with io.BytesIO() as buffer:\n        image.save(buffer, format=\"PNG\")\n        buffer: str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        data = {\"model\": model, \"image\": buffer}\n\n        response = requests.post(\"{}/predict\".format(model_url), json=data)\n\n    if not response.status_code == 200:\n        raise ValueError(\"Error in processing payload, {}\".format(response.text))\n\n    response = response.json()\n\n    return Result.parse_obj(response)\n\n\n# ---- Streamlit App ---\n\nst.title(\"NAME ME BECAUSE I AM AWESOME\")\n\nwith open(\"APP.md\") as f:\n    st.markdown(f.read())\n\n# --- Sidebar ---\n# defines an h1 header\n\nmodel_url = st.sidebar.text_input(label=\"Cluster URL\", value=\"http://localhost:8000\")\n\n_model_url = model_url.strip(\"/\")\n\nif st.sidebar.button(\"Send 'is alive' to IP\"):\n    try:\n        response = requests.get(\"{}/health\".format(_model_url))\n        if response.status_code == 200:\n            st.sidebar.success(\"Webapp responding at {}\".format(_model_url))\n        else:\n            st.sidebar.error(\"Webapp not respond at {}, check url\".format(_model_url))\n    except ConnectionError:\n        st.sidebar.error(\"Webapp not respond at {}, check url\".format(_model_url))\n\ntest_mode_on = st.sidebar.checkbox(label=\"Test Mode - Generate dummy answer\", value=False)\n\n# --- Main window\n\nst.markdown(\"## Inputs\")\nst.markdown(\"Describe something... You can also add things like confidence slider etc...\")\n\n# Here we should be able to choose between [\"yolov5s\", \"yolov5m\", \"yolov5l\"], perhaps a radio button with the three choices ?\nmodel_name = ...\n\n# Here we should be able to upload a file (our image)\nimage_file = ...\n\n# Converting image, this is done for you :)\nif image_file is not None:\n    image_file.seek(0)\n    image = image_file.read()\n    image = Image.open(io.BytesIO(image))\n\nif st.button(label=\"SEND PAYLOAD\"):\n\n    if test_mode_on:\n        st.warning(\"Simulating a dummy request to {}\".format(model_url))\n        result = ...  # call the proper function\n    else:\n        result = ...  # call the proper function\n\n    st.balloons()\n\n    st.markdown(\"## Display\")\n\n    st.markdown(\"Make something pretty, draw polygons and confidence..., here's an ugly output\")\n\n    st.image(image, width=512, caption=\"Uploaded Image\")\n\n    st.text(\"Model : {}\".format(result.model))\n    st.text(\"Processing time : {}s\".format(result.time))\n\n    for detection in result.detections:\n        st.json(detection.json())\n</code></pre> <p>La majorit\u00e9 des fonctions de requ\u00eate sont d\u00e9j\u00e0 impl\u00e9ment\u00e9es, il reste \u00e0 faire les fonctions d'entr\u00e9es utilisateurs et la visualisation</p> <ul> <li>Entr\u00e9e: Utilisation de <code>st.radio</code> et <code>st.file_uploader</code>: </li> </ul> <p>https://docs.streamlit.io/en/stable/getting_started.html</p> <p>https://docs.streamlit.io/en/stable/api.html#streamlit.radio</p> <p>https://docs.streamlit.io/en/stable/api.html#streamlit.file_uploader</p> <pre><code>st.markdown(\"## Inputs\")\nst.markdown(\"Select your model (Small, Medium or Large)\")\n\nmodel_name = st.radio(label=\"Model Name\", options=[\"yolov5s\", \"yolov5m\", \"yolov5l\"])\n\nst.markdown(\"Upload an image\")\n\nimage_file = st.file_uploader(label=\"Image File\", type=[\"png\", \"jpg\", \"tif\"])\n</code></pre> <ul> <li>Visualisations</li> </ul> <p>Exemple de code qui imite le notebook de pr\u00e9diction pour dessiner sur une image PIL</p> <pre><code>def draw_preds(image: Image, detections: [Detection]):\n\n    class_names = list(set([detection.class_name for detection in detections]))\n\n    image_with_preds = image.copy()\n\n    # Define colors\n    colors = plt.cm.get_cmap(\"viridis\", len(class_names)).colors\n    colors = (colors[:, :3] * 255.0).astype(np.uint8)\n\n    # Define font\n    font = list(Path(\"/usr/share/fonts\").glob(\"**/*.ttf\"))[0].name\n    font = ImageFont.truetype(font=font, size=np.floor(3e-2 * image_with_preds.size[1] + 0.5).astype(\"int32\"))\n    thickness = (image_with_preds.size[0] + image_with_preds.size[1]) // 300\n\n    # Draw detections\n    for detection in detections:\n        left, top, right, bottom = detection.x_min, detection.y_min, detection.x_max, detection.y_max\n        score = float(detection.confidence)\n        predicted_class = detection.class_name\n        class_idx = class_names.index(predicted_class)\n\n        label = \"{} {:.2f}\".format(predicted_class, score)\n\n        draw = ImageDraw.Draw(image_with_preds)\n        label_size = draw.textsize(label, font)\n\n        top = max(0, np.floor(top + 0.5).astype(\"int32\"))\n        left = max(0, np.floor(left + 0.5).astype(\"int32\"))\n        bottom = min(image_with_preds.size[1], np.floor(bottom + 0.5).astype(\"int32\"))\n        right = min(image_with_preds.size[0], np.floor(right + 0.5).astype(\"int32\"))\n\n        if top - label_size[1] &gt;= 0:\n            text_origin = np.array([left, top - label_size[1]])\n        else:\n            text_origin = np.array([left, top + 1])\n\n        # My kingdom for a good redistributable image drawing library.\n        for r in range(thickness):\n            draw.rectangle([left + r, top + r, right - r, bottom - r], outline=tuple(colors[class_idx]))\n        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=tuple(colors[class_idx]))\n\n        if any(colors[class_idx] &gt; 128):\n            fill = (0, 0, 0)\n        else:\n            fill = (255, 255, 255)\n\n        draw.text(text_origin, label, fill=fill, font=font)\n\n        del draw\n\n    return image_with_preds\n</code></pre> <p>Utilisation (exemple)</p> <pre><code>    if test_mode_on:\n        st.warning(\"Simulating a dummy request to {}\".format(model_url))\n        result = ...  # call the proper function\n    else:\n        result = ...  # call the proper function\n\n    st.balloons()\n\n    st.markdown(\"## Display\")\n\n    st.text(\"Model : {}\".format(result.model))\n    st.text(\"Processing time : {}s\".format(result.time))\n\n    image_with_preds = draw_preds(image, result.detections)\n    st.image(image_with_preds, width=1024, caption=\"Image with detections\")\n\n    st.markdown(\"### Detection dump\")\n    for detection in result.detections:\n        st.json(detection.json())\n</code></pre>"},{"location":"1_8_deployment_tp_long.html#corection-apppy","title":"Corection <code>app.py</code>","text":"Hint <pre><code>import base64\nimport io\nimport random\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nimport streamlit as st\nfrom PIL import Image\nfrom PIL import ImageDraw, ImageFont\nfrom pydantic import BaseModel\n\n# ---- Functions ---\n\n\nclass Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n\n\nclass Result(BaseModel):\n    detections: List[Detection] = []\n    time: float = 0.0\n    model: str\n\n\n@st.cache(show_spinner=True)\ndef make_dummy_request(model_url: str, model: str, image: Image) -&gt; Result:\n    \"\"\"\n    This simulates a fake answer for you to test your application without having access to any other input from other teams\n    \"\"\"\n    # We do a dummy encode and decode pass to check that the file is correct\n    with io.BytesIO() as buffer:\n        image.save(buffer, format=\"PNG\")\n        buffer: str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        data = {\"model\": model, \"image\": buffer}\n\n    # We do a dummy decode\n    _image = data.get(\"image\")\n    _image = _image.encode(\"utf-8\")\n    _image = base64.b64decode(_image)\n    _image = Image.open(io.BytesIO(_image))  # type: Image\n    if _image.mode == \"RGBA\":\n        _image = _image.convert(\"RGB\")\n\n    _model = data.get(\"model\")\n\n    # We generate a random prediction\n    w, h = _image.size\n\n    detections = [\n        Detection(\n            x_min=random.randint(0, w // 2 - 1),\n            y_min=random.randint(0, h // 2 - 1),\n            x_max=random.randint(w // w, w - 1),\n            y_max=random.randint(h // 2, h - 1),\n            class_name=\"dummy\",\n            confidence=round(random.random(), 3),\n        )\n        for _ in range(random.randint(1, 10))\n    ]\n\n    # We return the result\n    result = Result(time=0.1, model=_model, detections=detections)\n\n    return result\n\n\n@st.cache(show_spinner=True)\ndef make_request(model_url: str, model: str, image: Image) -&gt; Result:\n    \"\"\"\n    Process our data and send a proper request\n    \"\"\"\n    with io.BytesIO() as buffer:\n        image.save(buffer, format=\"PNG\")\n        buffer: str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        data = {\"model\": model, \"image\": buffer}\n\n        response = requests.post(\"{}/predict\".format(model_url), json=data)\n\n    if not response.status_code == 200:\n        raise ValueError(\"Error in processing payload, {}\".format(response.text))\n\n    response = response.json()\n\n    return Result.parse_obj(response)\n\n\ndef draw_preds(image: Image, detections: [Detection]):\n\n    class_names = list(set([detection.class_name for detection in detections]))\n\n    image_with_preds = image.copy()\n\n    # Define colors\n    colors = plt.cm.get_cmap(\"viridis\", len(class_names)).colors\n    colors = (colors[:, :3] * 255.0).astype(np.uint8)\n\n    # Define font\n    font = list(Path(\"/usr/share/fonts\").glob(\"**/*.ttf\"))[0].name\n    font = ImageFont.truetype(font=font, size=np.floor(3e-2 * image_with_preds.size[1] + 0.5).astype(\"int32\"))\n    thickness = (image_with_preds.size[0] + image_with_preds.size[1]) // 300\n\n    # Draw detections\n    for detection in detections:\n        left, top, right, bottom = detection.x_min, detection.y_min, detection.x_max, detection.y_max\n        score = float(detection.confidence)\n        predicted_class = detection.class_name\n        class_idx = class_names.index(predicted_class)\n\n        label = \"{} {:.2f}\".format(predicted_class, score)\n\n        draw = ImageDraw.Draw(image_with_preds)\n        label_size = draw.textsize(label, font)\n\n        top = max(0, np.floor(top + 0.5).astype(\"int32\"))\n        left = max(0, np.floor(left + 0.5).astype(\"int32\"))\n        bottom = min(image_with_preds.size[1], np.floor(bottom + 0.5).astype(\"int32\"))\n        right = min(image_with_preds.size[0], np.floor(right + 0.5).astype(\"int32\"))\n\n        if top - label_size[1] &gt;= 0:\n            text_origin = np.array([left, top - label_size[1]])\n        else:\n            text_origin = np.array([left, top + 1])\n\n        # My kingdom for a good redistributable image drawing library.\n        for r in range(thickness):\n            draw.rectangle([left + r, top + r, right - r, bottom - r], outline=tuple(colors[class_idx]))\n        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=tuple(colors[class_idx]))\n\n        if any(colors[class_idx] &gt; 128):\n            fill = (0, 0, 0)\n        else:\n            fill = (255, 255, 255)\n\n        draw.text(text_origin, label, fill=fill, font=font)\n\n        del draw\n\n    return image_with_preds\n\n\n# ---- Streamlit App ---\n\nst.title(\"Yolo v5 Companion App\")\n\nst.markdown(\n    \"A super nice companion application to send requests and parse results\\n\"\n    \"We wrap https://pytorch.org/hub/ultralytics_yolov5/\"\n)\n\n# ---- Sidebar ----\n\ntest_mode_on = st.sidebar.checkbox(label=\"Test Mode - Generate dummy answer\", value=False)\n\nst.sidebar.markdown(\"Enter the cluster URL\")\nmodel_url = st.sidebar.text_input(label=\"Cluster URL\", value=\"http://localhost:8000\")\n\n_model_url = model_url.strip(\"/\")\n\nif st.sidebar.button(\"Send 'is alive' to IP\"):\n    try:\n        health = requests.get(\"{}/health\".format(_model_url))\n        title = requests.get(\"{}/\".format(_model_url))\n        version = requests.get(\"{}/version\".format(_model_url))\n        describe = requests.get(\"{}/describe\".format(_model_url))\n\n        if health.status_code == 200:\n            st.sidebar.success(\"Webapp responding at {}\".format(_model_url))\n            st.sidebar.json({\"title\": title.text, \"version\": version.text, \"description\": describe.text})\n        else:\n            st.sidebar.error(\"Webapp not respond at {}, check url\".format(_model_url))\n    except ConnectionError:\n        st.sidebar.error(\"Webapp not respond at {}, check url\".format(_model_url))\n\n\n# ---- Main window ----\n\nst.markdown(\"## Inputs\")\nst.markdown(\"Select your model (Small, Medium or Large)\")\n\n# Data input\nmodel_name = st.radio(label=\"Model Name\", options=[\"yolov5s\", \"yolov5m\", \"yolov5l\"])\n\nst.markdown(\"Upload an image\")\n\nimage_file = st.file_uploader(label=\"Image File\", type=[\"png\", \"jpg\", \"tif\"])\n\nconfidence_threshold = st.slider(label=\"Confidence filter\", min_value=0.0, max_value=1.0, value=0.0, step=0.05)\n\n# UploadFile to PIL Image\nif image_file is not None:\n    image_file.seek(0)\n    image = image_file.read()\n    image = Image.open(io.BytesIO(image))\n\nst.markdown(\"Send the payload to {}/predict\".format(_model_url))\n\n# Send payload\nif st.button(label=\"SEND PAYLOAD\"):\n    if test_mode_on:\n        st.warning(\"Simulating a dummy request to {}\".format(model_url))\n        result = make_dummy_request(model_url=_model_url, model=model_name, image=image)\n    else:\n        result = make_request(model_url=_model_url, model=model_name, image=image)\n\n    st.balloons()\n\n    # Display results\n    st.markdown(\"## Display\")\n\n    st.text(\"Model : {}\".format(result.model))\n    st.text(\"Processing time : {}s\".format(result.time))\n\n    detections = [detection for detection in result.detections if detection.confidence &gt; confidence_threshold]\n\n    image_with_preds = draw_preds(image, detections)\n    st.image(image_with_preds, width=1024, caption=\"Image with detections\")\n\n    st.markdown(\"### Detection dump\")\n    for detection in result.detections:\n        st.json(detection.json())\n</code></pre> <p>Note</p> <p>Le test mode servait pour un ancien BE. Si vous avez tout fait dans l'ordre vous ne devriez pas en avoir besoin</p>"},{"location":"1_8_deployment_tp_long.html#construire-le-docker_1","title":"Construire le docker","text":"<pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\ndocker build -t eu.gcr.io/${PROJECT_ID}/{your app name}:{your version} -f Dockerfile .\n</code></pre>"},{"location":"1_8_deployment_tp_long.html#tester-le-docker_1","title":"Tester le docker","text":"<p>Warning</p> <p>Malheureusement, sur github codespace cela ne semble pas fonctionner. Nous allons devoir partir du principe que cela fonctionne du premier coup ! Le mieux est donc de s'assurer que le app.py correspond \u00e0 la correction puis de passer \u00e0 la section suivante</p> <p>Au lieu de faire <code>streamlit run app.py</code>, vous pouvez lancer le docker localement et aller sur {ip}:8501 pour tester le docker</p> <pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\ndocker run --rm -p 8501:8501 eu.gcr.io/${PROJECT_ID}/{your app name}:{your version}\n</code></pre> <p>Vous pouvez vous rendre sur l'ip de la machine sur le port 8501</p> <p>Indiquez l'ip de la machine port 8000 \u00e0 gauche</p>"},{"location":"1_8_deployment_tp_long.html#pousser-le-docker-sur-google-container-registry","title":"Pousser le docker sur google container registry","text":"<pre><code>gcloud auth configure-docker\ndocker push eu.gcr.io/${PROJECT_ID}/{your-name}-frontend:{your version}\n</code></pre>"},{"location":"1_8_deployment_tp_long.html#liens-utiles_1","title":"Liens Utiles","text":"<ul> <li>Doc Streamlit</li> </ul>"},{"location":"1_8_deployment_tp_long.html#4-deployer-le-modele-et-lux-sur-linstance-gcp","title":"4 - D\u00e9ployer le mod\u00e8le et l'UX sur l'instance GCP","text":"<p>Nous allons cr\u00e9er une machine virtuelle dans laquelle nous allons lancer les deux containers</p>"},{"location":"1_8_deployment_tp_long.html#41-creation-de-la-vm","title":"4.1 Cr\u00e9ation de la VM","text":"<p>Nous allons directement cr\u00e9er une machine avec le container du mod\u00e8le d\u00e9j\u00e0 lanc\u00e9</p> <p>Commen\u00e7ons par cr\u00e9er une instance GCP bien configur\u00e9e depuis laquelle se connecter:</p> <p>N'oubliez pas de renommer le nom de votre instance</p> <pre><code>export INSTANCE_NAME=\"tp-deployment-{yourgroup}-{yourname}\" # Don't forget to replace values !\n</code></pre> <pre><code>gcloud compute instances create $INSTANCE_NAME \\\n        --zone=\"europe-west1-b\" \\\n        --machine-type=\"n1-standard-2\" \\\n        --image-family=\"common-cpu\" \\\n        --image-project=\"deeplearning-platform-release\" \\\n        --maintenance-policy=TERMINATE \\\n        --scopes=\"storage-rw\" \\\n        --boot-disk-size=75GB\n</code></pre> <p>R\u00e9cuperez l'ip publique de la machine (via l'interface google cloud ou bien en faisant <code>gcloud compute instances list | grep {votre instance}</code> et notez l\u00e0 bien</p> <p>Depuis le github codespace, connectez vous \u00e0 la machine</p> <pre><code>    gcloud compute ssh {user}@{instance}\n</code></pre>"},{"location":"1_8_deployment_tp_long.html#42-execution-des-containers","title":"4.2 Execution des containers","text":"<p>Hint</p> <p>A executer dans la VM GCP</p> <p>On va utiliser <code>docker compose</code> pour lancer les deux applications en simultan\u00e9 de sorte \u00e0 ce qu'elles communiquent</p> <p>Plus d'infos sur docker compose</p> <ul> <li>Fermez tous les dockers etc.</li> <li>Cr\u00e9ez un fichier <code>docker-compose.yml</code></li> </ul> <p>Sur votre codespace, cr\u00e9ez ce fichier et modifiez le nom des images avec celles que vous avez utilis\u00e9es (respectivement model et frontend)</p> <pre><code>version: '3'\nservices:\n  yolo:\n    image: \"eu.gcr.io/third-ridge-138414/yolo-v5:1.2\"\n    ports:\n      - \"8000:8000\"\n    hostname: yolo\n  streamlit:\n    image: \"eu.gcr.io/third-ridge-138414/yolo-v5-streamlit:1.2\"\n    ports:\n      - \"8501:8501\"\n    hostname: streamlit\n</code></pre> <p>Copiez ensuite ce texte sur la VM dans un fichier <code>docker-compose.yml</code> (exemple : via nano)</p> <p>On constate qu'on d\u00e9clare 2 services: - 1 service \"yolo\" - 1 service \"streamlit\"</p> <p>On d\u00e9clare aussi les ports ouverts de chaque application</p> <p>Maintenant... comment lancer les deux applications ?</p> <p><code>docker-compose up</code> dans le dossier o\u00f9 se trouve votre <code>docker-compose.yml</code></p> <p>Hint</p> <p>Si <code>docker-compose</code> ne fonctionne pas, <code>sudo apt -y install docker-compose</code></p> <p>Normalement: - le service de mod\u00e8le est accessible sur le port 8000 de la machine - le service streamlit est accessible sur le port 8501 de la machine - vous devez indiquer l'hostname \"yolo\" pour communiquer entre streamlit et le mod\u00e8le. En effet, les services sont accessibles via un r\u00e9seau sp\u00e9cial \"local\" entre tous les containers lanc\u00e9s via docker-compose</p>"},{"location":"1_8_deployment_tp_long.html#acces-a-la-vm","title":"Acc\u00e8s \u00e0 la VM","text":"<p>Hint</p> <p>Cela ne risque de fonctionner que en 4G</p> <p>Connectez vous via l'IP publique de la machine via votre navigateur web, sur le port 8501 : <code>http://ip-de-la-machine:8501</code></p> <p>Vous devriez pouvoir acc\u00e9der \u00e0 votre d\u00e9ploiement !</p>"},{"location":"1_8_deployment_tp_long.html#conclusion","title":"Conclusion","text":"<p>\ud83c\udf89 Bravo ! \ud83c\udf89</p> <p>Vous avez d\u00e9ploy\u00e9 votre premier mod\u00e8le en production !</p>"},{"location":"2_1_overview.html","title":"Introduction to Data Distribution","text":"<p>Overview slides</p>"},{"location":"2_1_overview.html#course-overview","title":"Course Overview","text":"<ul> <li>Data Distribution &amp; Big Data Processing</li> </ul> <p>Harnessing the complexity of large amounts of data is a challenge in itself. </p> <p>But Big Data processing is more than that: originally characterized by the 3 Vs of Volume, Velocity and Variety,  the concepts popularized by Hadoop and Google requires dedicated computing solutions (both software and infrastructure),  which will be explored in this module.</p>"},{"location":"2_1_overview.html#objectives","title":"Objectives","text":"<p>By the end of this module, participants will be able to:</p> <ul> <li>Understand the differences and usage between main distributed computing architectures (HPC, Big Data, Cloud, CPU vs GPGPU)</li> <li>Implement the distribution of simple operations via the Map/Reduce principle in PySpark</li> <li>Understand the principle of Kubernetes</li> <li>Deploy a Big Data Processing Platform on the Cloud</li> <li>Implement the distribution of data wrangling/cleaning and training machine learning algorithms using PyData stack, Jupyter notebooks and Dask</li> </ul>"},{"location":"2_2_orchestration.html","title":"Intro to Orchestration and Kubernetes","text":""},{"location":"2_2_orchestration.html#intro-to-orchestration","title":"Intro to Orchestration","text":"<p>Link to slides</p> <p> </p>"},{"location":"2_2_orchestration.html#intro-to-kubernetes","title":"Intro to Kubernetes","text":"<p>Link to slides</p> <p> </p>"},{"location":"2_3_kub_handson.html","title":"Kubernetes: Zero to Jupyterhub using Google Kubernetes Engine","text":"<p>Slides here</p>"},{"location":"2_3_kub_handson.html#what-is-jupyterhub","title":"What is JupyterHub","text":"<p>JupyterHub brings the power of notebooks to groups of users. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which can be managed efficiently by system administrators.</p> <p>JupyterHub runs in the cloud or on your own hardware, and makes it possible to serve a pre-configured data science environment to any user in the world. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. Key features of JupyterHub</p> <p>Customizable - JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab, RStudio, nteract, and more.</p> <ul> <li> <p>Flexible - JupyterHub can be configured with authentication in order to provide access to a subset of users. Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub).</p> </li> <li> <p>Scalable - JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs on Kubernetes, and can run with up to tens of thousands of users.</p> </li> <li> <p>Portable - JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes commercial cloud providers, virtual machines, or even your own laptop hardware.</p> </li> </ul> <p>The foundational JupyterHub code and technology can be found in the JupyterHub repository. This repository and the JupyterHub documentation contain more information about the internals of JupyterHub, its customization, and its configuration.</p>"},{"location":"2_3_kub_handson.html#zero-to-jupyterhub-using-kubernetes","title":"Zero to Jupyterhub using Kubernetes","text":"<p>JupyterHub allows users to interact with a computing environment through a webpage. As most devices have access to a web browser, JupyterHub makes it is easy to provide and standardize the computing environment of a group of people (e.g., for a class of students or an analytics team).</p> <p>This project will help you set up your own JupyterHub on a cloud and leverage the clouds scalable nature to support large groups of users. Thanks to Kubernetes, we are not tied to a specific cloud provider.</p>"},{"location":"2_3_kub_handson.html#instructions","title":"Instructions","text":"<ul> <li> <p>Go here and follow the instructions</p> </li> <li> <p>Use Google Kubernetes Engine to setup your cluster</p> </li> <li> <p>Give some people the public IP of your cluster so that they can connect to it... try to make it scale !</p> </li> </ul>"},{"location":"2_4_functional.html","title":"Functional Programming","text":"<p>This section of the course is not given this year.</p>"},{"location":"2_4_functional.html#functional-programming-for-distributed-data","title":"Functional Programming for Distributed Data","text":"<p>Link to slides</p> <p> </p>"},{"location":"2_4_functional.html#introduction-to-julia","title":"Introduction to Julia","text":"<p>As the first exercise, you'll need to install Julia and IJulia locally or make a working Julia Colab Notebook. While Colab is sufficient for today's exercises, it is recommended to make a local installation:</p> <p>Julia download Julia kernel for Jupyter</p> <p>Here is a Colab template from this Github repository which will install the Julia kernel for a single Colab instance.</p> <p>Once you have a Julia Jupyter kernel, follow this Julia for Pythonistas notebook.</p> <p>Github Colab</p>"},{"location":"2_4_functional.html#functional-programming-in-julia","title":"Functional Programming in Julia","text":"<p>Julia documentation explaining:</p> <ul> <li>Functions, showing that they are first-class</li> <li>the <code>map</code> function which is a higher-order function</li> <li>distributed computing allowing for transfer of functions between threads or workers</li> </ul>"},{"location":"2_4_functional.html#distributed-data-in-julia","title":"Distributed Data in Julia","text":"<p>Julia's base language supports distributed calculation but there are a few packages which facilitate data processing tasks over distributed data: </p> <ul> <li>DistributedArrays - A general Array type which can be distributed over multiple workers.</li> <li>JuliaDB - A data structuring package which automatically handles distributed data storage and computation</li> <li>Spark.jl - A Julia interface to Apache Spark. Related blog post.</li> </ul>"},{"location":"2_4_functional.html#map-reduce-exercise","title":"Map Reduce Exercise","text":"<p>The second part of this class is an interactive notebook in the Julia language covering the MapReduce programming framework, from simple addition queries to a grep example.</p> <p>MapReduce notebook</p> <p>MapReduce notebook on Colab (requires adding Julia kernel installation)</p>"},{"location":"2_5_mapreduce.html","title":"Hadoop and MapReduce","text":"<p>In this class, we start with an overview of the Big Data ecosystem, contextualizing Hadoop, No-SQL Databases, and Business Intelligence tools. We then cover Hadoop and the HDFS in detail with a simple MapReduce example.</p> <p>Slides</p> <ul> <li>Introduction to Big Data and its ecosystem (1h)</li> <li>What is Big Data?</li> <li>Legacy \u201cBig Data\u201d ecosystem</li> <li>Big Data use cases</li> <li>Big Data to Machine Learning</li> <li>Big Data platforms, Hadoop &amp; Beyond (2h)</li> <li>Hadoop, HDFS and MapReduce,</li> <li>Datalakes, Data Pipelines</li> <li>From HPC to Big Data to Cloud and High Performance Data Analytics </li> <li>BI vs Big Data</li> <li>Hadoop legacy: Spark, Dask, Object Storage ...</li> </ul> <p>It contains also a short interactive exercise using Python Map Reduce.</p>"},{"location":"2_6_spark.html","title":"Spark","text":"<p>In this class, we cover the Apache Spark framework, explaining Resilient Distributed Datasets, SparkSQL, Spark MLLib, and how to interact with a Spark cluster. We use PySpark in a Jupyter notebook to explore RDDs and see an example of distributed K-Means.</p> <p>Spark introduction</p> <p>Spark notebook</p> <p>Spark notebook on Colab</p>"},{"location":"2_7_cloud.html","title":"Evolution of Data Management Systems","text":""},{"location":"2_7_cloud.html#fundamental-concepts-methods-and-applications","title":"Fundamental Concepts, Methods and Applications","text":"<p>In this three part class, students will cover the history of data management systems, from file systems to databases to distributed cloud storage. This class is given over the length of the Data Engineering course. Questions from the first two parts are integrated into the exam on cloud computing, and questions from the Cloud DMS section are integrated into the Dask notebook evaluation.</p>"},{"location":"2_7_cloud.html#objectives","title":"Objectives","text":"<p>The objectives of this course are: - Introduce the fundamental concepts - Describe, in a synthetic way, the main characteristics of the evolution of DMS (Data Management Systems) - Highlight targeted application classes.</p>"},{"location":"2_7_cloud.html#key-words","title":"Key Words","text":"<p>Data Management Systems, Uni-processor DBMS, Parallel DBMS, Data Integration Systems,Big Data, Cloud  Data Management Systems, High Performance, Scalability, Elasticity, Multi-store/Poly-store Systems</p>"},{"location":"2_7_cloud.html#targeted-skills","title":"Targeted Skills","text":"<ul> <li>Effectively exploit the DMS according to the environment (uniprocessor, parallel, distributed, cloud) in a perspective of decision support within an organization.</li> <li>Ability to choose, in a relevant way, a DMS in multiple environments for an optimal functioning of the applications of an organization</li> </ul>"},{"location":"2_7_cloud.html#indicative-program","title":"Indicative Program","text":"<ol> <li> <p>Introduction to Main Problems of Data Management</p> <ul> <li>From File Management Systems FMS to Database MS DBMS</li> <li>Motivations, Objectives, Organizations &amp; Drawbacks</li> <li>Databases &amp; Rel. DBMS: Motivations &amp; Objectives</li> <li>Resources:<ul> <li>Introduction</li> <li>SGF - File Systems</li> <li>Views - Relational Systems</li> <li>File Organization</li> </ul> </li> </ul> </li> <li> <p>Parallel Database Systems</p> <ul> <li>Objectives and Parallel Architecture Models</li> <li>Data Partitioning Strategies</li> <li>Parallel Query Processing</li> <li>Resources:<ul> <li>Parallel DBMS</li> <li>Parallel Queries</li> <li>Systems DB Parallel</li> </ul> </li> </ul> </li> <li> <p>From Distributed DB to Data Integration Systems DIS</p> <ul> <li>An Ex. of DDB, Motivations &amp; Objectives</li> <li>Designing of DDB</li> <li>Distributed Query Processing</li> <li>An Ex. of DIS</li> <li>Motivations &amp; Objectives</li> <li>Mediator-Adapters Architecture</li> <li>Design of a Global Schema (GAV, LAV)</li> <li>Query Processing Methodologies</li> <li>Resources:<ul> <li>Distributed DBMS - Chapter 1</li> <li>Distributed DBMS - Chapter 2</li> <li>Distributed DBMS - Chapter 3</li> <li>Systems for integrating heterogeneous and distributed data</li> <li>Integration Systems complement</li> <li>Distributed DBMS Dec 2023</li> </ul> </li> </ul> </li> <li> <p>Cloud Data Management Systems CDMS</p> <ul> <li>Motivations and Objectives</li> <li>Main Characteristics of Big Data and CDMS</li> <li>Classification of Cloud Data Management Systems CDMS</li> <li>Advantages and Weakness of Parallel RDBMS and CDMS</li> <li>Comparison between Parallel RDBMS and CDMS</li> <li>Introduction to Multi-store/Ploystore Systems</li> <li>Resources:<ul> <li>Cloud Systems</li> <li>MapReduce examples</li> </ul> </li> </ul> </li> <li> <p>Conclusion</p> <ul> <li>Maturity of Cloud DMS</li> <li>Key Criteria for Choosing a Data Management System</li> </ul> </li> </ol>"},{"location":"2_7_cloud.html#additional-reading","title":"Additional Reading","text":"<ol> <li> <p>Principles of Distributed Database Systems,  M. Tamer Ozsu  and Patrick Valduriez; Springer-Verlag ;  Fourth Edition,  December 2019.</p> </li> <li> <p>Data Management in the Cloud: Challenges and Opportunities Divyakant Agrawal, Sudipto Das, and Amr El Abbadi; Synthesis Lectures on Data Management, December 2012, Vol. 4, No. 6 , Pages 1-138.</p> </li> <li> <p>Query Processing in Parallel Relational Database Systems; H. Lu, B.-C Ooi and K.-L. Tan; IEEE Computer Society Press, CA, USA, 1994.</p> </li> <li> <p>Traitement parall\u00e8le dans les bases de donn\u00e9es relationnelles : concepts, m\u00e9thodes et applications Abdelkader Hameurlain, Pierre Bazex, Franck Morvan; C\u00e9padu\u00e8s Editions,  Octobre 1996.  </p> </li> </ol>"},{"location":"2_8_dask.html","title":"Dask on Kubernetes","text":"<p>In this class, we focus on getting a Dask cluster running in Kubernetes, which we will then use in the Dask project. Dask is a parallel computing library in Python which integrates well with machine learning tools like scikit-learn.</p> <p>This class builds on the orchestration class, going into further detail on K8S specifics.</p> <p>Kubernetes</p> <p>Dask presentation</p> <p>Students will use GCP for this class. Be sure to stop your cluster after class to conserve GCP credits.</p> <p>Additional resources can be found in the dask documentation.</p>"},{"location":"2_8_dask.html#deploying-a-dask-hub","title":"Deploying a Dask Hub","text":"<p>This material is taken from the following docs:</p> <ul> <li>https://docs.dask.org/en/latest/setup/kubernetes-helm.html</li> <li>https://zero-to-jupyterhub.readthedocs.io/en/latest/kubernetes/setup-kubernetes.html</li> <li>https://zero-to-jupyterhub.readthedocs.io/en/latest/kubernetes/setup-helm.html</li> </ul>"},{"location":"2_8_dask.html#creating-a-kubernetes-cluster","title":"Creating a Kubernetes Cluster","text":"<p>First, you need to enable the Kubernetes API if not already done:</p> <ul> <li>Go to console.cloud.google.com</li> <li>Select the Kubernetes Engine in the menu</li> <li>Enable the API if not already done.</li> </ul> <p>Then you'll need a terminal with <code>gcloud</code> and <code>kubectl</code>. The simplest is just to use the Google Cloud Shell from console.cloud.google.com. If you prefer, you can follow the links above to find how to install everything on your computer.</p> <p>Ask Google Cloud to create a managed Kubernetes cluster and a default node pool to get nodes from:</p> <pre><code>gcloud container clusters create \\\n  --machine-type n1-standard-4 \\\n  --enable-autoscaling \\\n  --min-nodes 1 \\\n  --max-nodes 10 \\\n  --num-nodes 1 \\\n  --zone europe-west1-b \\\n  --cluster-version 1.23 \\\n  dask-hub-k8s\n</code></pre> <p>Yhis will take a few minutes (maybe 2 or 3).</p> <pre><code>gcloud container clusters list\n</code></pre> <p>You can then test if the cluster is running:</p> <pre><code>kubectl get node\n</code></pre> <p>Then get permissions to perform all administrative actions needed.</p> <p>\u26a0\ufe0fDon't forget to replace your email below.\u26a0\ufe0f</p> <pre><code>kubectl create clusterrolebinding cluster-admin-binding \\\n  --clusterrole=cluster-admin \\\n  --user=&lt;GOOGLE-EMAIL-ACCOUNT&gt;\n</code></pre>"},{"location":"2_8_dask.html#setting-up-helm","title":"Setting up Helm","text":"<p>From your Google Cloud Shell or terminal:</p> <pre><code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\nhelm list\n</code></pre> <p>should return:</p> <pre><code>NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION\n</code></pre>"},{"location":"2_8_dask.html#helm-install-a-dask-hub","title":"Helm install a Dask Hub","text":"<p>Default Daskhub configuration uses dask-gateway, which is here to handle multiple users with fine grain authorisations. We don't need this, and it iss a little more complicated setup than what we'll do.</p> <p>Instead, we'll deploy a Daskhub with dask-kubernetes, which assumes some authorisations inside the Pods of the Kubernetes cluster (potential security leak), but is more straightforward for our usage.</p> <p>Verify that you\u2019ve set up a Kubernetes cluster and added Dask\u2019s helm charts:</p> <pre><code>helm repo add dask https://helm.dask.org/\nhelm repo update\n</code></pre> <p>Generate token to configure Jupyterhub:</p> <pre><code>openssl rand -hex 32  &gt; token1.txt\ncat token1.txt\n</code></pre> <p>Create the file below (for example using vim or cloud shell editor) and substitute the  value. <pre><code># file: daskhub-config.yaml\njupyterhub:\n  proxy:\n    secretToken: \"&lt;token-1&gt;\"\n  scheduling:\n    podPriority:\n      enabled: true\n    userPlaceholder:\n      # Specify three dummy user pods will be used as placeholders\n      replicas: 1\n    userScheduler:\n      enabled: true\n  singleuser:\n    serviceAccountName: daskkubernetes\n    image:\n      name: guillaumeeb/pangeo-ml-notebook # Image to use for singleuser environment. Must include dask-kubernetes.\n      tag: 2021.11.14\n\ndask-gateway:\n  enabled: false\n  gateway:\n    auth:\n      type: simple\n      simple:\n        password: \"unused\"\n\ndask-kubernetes:\n  enabled: true\n</code></pre> <p>Now we just install Dask Hub:</p> <pre><code>helm upgrade --wait --install --render-subchart-notes \\\n    --namespace daskhub \\\n    --create-namespace \\\n    dhub dask/daskhub \\\n    --values=daskhub-config.yaml\n</code></pre> <p>This will again take a few minutes.</p> <pre><code>helm list -n daskhub\n</code></pre> <p>Check install and go to Jupyter!</p> <p>To get the public IP of your hub deployment:</p> <pre><code>kubectl --namespace=daskhub get service proxy-public\n</code></pre> <p>Get the external IP, and open it in your browser. You should be able to login with any username/password Ensure Dask is working, and K8S mecanisms too!</p>"},{"location":"2_8_dask.html#create-a-dask-kubernetes-cluster","title":"Create a dask-kubernetes cluster","text":"<p>Create a yaml file within the Jupyterhub interface:</p> <pre><code># worker-spec.yaml\n\nkind: Pod\nmetadata:\n  labels:\n    foo: bar\nspec:\n  restartPolicy: Never\n  containers:\n  - image: guillaumeeb/pangeo-ml-notebook:2021.11.14\n    imagePullPolicy: IfNotPresent\n    args: [dask-worker, --nthreads, '2', --no-dashboard, --memory-limit, 6GB, --death-timeout, '60']\n    name: dask\n    env:\n      - name: EXTRA_PIP_PACKAGES\n        value: xgboost\n    resources:\n      limits:\n        cpu: \"2\"\n        memory: 6G\n      requests:\n        cpu: \"1.7\"\n        memory: 6G\n</code></pre> <p>Just open a notebook in your newly created Dask enabled hub, and try to copy and past the following cells:</p> <p>Set some config to ease usage.</p> <pre><code>import dask\nimport dask.distributed  # populate config with distributed defaults\nimport dask_kubernetes\n\ndask.config.set({\"kubernetes.worker-template-path\": \"worker-spec.yaml\"})\ndask.config.set({\"distributed.dashboard.link\": \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{port}/status\"})\n</code></pre> <p>Create a cluster object.</p> <pre><code>from dask_kubernetes import KubeCluster\n\ncluster = KubeCluster(deploy_mode='local') # Scheduler is started in the notebook process\ncluster\n</code></pre> <p>This should display a fancy widget. You can open the Dask Dashboard from here.</p> <p>Now scale the cluster to get Dask-workers and connect to it.</p> <pre><code>cluster.scale(20)\n</code></pre> <pre><code>from distributed import Client\n\nclient = Client(cluster)\nclient\n</code></pre> <p>What's happening in your K8S cluster after some seconds/minutes? Launch some computation, what about Pi?</p> <p>We'll use Dask array, a Numpy extension, for this:</p> <pre><code>import dask.array as da\n\nsample = 10_000_000_000  # &lt;- this is huge!\nxxyy = da.random.uniform(-1, 1, size=(2, sample))\nnorm = da.linalg.norm(xxyy, axis=0)\nsumm = da.sum(norm &lt;= 1)\ninsiders = summ.compute()\npi = 4 * insiders / sample\nprint(\"pi ~= {}\".format(pi))\n</code></pre> <p>How many workers did you get? Why?</p> <p>Now just close the cluster.</p> <pre><code>cluster.close()\n</code></pre> <p>What happens after a few minutes?</p>"},{"location":"2_8_dask.html#deleting-a-kubernetes-cluster","title":"Deleting a Kubernetes Cluster","text":"<p>Get your cluster name and region</p> <pre><code>gcloud container clusters list\n</code></pre> <p>Delete your kubernetes cluster</p> <pre><code>gcloud container clusters delete &lt;YOUR_CLUSTER_NAME&gt; --region &lt;YOUR_CLUSTER_REGION&gt;\n</code></pre>"},{"location":"2_9_project.html","title":"Project - Dask","text":"<p>The evaluation for this class is a Dask notebook. You should run this notebook on a Daskhub using Kubernetes, like in the Dask on Kubernetes class. You should complete the exercises and answer the questions in the notebook, then turn it in through the LMS. You should work in a group of 2 to 4 and separate the work out equally between responding to questions, managing the infrastructure, and trying out different algorithms. Be sure to include the names of your group members in your submission.</p> <p>The notebook is due on March 12, 2024 at 23h59.</p> <p>Dask tutorial, if needed</p> <p>Evaluation notebook</p> <p>LMS depot</p>"},{"location":"ctf.html","title":"Data Engineering Fundamentals Capture the Flag","text":"<p>This class is a five day Capture the Flag event to get to know with the basics of systems usage, specifically linux, git, and ssh. There is also a large section on python, with an emphasis on data science scripting practices using numpy and pandas in jupyter notebooks.</p> <p>This is a self-guided exercise with resources and questions on this site. You, the participant, must look for the answer to the questions through reading documentation, discussing with others, and trying things. Try to avoid searching for answers online in a search engine; the answers can almost always be found in documentation.</p> <p>Answers can be submitted through an API with the CTF server. Questions will be made available over the course of 5 sessions. Responding correctly to a question gives 1 point, and an additional 0.5 points are awarded for being the first to submit the correct answer to a question. That half point is the flag - be the first to capture it!</p> <p>If you're speeding through the questions, consider helping others learn the material. Depending on your background, you may have varied experience with these tools. Get to know the other participants by helping them capture a flag too.</p>"},{"location":"ctf.html#linux","title":"Linux","text":"<p>Linux is an open-source operating system based on Unix. It is a standard choice for development and is the most dominant operating system for web servers, cloud computing, and high performance computing at 80% of global public servers. There are many different distributions but they share a common set of tools, notably GNU software. A very common Linux distribution is Android, at 73% of all mobile devices, so you might be a Linux user already without realizing it!</p> <p>You most likely don't use Linux as the operating system of your personal computer, however. If you are using one the 2.5 % of personal computers with Linux, you can skip straight to the Submission section</p> <p>MacOS is also based on Unix, so if you're using MacOS, most things should work just as in Linux! A few commands will be different from the course instructions, and the questions will always refer to Linux resources, for example documentation. It is highly recommended to install homebrew (https://brew.sh/) which will allow for package installation via the command line.</p>"},{"location":"ctf.html#installation-on-windows","title":"Installation on Windows","text":"<p>The easiest way to use Linux on Windows is through the Windows Subsystem for Linux. Installation instructions are here: https://docs.microsoft.com/en-us/windows/wsl/install. Make sure to follow all instructions carefully. If asked to join a \"Windows Insiders Program\", ignore this. By default, this installs Ubuntu, which is good for this systems class and for all of SDD.</p> <p>The WSL is similar to a virtual machine inside of Windows, but it integrates with some existing components of Windows. You can access your Windows files from Linux at <code>/mnt/</code>, but you should make sure you're familiar with Linux first.</p> <ul> <li>About the WSL</li> <li>WSL FAQ</li> <li>How to Access WSL Linux Files from Windows</li> </ul>"},{"location":"ctf.html#submission","title":"Submission","text":"<p>All questions will be posted to the CTF github repository. In the second class, we will use git to download this repository locally, and it will be used to host the files and data needed to respond to questions.</p> <p>The CTF server's IP address is <code>34.163.196.38</code>. You can see a leaderboard there and it is the address for submitting answers. The first way we'll look at submitting answers is with <code>curl</code> in Linux.</p> <p>Once you have a Unix-type environment, either native Linux or macOS, or through the WSL, you're ready to submit to the CTF. You will use the <code>curl</code> command; you can verify that you have <code>curl</code> by running <code>which curl</code> in the command line. <code>curl</code> is a tool for transferring data from or to a server. How do you know that? By checking the documentation of <code>curl</code> using <code>man curl</code>. Try it out!</p> <p>To respond to a question, send a POST request with the data of the question <code>number</code> and <code>answer</code>, and your username as <code>user</code> (your username should be your ISAE login, but you can also check on the leaderboard). For example, the first question asks where the <code>curl</code> executable is (hint: use <code>which</code>). Then use <code>curl</code>:</p> <pre><code>curl -X POST 'http://34.163.196.38/' \\\n    -d 'number=1' \\\n    -d 'answer=your answer here' \\\n    -d 'user=your username here'\n</code></pre> <p>Some of the questions will require access to some files, called <code>file_a.txt</code>, <code>file_b.txt</code>, and <code>file_c.txt</code>. Those are available on the CTF git repository.</p> <p>You are ready to start answering questions! If you don't know an answer, check the resources below and read documentation using <code>man</code>.</p> <p>You can see which questions you have answered by sending a GET request:</p> <pre><code>curl 'http://34.163.196.38/user/d.wilson'\n</code></pre> <p>You can also see which questions have remaining flags, the bonus points associated with answering the question for the first time, with a GET request:</p> <pre><code>curl 'http://34.163.196.38/answers/'\n</code></pre>"},{"location":"ctf.html#python-submission","title":"Python Submission","text":"<p>Note that you can use the <code>requests</code> library to submit responses:</p> <pre><code>import requests\ndata = {\"number\": \"1\",\n        \"answer\": \"\",\n        \"user\": \"d.wilson\"}\nr = requests.post(\"http://34.163.196.38/\", data=data)\n</code></pre>"},{"location":"ctf.html#bash-resources","title":"Bash Resources","text":"<ul> <li>ISAE class on CLI, Linux, and Bash</li> <li>Shell class from MIT</li> <li>Bash exercises</li> <li>More bash exercises</li> <li>Short exercises in regular expressions</li> </ul>"},{"location":"ctf.html#linux-tools","title":"Linux tools","text":"<p>Now that you're an expert in Linux, let's quickly look at some useful tools. You may need to install some of these, either using <code>apt</code>, <code>brew</code>, <code>yum</code>, <code>pacman</code>, or whichever package manager you use. Linux comes with many programs installed by default, especially distributions like Ubuntu, however the tools in this section will be more useful than the base Linux tools. We'll cover four: <code>apt</code> for package management, <code>top</code> for system monitoring, <code>tmux</code> for terminal management, and <code>vim</code> for file editing. There are alternatives to all of these programs that are great, but it is worth being familiar with these four.</p>"},{"location":"ctf.html#linux-resources","title":"Linux Resources","text":"<ul> <li>apt manual</li> <li>Alternatives to top</li> <li>Guide to tmux</li> <li>tmux cheat sheet</li> <li>Editors from MIT class</li> <li>Vim adventures</li> <li>tldr, short man pages</li> </ul>"},{"location":"ctf.html#git","title":"Git","text":"<p>Git is a version control system used worldwide for maintaining code, documents, video games, and much more. It has seen wide adoption with servers like Github and Gitlab while being an open-source tool that anyone can install as a client or server. In this class, we will look at repositories hosted on Github, but git is much larger than that and many organizations like ISAE have their own private git server.</p>"},{"location":"ctf.html#installation","title":"Installation","text":"<p>If you're using Ubuntu, chances are you already have <code>git</code>. If not, simply do:</p> <p><code>sudo apt install git</code></p> <p>These questions concern two repositories: the Machine Learning class in SDD (https://github.com/SupaeroDataScience/machine-learning) and the Seaborn library, a popular graphing library (https://github.com/mwaskom/seaborn). You will need to download both repositories. First choose a directory to host them in, for example <code>~/SDD/FSD312</code>:</p> <pre><code>mkdir -p ~/SDD/FSD312\ncd ~/SDD/FSD312\n</code></pre> <p>and then download them using git clone:</p> <pre><code>git clone https://github.com/SupaeroDataScience/machine-learning.git\ngit clone https://github.com/mwaskom/seaborn.git\n</code></pre> <p>The commit for all questions on the <code>seaborn</code> repository is <code>1e6739</code> :</p> <pre><code>git checkout 1e6739\n</code></pre>"},{"location":"ctf.html#git-resources","title":"Git Resources","text":"<ul> <li>Git course</li> <li>Introduction to github</li> <li>Github video course</li> <li>Learn git branching</li> <li>Git SCM book</li> <li>Git cheat sheet</li> </ul>"},{"location":"ctf.html#git-exercise","title":"Git Exercise","text":"<p>In order to access the server for the next parts of the CTF, you will need to provide your public ssh key. The SSH section has references explaining public-key cryptography, but in general you will make a key pair with a private side and public side. You will give the public side to services like this class or Github to perform secure communication, keeping your private key secret to prove that it is you.</p> <p>First, start by making a key pair and uploading your public key to Github. This will allow you use to SSH to make push requests, instead of using a personal access token. Create an SSH key and add it to your Github account.</p> <p>Then, we will use git as a way for you to transfer your public key to the class. We could use another means, like a USB key, email, or a very large QR code, but for this exercise we will use git.</p> <ul> <li>First make a fork of the https://github.com/SupaeroDataScience/ctf2025 repository.</li> <li>Then, make a pull request with your key as a file in <code>keys/</code>. Please name your key with your name, like the example <code>keys/dennis-wilson.pub</code>. Be sure to upload only your public key. Do not ever upload your private key to public servers.</li> </ul> <p>Once your key is in the repository, you are ready for the SSH and Python portions of the CTF.</p>"},{"location":"ctf.html#ssh","title":"SSH","text":"<p>For the ssh section, you will connect to a new server to answer questions about the remote environment.</p> <p>Pre-requisite: Your public key must be uploaded to the git repository above to get access to the server. You will use the corresponding private key to access the server.</p> <p>Your user on the server is <code>ctf</code> and the IP is: <code>35.190.209.18</code>.</p> <p>Please note that ISAE-EDU and ethernet block ssh to most servers, including this one and <code>github.com</code>. In order to ssh to the server, you will need to either use the eduroam network or a different network like a mobile hotspot.</p>"},{"location":"ctf.html#ssh-resources","title":"SSH Resources","text":"<ul> <li>Ubuntu ssh manual</li> <li>Guide in French</li> <li>Cryptographie Asym\u00e9trique</li> <li>How SSH works</li> </ul>"},{"location":"ctf.html#python","title":"Python","text":"<p>An overview and reminder of the python programming language, with a focus on numpy and pandas manipulation using Jupyter.</p>"},{"location":"ctf.html#installation_1","title":"Installation","text":"<p>You most likely have python installed on your Linux system, but it is worthwhile to make sure and to upgrade. Python 3.8, 3.9, or 3.10 are all supported.</p> <p>There are multiple ways to setup Python.The recommended one is to install Python through Anaconda. Conda, or the platform Anaconda, can be useful on Windows as many packages are built specifically for windows, but not all packages are available via conda.</p> <p>The installation instructions can be found on the Anaconda website.</p> <p>It is highly recommended to make a <code>virtual environment</code> to manage your python packages. This feature is directly available with <code>conda</code> .</p> <p>For example the following command creates a <code>conda</code> virtual environment named <code>myenv</code>\u00a0with Python 3.12:</p> <pre><code>conda create -y -n myenv -c conda-forge python=3.12\n</code></pre> <p>Then activate the newly created environment with:</p> <pre><code>conda activate myenv\n</code></pre> <p>Within the virtual environment all operations implying Python (<code>pip install</code> , running <code>python script.py</code>...) are isolated from the Python (or \"system Python\", the Python used by the OS itself) already installed on the laptop. This mitigates the risk of dealing unrecoverable damage to the OS (and avoids to reinstall everything from scratch).</p> <p>More information can be found in Conda's docs.</p> <p>There are other well-known libraries for virtual environments:</p> <ul> <li>Virtualenv most straight-forward and convenient for new users on Linux with Python already installed.</li> <li>Pipenv is an exciting project aimed at Python developers, but it adds additional complexity.</li> </ul> <p>Once you have a virtual environment created and activated, please install the following packages for the rest of the Seminars class:</p> <pre><code>numpy\npandas\nscipy\nmatplotlib\njupyter\n</code></pre> <p>The following packages will also be used in SDD:</p> <pre><code>seaborn\nscikit-learn\nkeras\ntorch\ngeos\ngraphviz\nnltk\nnetworkx\nstatsmodels\npyspark\ncython\ncma\ngym\n</code></pre>"},{"location":"ctf.html#jupyter","title":"Jupyter","text":"<p>Jupyter (stands for the three original languages in the project: Julia, Python, and R) is a way to use and develop code interactively in the browser. Once you've installed the jupyter package, you can run a Jupyter notebook by simply running <code>jupyter notebook</code>.</p> <p>For Windows users, you can run Jupyter in the WSL. As explained in this blog post, you simply need to execute <code>jupyter notebook --no-browser</code> on the WSL and then copy and paste the URL and token generated into a Windows browser.</p> <p>Some additional packages for improving Jupyter are <code>nbopen nbdime RISE</code>. Be sure to read their documentation before installing to verify if these are relevant to you.</p>"},{"location":"ctf.html#python-resources","title":"Python Resources","text":"<ul> <li>Python 3 Documentation</li> <li>Pip documentation</li> <li>Pandas cheatsheet</li> <li>Stanford Python and Numpy tutorial</li> <li>Python seminar</li> <li>Google Colab: Jupyter notebooks on the cloud</li> <li>Binder: Also Jupyter notebooks on the cloud, not hosted by Google</li> </ul>"}]}